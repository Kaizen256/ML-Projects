{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39e2c9f",
   "metadata": {},
   "source": [
    "# Data Preparation and Augmentation\n",
    "\n",
    "Data augmentation techniques include RandomResizedCrop, RandomHorizontalFlip, RandAugment (applies random augmentations), and RandomErasing (randomly erases a rectangular region in an image). The pixel values are also normalized using the mean and standard deviation of the dataset. I found this in my ResNet-34 from scratch project.\n",
    "\n",
    "Dataset is then split into a 90% training set and a 10% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a5668f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "image_size = 64\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random'),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "])\n",
    "\n",
    "data_dir = 'tiny-imagenet-200/train'\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=train_transform)\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0bdc61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = train_dataset[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cc333",
   "metadata": {},
   "source": [
    "## Patching and Splitting Windows\n",
    "Unlike traditional CNNs that use sliding convolutional filters, Vision Transformers break the image down into a sequence of patches, treating them similarly to words in a sentence.\n",
    "\n",
    "Patchify Class: Takes an input image and converts it into patch embeddings. It uses a single convolutional layer where the kernel size and stride are equal to the patch_size. It divides the image into non overlapping patches and creating an initial vector embedding for each one.\n",
    "\n",
    "split_into_windows: Takes the patches and splits them into smaller windows. Self-attention is calculated within these windows, which is far more computationally efficient than the original ViT's approach of global attention across all patches.\n",
    "\n",
    "reverse_windows: Merges the split windows back into their original spatial layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "235210fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert an image into patch embeddings using a convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        in_channels: Number of input channels (e.g., 3 for RGB).\n",
    "        embed_dim: Output embedding dimension per patch.\n",
    "        patch_size: Size of each square patch\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS, H//patch_size, W//patch_size, embed_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, embed_dim, patch_size):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (B, 3, H, W)\n",
    "        x = self.conv(x)         # (BS, embed_dim, H//patch_size, W//patch_size)\n",
    "        return x.permute(0, 2, 3, 1) # (BS, H//patch_size, W//patch_size, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73990fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_windows(x, M):\n",
    "    \"\"\"\n",
    "    Splits (BS, H, W, channels) into non overlapping MxM windows.\n",
    "    Args:\n",
    "        x: Tensor of shape (BS, H, W, channels)\n",
    "        M: Window size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS * num_windows, M*M, channels)\n",
    "    \"\"\"\n",
    "    BS, H, W, channels = x.shape\n",
    "    x = x.reshape(BS, H//M, M, W//M, M, channels)\n",
    "    # Permute fixes the order so the MxM pixels are together properly\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5)     # (BS, H//M, W//M, M, M, channels)\n",
    "    return x.reshape(-1, M*M, channels) # (BS * num_windows, M*M, channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c525ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_windows(x, M, H, W, channels):\n",
    "    \"\"\"\n",
    "    Reverses MxM window tokens back into the original layout.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape (BS * num_windows, M*M, channels)\n",
    "        M: Window size\n",
    "        H: Original image height\n",
    "        W: Original image width\n",
    "        channels: Number of channels\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS, H, W, channels)\n",
    "    \"\"\"\n",
    "    BS = x.shape[0] // (H//M * W//M) # Original BS\n",
    "    x = x.reshape(BS, H//M, W//M, M, M, channels)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5)      # (BS, H//M, M, W//M, M, channels)\n",
    "    return x.reshape(BS, H, W, channels) # (BS, H, W, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3fc390",
   "metadata": {},
   "source": [
    "## Relative Position Bias\n",
    "\n",
    "Relative Position Bias shows self-attention mechanism about the geometry of the image. It computes a learnable bias between every pair of tokens in a window, based on how far apart they are. For each pair of tokens in an M x M window, compute relative position, use that to index into a learnable bias table, add this bias to the attention logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61586fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes relative position bias for self-attention.\n",
    "\n",
    "    Generates a table of learnable relative position biases between token pairs\n",
    "    within an attention window of shape (M x M). The relative position between\n",
    "    any two tokens is encoded as a bias vector per attention head, and these biases are\n",
    "    added to the attention scores in self-attention.\n",
    "\n",
    "    Args:\n",
    "        M: The height/width of the attention window. The total number of tokens is M * M.\n",
    "        nheads: Number of attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        relative_table: Learnable parameter of shape ((2M - 1)^2, nheads),\n",
    "            where each entry represents a bias value for a specific relative position and head.\n",
    "        relative_index: Lookup table of shape (M*M, M*M), where each entry is an index\n",
    "            into relative_table that maps the relative position between two tokens to a bias vector.\n",
    "\n",
    "    Forward Output:\n",
    "        Tensor of shape (nheads, M*M, M*M) containing the relative bias for each token pair\n",
    "        and each attention head. This can be directly added to attention logits.\n",
    "\n",
    "    Example:\n",
    "        relative = RelativePositionBias(M=3, nheads=4)\n",
    "        bias = relative()  # Output shape: (4, 9, 9), for 4 heads and 3x3 tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, M, nheads):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.nheads = nheads\n",
    "        # (2M-1)^2 because there are up to M-1 tokens above or below or left or right of each token.\n",
    "        # 2M - 1 possibilities for above and below, same for left and right. So (2M -1)^2 total.\n",
    "        # If M = 3 row and col would go between -2 and +2 when comparing two tokens.\n",
    "        # That gives 5^2 possible combinations. len(-2, -1, 0, 1, 2)^2\n",
    "        self.relative_table = nn.Parameter(torch.zeros(size=((2*M - 1) * (2*M -1), nheads)))\n",
    "\n",
    "        # Coordinate grid of token positions shows where each token is in the window.\n",
    "        # It gives every token a (row, col) coordinate.\n",
    "        # If M = 3: coords[0] (rows): [[0, 0, 0], [1, 1, 1], [2, 2, 2]], \n",
    "        #           coords[1] (cols): [[0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
    "        coords = torch.stack(torch.meshgrid( # Matrix style over Cartesian\n",
    "            torch.arange(M), torch.arange(M), indexing='ij'))  # (2, M, M)\n",
    "        \n",
    "        # Flatten the coordinates for token indices, so coords[:, i] is the (row, col) of token i\n",
    "        coords = coords.flatten(1) # (2, M*M)\n",
    "\n",
    "        # Compute relative positions so we have the position of a token relative to another token\n",
    "        # coords[:, :, None] shape: (2, M*M, 1), coords[:, None, :] shape: (2, 1, M*M)\n",
    "        relative = coords[:, :, None] - coords[:, None, :] # (2, M*M, M*M)\n",
    "\n",
    "        # Reformat so we can use each (row, col) as an index into a table but row/col values\n",
    "        # range from -(M-1) to (M-1) so we shift them up so they are positive: [0, 2M -2]\n",
    "        relative = relative.permute(1, 2, 0) # (M*M, M*M, 2)\n",
    "        relative[:, :, 0] += M - 1\n",
    "        relative[:, :, 1] += M - 1\n",
    "\n",
    "        # Flatten 2D positions into 1D. To convert: row * num_cols + col\n",
    "        self.register_buffer(   # Register buffer to move to GPU\n",
    "            \"relative_index\",\n",
    "            (relative[:, :, 0] * (2*M - 1) + relative[:, :, 1]).long() # (M*M, M*M)\n",
    "        )\n",
    "    def forward(self): \n",
    "        # Use index to get bias values, look up the bias vector for each token pair\n",
    "        bias = self.relative_table[self.relative_index.view(-1)] # (M*M * M*M, nheads)\n",
    "        bias = bias.reshape(self.relative_index.shape[0], self.relative_index.shape[1], self.nheads)\n",
    "        return bias.permute(2, 0, 1) # (nheads, M*M, M*M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b037665",
   "metadata": {},
   "source": [
    "## Windowed Multi-Head Self-Attention (W-MSA)\n",
    "\n",
    "Implements Windowed Multi-head Self-Attention, which is a more efficient version of the standard attention used in ViT. Instead of calculating attention across all patches in the entire image, W-MSA computes attention within M x M windows. Significantly reduces the number of calculations needed.\n",
    "\n",
    "Attention(Q, K, V) = Softmax((QK.T / √d) + B + mask) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c69239a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window-based Multihead Self-Attention with relative position bias.\n",
    "\n",
    "    Performs self-attention within non overlapping MxM windows of the input feature map.\n",
    "    It incorporates relative positional encoding and attention masks for shifted windows\n",
    "\n",
    "    Attention(Q, K, V) = Softmax((QK.T / √d) + B + mask) @ V\n",
    "    B is relative position bias.\n",
    "    Args:\n",
    "        channels: Input channels\n",
    "        M: Height and width of the attention window.\n",
    "        nheads: Number of attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        q, k, v: Linear layers for queries, keys, and values.\n",
    "        out: Output linear layer after attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, M, nheads):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.nheads = nheads\n",
    "        self.rootd = (channels // nheads) ** -0.5 # (1 / √d) == (1 / √dim_per_head)\n",
    "\n",
    "        self.q = nn.Linear(channels, channels)\n",
    "        self.k = nn.Linear(channels, channels)\n",
    "        self.v = nn.Linear(channels, channels)\n",
    "        self.out = nn.Linear(channels, channels)\n",
    "\n",
    "        self.relative = RelativePositionBias(M, nheads)\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for window based self-attention.\n",
    "\n",
    "        Args:\n",
    "            x: shape (B * nW, M*M, channels) where nW is number of windows\n",
    "            attn_mask: Attention mask used for shifted windows to prevent cross-window attention.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (B*nW, M*M, channels)\n",
    "        \"\"\"\n",
    "        # x shape = (B * nW, M*M, channels) where nW is number of windows\n",
    "        BnW, M_sq, channels = x.shape # M_sq is M*M\n",
    "\n",
    "        # d stands for dim_per_head. Permute on k so no transpose when computing attn.\n",
    "        # q: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> transpose(1, 2) --> (BnW, heads, M*M, d)\n",
    "        # k: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> permute(0, 2, 3, 1) --> (BnW, heads, d, M*M)\n",
    "        # v: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> transpose(1, 2) --> (BnW, heads, M*M, d)\n",
    "        q = self.q(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).transpose(1, 2)\n",
    "        k = self.k(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).permute(0, 2, 3, 1)\n",
    "        v = self.v(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).transpose(1, 2)\n",
    "\n",
    "        # Attention: (Q @ K.T) / √d + relative bias + optional mask\n",
    "        # k is already transposed.\n",
    "        attn = (q @ k) * self.rootd # (BnW, nheads, M*M, M*M)\n",
    "        attn = attn + self.relative()\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (M*M, M*M) --> unsqueeze(0) --> (1, M*M, M*M)\n",
    "            # Broadcasted to (BnW, nheads, M*M, M*M)\n",
    "            nW = attn_mask.shape[0]\n",
    "            BS = BnW // nW\n",
    "            attn_mask = attn_mask.to(attn.device)\n",
    "            attn = attn.view(BS, nW, self.nheads, M_sq, M_sq) + attn_mask.unsqueeze(0).unsqueeze(2)\n",
    "            attn = attn.view(BnW, self.nheads, M_sq, M_sq)\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(BnW, M_sq, channels)\n",
    "        return self.out(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b163964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(H, W, M, shift):\n",
    "    \"\"\"\n",
    "    Creates an attention mask for shifted window self-attention (SW-MSA).\n",
    "\n",
    "    This function generates a mask to prevent tokens from attending across windows when \n",
    "    performing SW-MSA. It divides the feature map into distinct regions, assigns unique labels\n",
    "    to each, uses cyclic shifting, partitions it into non overlapping windows, and then\n",
    "    builds an attention mask that blocks attention between different labeled regions.\n",
    "\n",
    "    Args:\n",
    "        H: Height of the feature map.\n",
    "        W: Width of the feature map.\n",
    "        M: Window size.\n",
    "        shift: Number of pixels to cyclically shift the window. \n",
    "               If shift is 0, no mask is needed. In Swin it is M // 2\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (nW, M*M, M*M) where nW is the number of windows.\n",
    "        Or None if shift is 0.\n",
    "    \"\"\"\n",
    "    if shift == 0:\n",
    "        return None\n",
    "    \n",
    "    img_mask = torch.zeros((1, H, W, 1))  # Mask\n",
    "\n",
    "    count = 0\n",
    "    H, W = img_mask.shape[1:3]\n",
    "\n",
    "    # Split image into 9 regions\n",
    "    h_ranges = [(0, H - M), (H - M, H - shift), (H - shift, H)]\n",
    "    w_ranges = [(0, W - M), (W - M, W - shift), (W - shift, W)]\n",
    "\n",
    "    # so if H = W = 12, M = 6, shift = 3\n",
    "    # h_ranges = [(0, 6), (6, 9), (9, 12)]\n",
    "    # w_ranges = [(0, 6), (6, 9), (9, 12)]\n",
    "\n",
    "    # Fill each region with a unique integer\n",
    "    for h_start, h_end in h_ranges:\n",
    "        for w_start, w_end in w_ranges:\n",
    "            img_mask[:, h_start:h_end, w_start:w_end, :] = count\n",
    "            count += 1\n",
    "            \n",
    "    # Cyclic shift the mask\n",
    "    img_mask = torch.roll(img_mask, shifts=(-shift, -shift), dims=(1,2))\n",
    "\n",
    "    # Split into M*M windows\n",
    "    mask_windows = split_into_windows(img_mask, M)  # (nW, M*M, 1)\n",
    "    mask_windows = mask_windows.squeeze(-1)       # (nW, M*M)\n",
    "    # Create attention mask\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)  # (nW, M*M, M*M)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float('-inf')).masked_fill(attn_mask == 0, 0.0)\n",
    "    return attn_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00091934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_attention_mask(12, 12, 6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409aa504",
   "metadata": {},
   "source": [
    "## Stochastic Depth\n",
    "\n",
    "Stochastic depth is a regularization technique used to improve generalization and reduce overfitting. Instead of dropping individual neurons, entire residual branches are skipped during training with a given probability (drop_prob)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24a0cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_depth(x, drop_prob, training):\n",
    "    \"\"\"\n",
    "    Applies stochastic depth to the input.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        drop_prob: Probability of dropping the path.\n",
    "        training: If True, stochastic depth is applied. If False, input is returned unchanged.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor with some residual paths zeroed out.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "\n",
    "    keep_prob = 1.0 - drop_prob\n",
    "    # Create mask with shape (BS, 1, 1, ..., 1) so it broadcasts over all non batch dims\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "    # 1 with prob keep_prob, 0 with prob drop_prob\n",
    "    mask = torch.rand(shape, dtype=x.dtype, device=x.device) < keep_prob\n",
    "    # Scale\n",
    "    x = x / keep_prob\n",
    "    return x * mask\n",
    "\n",
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"\n",
    "    Module for stochastic depth.\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob):\n",
    "        super().__init__()\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return stochastic_depth(x, self.drop_prob, self.training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fccf37",
   "metadata": {},
   "source": [
    "## SwinBlock (Shifted Window Transformer Block)\n",
    "\n",
    "Applies windowed self-attention over non overlapping M×M windows and alternates between non-shifted and shifted windows across consecutive blocks to enable cross-window connections. There is also an MLP with GELU at the end. Residual connections are used.\n",
    "\n",
    "![Block Architecture](figures/block_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f82ce04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer block with shifted/non-shifted window self-attention + MLP.\n",
    "\n",
    "    Args:\n",
    "        dim: Channel dimension of the input features.\n",
    "        H: Feature map height.\n",
    "        W: Feature map width.\n",
    "        nheads: Number of attention heads in WindowAttention.\n",
    "        M: Window size.\n",
    "        shift: Cyclic shift size (0 for non-shifted windows, M//2 for shifted).\n",
    "        ratio: Expansion ratio for the MLP hidden size (hidden dim = ratio * dim).\n",
    "        stoch_depth: stochastic depth probability for dropping residual branches.\n",
    "\n",
    "    Attributes:\n",
    "        norm1: Pre attention normalization.\n",
    "        attn (WindowAttention): Window-based multi-head self-attention.\n",
    "        drop_path: Stochastic depth module or identity if stoch_depth == 0.\n",
    "        norm2: Pre MLP normalization.\n",
    "        mlp: Two-layer feed forward network with GELU activation.\n",
    "        attn_mask: Mask for shifted attention, shape (nW, M*M, M*M) when shift > 0, else None.\n",
    "\n",
    "    Input:\n",
    "        x: Tensor of shape (BS, H, W, channels).\n",
    "\n",
    "    Output:\n",
    "        Tensor of shape (BS, H, W, Channels), same spatial shape and channels as input.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, H, W, nheads, M, shift, ratio, stoch_depth):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "        self.shift = shift\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(channels=dim, M=M, nheads=nheads)\n",
    "        if stoch_depth > 0:\n",
    "            self.stoch = StochasticDepth(stoch_depth)\n",
    "        else:\n",
    "            self.stoch = nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * ratio)), # Error I had before, forgot to wrap with int()\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * ratio), dim)\n",
    "        )\n",
    "        self.attn_mask = create_attention_mask(H, W, M, shift)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of SwinBlock.\n",
    "\n",
    "        LayerNorm --> cyclic shift --> split windows --> WindowAttention (masked if shifted)\n",
    "        --> reverse windows --> reverse shift --> residual + stochastic depth --> LayerNorm\n",
    "        --> MLP → residual +stochastic depth\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (BS, H, W, channels).\n",
    "        \"\"\"\n",
    "        BS, H, W, channels = x.shape\n",
    "\n",
    "        store = x # For Residual connection\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        if self.shift > 0:\n",
    "            x = torch.roll(x, shifts=(-self.shift, -self.shift), dims=(1, 2)) # Cyclic shift\n",
    "        x_windows = split_into_windows(x, self.M)                  # (BS*nW, M*M, channels)\n",
    "        x_windows = self.attn(x_windows, attn_mask=self.attn_mask) # (BS*nW, M*M, channels)\n",
    "        x = reverse_windows(x_windows, self.M, H, W, channels)     # (BS, H, W, channels)\n",
    "        if self.shift > 0:\n",
    "            x = torch.roll(x, shifts=(self.shift, self.shift), dims=(1, 2))\n",
    "\n",
    "        x = store + self.stoch(x)\n",
    "\n",
    "        return x + self.stoch(self.mlp(self.norm2(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a9350c",
   "metadata": {},
   "source": [
    "## Patch Merging Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c4a4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    \"\"\"\n",
    "    Reduces H and W by 2x in each dimension. Doubles the channel dimension.\n",
    "    - Extract non overlapping 2x2 patches from the feature map.\n",
    "    - Concatenate features from each patch along the channel dimension.\n",
    "    - Apply LayerNorm for normalization across channels.\n",
    "    - Lower 4*channels down to 2*channels with a Linear layer.\n",
    "\n",
    "    Input:\n",
    "        x: shape (BS, H, W, channels)\n",
    "\n",
    "    Output:\n",
    "        shape (BS, H/2, W/2, 2*channels)\n",
    "\n",
    "    H and W should be even.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4*dim, 2*dim)\n",
    "        self.norm = nn.LayerNorm(4*dim) # 4 * channels --> 2 * channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (BS, H, W, channels)\n",
    "        x1 = x[:, 0::2, 0::2, :]\n",
    "        x2 = x[:, 1::2, 0::2, :]\n",
    "        x3 = x[:, 0::2, 1::2, :]\n",
    "        x4 = x[:, 1::2, 1::2, :]\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=3) # (BS, H/2, W/2, 4*channels)\n",
    "        x = self.norm(x)\n",
    "        return self.lin(x) # (BS, H/2, W/2, 2*channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf769e2",
   "metadata": {},
   "source": [
    "## Stage (Stack of Swin Blocks + Optional Patch Merging)\n",
    "\n",
    "A Stage stacks SwinBlocks, alternating between:\n",
    "- W-MSA (non-shifted windows, shift=0)\n",
    "- SW-MSA (shifted windows, shift=M//2)\n",
    "\n",
    "Then optionally applies PatchMerging to downsample and increase channels for the next stage.\n",
    "After PatchMerging, the next stage should be constructed with dim = 2 * previous_dim and H and W halved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd036435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dim: Channel dimension.\n",
    "        H: Feature map height.\n",
    "        W: Feature map width.\n",
    "        M: Window size.\n",
    "        blocks: Number of SwinBlocks in this stage.\n",
    "        nheads: Number of attention heads per block.\n",
    "        stoch_depth_list: List of stochastic depth probabilities (len == blocks).\n",
    "        patch_merging: If True, apply PatchMerging at the end of the stage.\n",
    "        ratio: MLP expansion ratio (hidden dim = ratio * dim).\n",
    "\n",
    "    Input:\n",
    "        x: Tensor of shape (BS, H, W, channels or dim)\n",
    "\n",
    "    Output:\n",
    "        - If patch_merging is False: (BS, H, W, dim)\n",
    "        - If patch_merging is True:  (BS, H/2, W/2, 2*dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, H, W, M, blocks, nheads, stoch_depth_list, patch_merging, ratio):\n",
    "        # stoch_depth_list is a list of the stochastic depth rates for each SwinBlock.\n",
    "\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(blocks):\n",
    "            if i % 2 == 0:\n",
    "                shift = 0  # Alternate between W-MSA and SW-MSA, W-MSA has no shift.\n",
    "            else:\n",
    "                shift = M // 2\n",
    "            self.blocks.append(\n",
    "                SwinBlock(dim, H, W, nheads, M, shift, ratio, stoch_depth_list[i])\n",
    "            )\n",
    "        if patch_merging:\n",
    "            self.patch = PatchMerging(dim)\n",
    "        else:\n",
    "            self.patch = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (BS, H, W, channels)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.patch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893ac1d",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "- Patch size: 4 × 4\n",
    "- Base embed dim C: 96\n",
    "- Depths: [2, 2, 6, 2]\n",
    "- Num heads: [3, 6, 12, 24]\n",
    "- Window size: 7 for all blocks\n",
    "- Shift: 3 (7 // 2) for all shift blocks\n",
    "- MLP expantion ratio: 4.0 for all blocks\n",
    "- Drop path rate: 0.2 (linearly increased across all blocks)\n",
    "- Patch Merging / Downsample at the end of Stage 1, 2 and 3.\n",
    "\n",
    "\n",
    "| Stage         | Blocks | Heads | Stoch_dep | In Channels | Out Channels | Output Shape             |\n",
    "|---------------|--------|-------|-----------|-------------|--------------|--------------------------|\n",
    "| PatchEmbed    | None   | None  | None           | 3           | 96           | (BS, H/4, W/4, 96)       |\n",
    "| Stage 1       | 2      | 3     | [0.0000, 0.0182]          | 96          | 192          | (BS, H/8, W/8, 192)      |\n",
    "| Stage 2       | 2      | 6     | [0.0364, 0.0545]          | 192         | 384          | (BS, H/16, W/16, 384)    |\n",
    "| Stage 3       | 6      | 12    | [0.0727, 0.0909, 0.1091, 0.1273, 0.1455, 0.1636]          | 384         | 768          | (BS, H/32, W/32, 768)    |\n",
    "| Stage 4       | 2      | 24    | [0.1818, 0.2000]          | 768         | 768          | (BS, H/32, W/32, 768)    |\n",
    "| Head          | None   | None  | None          | 768         | num_classes  | (BS, num_classes)        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21886056",
   "metadata": {},
   "source": [
    "## Swin Transformer Architecture\n",
    "\n",
    "![Architecture](figures/Architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da3dd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer\n",
    "\n",
    "    Steps:\n",
    "        - Patchify to get (BS, H/patch, W/patch, emb_dim)\n",
    "        - 4 stages of Swin blocks with alternating W-MSA (shift=0) and SW-MSA (shift=M//2)\n",
    "        - Patch Merging at the end of stages 1-3\n",
    "        - Global average pooling and linear classifier head\n",
    "\n",
    "    Args:\n",
    "        img_size: Input image size\n",
    "        patch_size: Patch size for Patchify.\n",
    "        emb_dim: Base embedding dimension channel for stage 1.\n",
    "        blocks (List[int]): Number of blocks per stage.\n",
    "        nheads (List[int]): Number of attention heads per stage.\n",
    "        M: Window size for all blocks.\n",
    "        n_classes: Number of output classes for the classifier head.\n",
    "        stochastic_endpoint: Stochastic depth ratio endpoint for linspace.\n",
    "\n",
    "    Shapes:\n",
    "        Input:  (BS, 3, 224, 224)\n",
    "        After patchify: (BS, 56, 56, 96)\n",
    "        After stage1:   (BS, 28, 28, 192)\n",
    "        After stage2:   (BS, 14, 14, 384)\n",
    "        After stage3:   (BS, 7, 7, 768)\n",
    "        After stage4:   (BS, 7, 7, 768)\n",
    "        Output logits:  (BS, n_classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, emb_dim, blocks,\n",
    "                 nheads, M, n_classes, stochastic_endpoint):\n",
    "        super().__init__()\n",
    "\n",
    "        H = img_size // patch_size\n",
    "        W = img_size // patch_size\n",
    "        dims = [emb_dim, 2*emb_dim, 4*emb_dim, 8*emb_dim]\n",
    "\n",
    "        self.patchify = Patchify(3, emb_dim, patch_size)\n",
    "\n",
    "        # Linearly increase across all blocks (inclusive endpoints)\n",
    "        stoch_depth = list(np.linspace(0, stochastic_endpoint, sum(blocks)))\n",
    "        \n",
    "        # 4 Stages with stochastic depth aligned with the number of blocks in each stage.\n",
    "        ind = 0\n",
    "        self.stage1 = Stage(\n",
    "            dims[0], H, W, M[0], \n",
    "            blocks[0], nheads[0], \n",
    "            stoch_depth[ind:ind+blocks[0]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[0]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage2 = Stage(\n",
    "            dims[1], H, W, M[1], \n",
    "            blocks[1], nheads[1], \n",
    "            stoch_depth[ind:ind+blocks[1]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[1]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage3 = Stage(\n",
    "            dims[2], H, W, M[2], \n",
    "            blocks[2], nheads[2], \n",
    "            stoch_depth[ind:ind+blocks[2]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[2]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage4 = Stage(\n",
    "            dims[3], H, W, M[3], \n",
    "            blocks[3], nheads[3], \n",
    "            stoch_depth[ind:ind+blocks[3]], \n",
    "            patch_merging=False, ratio=4.0)\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[3])\n",
    "        self.head = nn.Linear(dims[3], n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (BS, 3, img_size, img_size).\n",
    "\n",
    "        Returns:\n",
    "            Class logits of shape (BS, n_classes).\n",
    "        \"\"\"\n",
    "        # x shape = (BS, 3, 224, 224)\n",
    "        x = self.patchify(x) # (BS, H//patch_size, W//patch_size, embed_dim)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.norm(x).mean(dim=(1, 2)) # Global average pooling over H, W.\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f724356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rowes\\AppData\\Local\\Temp\\ipykernel_18260\\3172234026.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
      "C:\\Users\\rowes\\AppData\\Local\\Temp\\ipykernel_18260\\3172234026.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
      "C:\\Users\\rowes\\AppData\\Local\\Temp\\ipykernel_18260\\3172234026.py:77: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n",
      "Epoch 1: Train Loss: 4.9961, Train Acc: 0.0305, Val Loss: 4.8224, Val Acc: 0.0481\n",
      "Saved new best model.\n",
      "Epoch 2: Train Loss: 4.4569, Train Acc: 0.0796, Val Loss: 3.9954, Val Acc: 0.1405\n",
      "Saved new best model.\n",
      "Epoch 3: Train Loss: 3.9401, Train Acc: 0.1400, Val Loss: 3.7650, Val Acc: 0.1715\n",
      "Saved new best model.\n",
      "Epoch 4: Train Loss: 3.6598, Train Acc: 0.1807, Val Loss: 3.5394, Val Acc: 0.2026\n",
      "Saved new best model.\n",
      "Epoch 5: Train Loss: 3.4769, Train Acc: 0.2119, Val Loss: 3.3565, Val Acc: 0.2365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rowes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n",
      "Epoch 6: Train Loss: 3.3371, Train Acc: 0.2327, Val Loss: 3.2954, Val Acc: 0.2392\n",
      "Saved new best model.\n",
      "Epoch 7: Train Loss: 3.1219, Train Acc: 0.2742, Val Loss: 3.1144, Val Acc: 0.2778\n",
      "Saved new best model.\n",
      "Epoch 8: Train Loss: 2.9474, Train Acc: 0.3052, Val Loss: 3.1205, Val Acc: 0.2830\n",
      "Saved new best model.\n",
      "Epoch 9: Train Loss: 2.7992, Train Acc: 0.3318, Val Loss: 2.9446, Val Acc: 0.3118\n",
      "Saved new best model.\n",
      "Epoch 10: Train Loss: 2.6728, Train Acc: 0.3566, Val Loss: 2.9510, Val Acc: 0.3177\n",
      "Epoch 11: Train Loss: 2.5493, Train Acc: 0.3807, Val Loss: 2.9413, Val Acc: 0.3158\n",
      "Saved new best model.\n",
      "Epoch 12: Train Loss: 2.4395, Train Acc: 0.3999, Val Loss: 2.9088, Val Acc: 0.3263\n",
      "Saved new best model.\n",
      "Epoch 13: Train Loss: 2.3367, Train Acc: 0.4202, Val Loss: 2.8796, Val Acc: 0.3342\n",
      "Epoch 14: Train Loss: 2.2344, Train Acc: 0.4393, Val Loss: 2.9586, Val Acc: 0.3219\n",
      "Epoch 15: Train Loss: 2.1385, Train Acc: 0.4591, Val Loss: 2.9390, Val Acc: 0.3280\n",
      "Epoch 16: Train Loss: 2.0504, Train Acc: 0.4777, Val Loss: 2.9715, Val Acc: 0.3261\n",
      "Epoch 17: Train Loss: 1.9639, Train Acc: 0.4943, Val Loss: 3.0193, Val Acc: 0.3240\n",
      "Epoch 18: Train Loss: 1.8817, Train Acc: 0.5119, Val Loss: 3.0287, Val Acc: 0.3278\n",
      "Epoch 19: Train Loss: 1.8010, Train Acc: 0.5279, Val Loss: 3.0667, Val Acc: 0.3264\n",
      "Epoch 20: Train Loss: 1.7207, Train Acc: 0.5487, Val Loss: 3.0674, Val Acc: 0.3287\n",
      "Epoch 21: Train Loss: 1.6518, Train Acc: 0.5627, Val Loss: 3.1160, Val Acc: 0.3231\n",
      "Epoch 22: Train Loss: 1.5834, Train Acc: 0.5770, Val Loss: 3.1626, Val Acc: 0.3233\n",
      "Epoch 23: Train Loss: 1.5169, Train Acc: 0.5925, Val Loss: 3.2138, Val Acc: 0.3186\n",
      "Epoch 24: Train Loss: 1.4619, Train Acc: 0.6046, Val Loss: 3.2532, Val Acc: 0.3173\n",
      "Epoch 25: Train Loss: 1.4049, Train Acc: 0.6190, Val Loss: 3.2617, Val Acc: 0.3224\n",
      "Epoch 26: Train Loss: 1.3364, Train Acc: 0.6355, Val Loss: 3.3539, Val Acc: 0.3184\n",
      "Epoch 27: Train Loss: 1.2845, Train Acc: 0.6473, Val Loss: 3.3263, Val Acc: 0.3175\n",
      "Epoch 28: Train Loss: 1.2322, Train Acc: 0.6601, Val Loss: 3.3586, Val Acc: 0.3170\n",
      "Epoch 29: Train Loss: 1.1961, Train Acc: 0.6699, Val Loss: 3.4356, Val Acc: 0.3159\n",
      "Epoch 30: Train Loss: 1.1418, Train Acc: 0.6829, Val Loss: 3.4754, Val Acc: 0.3201\n",
      "Epoch 31: Train Loss: 1.1015, Train Acc: 0.6936, Val Loss: 3.4228, Val Acc: 0.3201\n",
      "Epoch 32: Train Loss: 1.0631, Train Acc: 0.7031, Val Loss: 3.4059, Val Acc: 0.3267\n",
      "Epoch 33: Train Loss: 1.0183, Train Acc: 0.7138, Val Loss: 3.5454, Val Acc: 0.3099\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "import json\n",
    "\n",
    "model = SwinTransformer(\n",
    "    img_size=64, patch_size=4, emb_dim=64,\n",
    "    blocks=[2, 2, 4, 2], nheads=[2, 4, 8, 16],\n",
    "    M=[8, 8, 4, 2], n_classes=200, stochastic_endpoint=0.1).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Scheduler Epochs\n",
    "warmup_epochs = 5  # Gradually increase lr for the first 5 epochs\n",
    "cosine_epochs = 95 # Cosine-anneal lr for the remaining 95 epochs\n",
    "\n",
    "# AdamW optimizer, weight decay taken directly from the paper.\n",
    "# fused=True uses a fused CUDA kernel\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05, fused=True)\n",
    "\n",
    "# Linear warmup scheduler. Start at 1% of lr and gradually fo up to 100%\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_epochs)\n",
    "# Cosine annealing scheduler. Decay lr following a cosine curve\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=cosine_epochs)\n",
    "# Run warmup_scheduler first, then cosine_scheduler\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
    "# GradScaler for Automatic Mixed Precision (AMP). Saves VRAM.\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
    "\n",
    "best_val_acc = 0.0\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"lr\": []\n",
    "}\n",
    "\n",
    "for epoch in range(warmup_epochs + cosine_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True) # set_to_none saves a bit of memory.\n",
    "        # Forward and loss in mixed precision\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "            yhat = model(xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "        # Backprop with scaled loss\n",
    "        scaler.scale(loss).backward()\n",
    "        # unscale before clipping, then clip\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # Step optimizer only goes if gradients are finite\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = torch.argmax(yhat, dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    # Loss and accuracy\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "                yhat = model(xb)\n",
    "                loss = loss_fn(yhat, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            preds = torch.argmax(yhat, dim=1)\n",
    "            val_correct += (preds == yb).sum().item()\n",
    "            val_total += xb.size(0)\n",
    "    \n",
    "    # Validation loss and accuracy\n",
    "    avg_val_loss = val_loss / val_total\n",
    "    val_accuracy = val_correct / val_total\n",
    "    \n",
    "    # Get current learning rate. I had this wrong before, I was grabbing the past lr instead of current\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"train_acc\"].append(accuracy)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_acc\"].append(val_accuracy)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "\n",
    "    with open(\"training_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val_acc\": best_val_acc\n",
    "        }, \"swin_t_best.pt\")\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss: {avg_loss:.4f}, Train Acc: {accuracy:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a5421",
   "metadata": {},
   "source": [
    "224 images by 224 was way too much for my GPU. So I went down to 64 x 64 and adjusted. I didn't want M to be 2 so I modified so each stage has a different M. Entire architecture was adapted and controlled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
