{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a5668f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from torch import nn, optim\n",
    "\n",
    "image_size = 224  \n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random'),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "])\n",
    "\n",
    "data_dir = 'tiny-imagenet-200/train'\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=train_transform)\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0bdc61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = train_dataset[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd428424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 64])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(64, 128, 8, 8).flatten(2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cc333",
   "metadata": {},
   "source": [
    "Swin-T: C = 96, layer numbers = {2, 2, 6, 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "235210fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(nn.Module):\n",
    "    \"\"\"\n",
    "    Converts an input image into a sequence of flattened patch embeddings\n",
    "\n",
    "    Args:\n",
    "        in_channels: Number of input channels (3 RGB).\n",
    "        embed_dim: Dimensionality of output patch embeddings.\n",
    "        patch_size: Size of each square patch\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS, N_patches, embed_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, embed_dim=96, patch_size=4):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (B, 3, H, W)\n",
    "        x = self.conv(x)         # (BS, embed_dim, H//patch_size, W//patch_size)\n",
    "        x = x.flatten(2)         # (BS, embed_dim, N_patches)\n",
    "        return x.transpose(1, 2) # (BS, N_patches, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73990fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_windows(x, M):\n",
    "    \"\"\"\n",
    "    Splits (BS, H, W, channels) into non overlapping MxM windows.\n",
    "    Args:\n",
    "        x: Tensor of shape (BS, H, W, channels)\n",
    "        M: Window size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS * num_windows, M*M, channels)\n",
    "    \"\"\"\n",
    "    BS, H, W, channels = x.shape\n",
    "    x = x.reshape(BS, H//M, M, W//M, M, channels)\n",
    "    # Permute fixes the order so the MxM pixels are together properly\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5)     # (BS, H//M, W//M, M, M, channels)\n",
    "    return x.reshape(-1, M*M, channels) # (BS * num_windows, M*M, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c525ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_windows(x, M, H, W, channels):\n",
    "    \"\"\"\n",
    "    Reverses MxM window tokens back into the original layout.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape (BS * num_windows, M*M, channels)\n",
    "        M: Window size\n",
    "        H: Original image height\n",
    "        W: Original image width\n",
    "        channels: Number of channels\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS, H, W, channels)\n",
    "    \"\"\"\n",
    "    BS = x.shape[0] // (H//M * W//M) # Original BS\n",
    "    x = x.reshape(BS, H//M, W//M, M, M, channels)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5)      # (BS, H//M, M, W//M, M, channels)\n",
    "    return x.reshape(BS, H, W, channels) # (BS, H, W, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3fc390",
   "metadata": {},
   "source": [
    "Relative Position Bias computes a learnable bias between every pair of tokens in a window, based on how fat apart they are. For each pair of tokens in an M x M window, compute relative position, use that to index into a learnable bias table, add this bias to the attention logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61586fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes relative position bias for self-attention.\n",
    "\n",
    "    Generates a table of learnable relative position biases between token pairs\n",
    "    within an attention window of shape (M x M). The relative position between\n",
    "    any two tokens is encoded as a bias vector per attention head, and these biases are\n",
    "    added to the attention scores in self-attention.\n",
    "\n",
    "    Args:\n",
    "        M: The height/width of the attention window. The total number of tokens is M * M.\n",
    "        nheads: Number of attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        relative_table: Learnable parameter of shape ((2M - 1)^2, nheads),\n",
    "            where each entry represents a bias value for a specific relative position and head.\n",
    "        relative_index: Lookup table of shape (M*M, M*M), where each entry is an index\n",
    "            into relative_table that maps the relative position between two tokens to a bias vector.\n",
    "\n",
    "    Forward Output:\n",
    "        Tensor of shape (nheads, M*M, M*M) containing the relative bias for each token pair\n",
    "        and each attention head. This can be directly added to attention logits.\n",
    "\n",
    "    Example:\n",
    "        relative = RelativePositionBias(M=3, nheads=4)\n",
    "        bias = relative()  # Output shape: (4, 9, 9), for 4 heads and 3x3 tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, M, nheads):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.nheads = nheads\n",
    "        # (2M-1)^2 because there are up to M-1 tokens above or below or left or right of each token.\n",
    "        # 2M - 1 possibilities for above and below, same for left and right. So (2M -1)^2 total.\n",
    "        # If M = 3 row and col would go between -2 and +2 when comparing two tokens.\n",
    "        # That gives 5^2 possible combinations. len(-2, -1, 0, 1, 2)^2\n",
    "        self.relative_table = nn.Parameter(torch.zeros(size=((2*M - 1) * (2*M -1), nheads)))\n",
    "\n",
    "        # Coordinate grid of token positions lets us know where each token is in the window.\n",
    "        # It gives every token a (row, col) coordinate.\n",
    "        # If M = 3: coords[0] (rows): [[0, 0, 0], [1, 1, 1], [2, 2, 2]], \n",
    "        #           coords[1] (cols): [[0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
    "        coords = torch.stack(torch.meshgrid( # Matrix style over Cartesian\n",
    "            torch.arange(M), torch.arange(M), indexing='ij'))  # (2, M, M)\n",
    "        \n",
    "        # Flatten the coordinates for token indices, so coords[:, i] is the (row, col) of token i\n",
    "        coords = coords.flatten(1) # (2, M*M)\n",
    "\n",
    "        # Compute relative positions so we have the position of a token relative to another token\n",
    "        # coords[:, :, None] shape: (2, M*M, 1), coords[:, None, :] shape: (2, 1, M*M)\n",
    "        relative = coords[:, :, None] - coords[:, None, :] # (2, M*M, M*M)\n",
    "\n",
    "        # Reformat so we can use each (row, col) as an index into a table but row/col values\n",
    "        # range from -(M-1) to (M-1) so we shift them up so they are positive: [0, 2M -2]\n",
    "        relative = relative.permute(1, 2, 0) # (M*M, M*M, 2)\n",
    "        relative[:, :, 0] += M - 1\n",
    "        relative[:, :, 1] += M - 1\n",
    "\n",
    "        # Flatten 2D positions into 1D. To convert: row * num_cols + col\n",
    "        self.register_buffer(   # Register buffer to move to GPU\n",
    "            \"relative_index\",\n",
    "            (relative[:, :, 0] * (2*M - 1) + relative[:, :, 1]).long() # (M*M, M*M)\n",
    "        )\n",
    "    def forward(self): \n",
    "        # Use index to get bias values, look up the bias vector for each token pair\n",
    "        bias = self.relative_table[self.relative_index.view(-1)] # (M*M * M*M, nheads)\n",
    "        bias = bias.reshape(self.relative_index.shape[0], self.relative_index.shape[1], self.nheads)\n",
    "        return bias.permute(2, 0, 1) # (nheads, M*M, M*M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c69239a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window-based Multihead Self-Attention with relative position bias.\n",
    "\n",
    "    Performs self-attention within non overlapping MxM windows of the input feature map.\n",
    "    It incorporates relative positional encoding and attention masks for shifted windows\n",
    "\n",
    "    Attention(Q, K, V) = Softmax((QKᵀ / √d) + B + mask) @ V\n",
    "\n",
    "    Args:\n",
    "        channels: Input channels\n",
    "        M: Height and width of the attention window.\n",
    "        nheads: Number of attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        q, k, v: Linear layers for queries, keys, and values.\n",
    "        out: Output linear layer after attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, M, nheads):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.nheads = nheads\n",
    "        self.rootd = (channels // nheads) ** -0.5 # (1 / √d) == (1 / √dim_per_head)\n",
    "\n",
    "        self.q = nn.Linear(channels, channels)\n",
    "        self.k = nn.Linear(channels, channels)\n",
    "        self.v = nn.Linear(channels, channels)\n",
    "        self.out = nn.Linear(channels, channels)\n",
    "\n",
    "        self.relative = RelativePositionBias(M, nheads)\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for window based self-attention.\n",
    "\n",
    "        Args:\n",
    "            x: shape (B * nW, M*M, channels) where nW is number of windows\n",
    "            attn_mask: Attention mask used for shifted windows to prevent cross-window attention.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (B*nW, M*M, channels)\n",
    "        \"\"\"\n",
    "        # x shape = (B * nW, M*M, channels) where nW is number of windows\n",
    "        BnW, M_sq, channels = x.shape # M_sq is M*M\n",
    "\n",
    "        # d stands for dim_per_head. Permute on k so no transpose when computing attn.\n",
    "        # q: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> transpose(1, 2) --> (BnW, heads, M*M, d)\n",
    "        # k: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> permute(0, 2, 3, 1) --> (BnW, heads, d, M*M)\n",
    "        # v: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> transpose(1, 2) --> (BnW, heads, M*M, d)\n",
    "        q = self.q(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).transpose(1, 2)\n",
    "        k = self.k(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).permute(0, 2, 3, 1)\n",
    "        v = self.v(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).transpose(1, 2)\n",
    "\n",
    "        # Attention: (Q @ Kᵀ) / √d + relative bias + optional mask\n",
    "        # k is already transposed.\n",
    "        attn = (q @ k) * self.rootd # (BnW, nheads, M*M, M*M)\n",
    "        attn = attn + self.relative()\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (M*M, M*M) --> unsqueeze(0) --> (1, M*M, M*M)\n",
    "            # Broadcasted to (BnW, nheads, M*M, M*M)\n",
    "            attn = attn + attn_mask.unsqueeze(0)\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        out = (attn @ v).transpose(1, 2).reshape(BnW, M_sq, channels)\n",
    "        return self.out(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b163964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(H, W, M, shift):\n",
    "    if shift == 0:\n",
    "        return None\n",
    "    \n",
    "    img_mask = torch.zeros((1, H, W, 1))  # Mask\n",
    "\n",
    "    count = 0\n",
    "    H, W = img_mask.shape[1:3]\n",
    "\n",
    "    # Split image into 9 regions\n",
    "    h_ranges = [(0, H - M), (H - M, H - shift), (H - shift, H)]\n",
    "    w_ranges = [(0, W - M), (W - M, W - shift), (W - shift, W)]\n",
    "\n",
    "    # so if H = W = 12, M = 6, shift = 3\n",
    "    # h_ranges = [(0, 6), (6, 9), (9, 12)]\n",
    "    # w_ranges = [(0, 6), (6, 9), (9, 12)]\n",
    "\n",
    "    # Fill each region with a unique integer\n",
    "    for h_start, h_end in h_ranges:\n",
    "        for w_start, w_end in w_ranges:\n",
    "            img_mask[:, h_start:h_end, w_start:w_end, :] = count\n",
    "            count += 1\n",
    "            \n",
    "    # Cyclic shift the mask\n",
    "    img_mask = torch.roll(img_mask, shifts=(-shift, -shift), dims=(1,2))\n",
    "\n",
    "    # Split into M*M windows\n",
    "    mask_windows = split_into_windows(img_mask, M)  # (nW, M*M, 1)\n",
    "    mask_windows = mask_windows.squeeze(-1)       # (nW, M*M)\n",
    "    # Create attention mask\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)  # (nW, M*M, M*M)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float('-inf')).masked_fill(attn_mask == 0, 0.0)\n",
    "    return attn_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00091934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_attention_mask(12, 12, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ce04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cddfcc8c",
   "metadata": {},
   "source": [
    "Input image: B × 3 × H × W\n",
    "\n",
    "Patch size: P = 4\n",
    "\n",
    "Window size: M = 7\n",
    "\n",
    "Base embedding dim (stage 1): C = 96 (Swin-T)\n",
    "\n",
    "Depths per stage (Swin-T): [2, 2, 6, 2]\n",
    "\n",
    "Heads per stage: [3, 6, 12, 24] ⇒ per-head dim is ≈ 32\n",
    "\n",
    "MLP expansion: α = 4\n",
    "\n",
    "Shift size in SW-MSA: s = M // 2 = 3 (for M=7)\n",
    "\n",
    "After patch embed, the feature map resolution is H' = H/P, W' = W/P."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
