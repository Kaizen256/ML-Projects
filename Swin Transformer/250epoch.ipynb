{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33249c62",
   "metadata": {
    "papermill": {
     "duration": 0.006049,
     "end_time": "2025-08-17T03:57:18.623551",
     "exception": false,
     "start_time": "2025-08-17T03:57:18.617502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation and Augmentation\n",
    "\n",
    "Data augmentation techniques include RandomResizedCrop, RandomHorizontalFlip, RandAugment (applies random augmentations), and RandomErasing (randomly erases a rectangular region in an image). The pixel values are also normalized using the mean and standard deviation of the dataset. I found this in my ResNet-34 from scratch project.\n",
    "\n",
    "Dataset is then split into a 90% training set and a 10% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e25cb8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T03:57:18.634899Z",
     "iopub.status.busy": "2025-08-17T03:57:18.634635Z",
     "iopub.status.idle": "2025-08-17T03:58:49.713530Z",
     "shell.execute_reply": "2025-08-17T03:58:49.712792Z"
    },
    "papermill": {
     "duration": 91.090494,
     "end_time": "2025-08-17T03:58:49.719673",
     "exception": false,
     "start_time": "2025-08-17T03:57:18.629179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reorganized validation images → /kaggle/working/tiny-imagenet-200-val\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, csv, pathlib\n",
    "\n",
    "ROOT = \"/kaggle/input/tiny-imagenet-200/tiny-imagenet-200\"\n",
    "VAL_DIR = os.path.join(ROOT, \"val\")\n",
    "VAL_ANN = os.path.join(VAL_DIR, \"val_annotations.txt\")\n",
    "VAL_IMAGES = os.path.join(VAL_DIR, \"images\")\n",
    "\n",
    "# Where to build an ImageFolder-compatible val/ structure\n",
    "OUT_VAL = \"/kaggle/working/tiny-imagenet-200-val\"\n",
    "\n",
    "os.makedirs(OUT_VAL, exist_ok=True)\n",
    "\n",
    "# Build mapping: filename -> wnid (class folder)\n",
    "fname_to_wnid = {}\n",
    "with open(VAL_ANN, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        fname, wnid = row[0], row[1]\n",
    "        fname_to_wnid[fname] = wnid\n",
    "\n",
    "# Copy images into OUT_VAL/<wnid>/<filename>\n",
    "for fname, wnid in fname_to_wnid.items():\n",
    "    src = os.path.join(VAL_IMAGES, fname)\n",
    "    dst_dir = os.path.join(OUT_VAL, wnid)\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    dst = os.path.join(dst_dir, fname)\n",
    "    if not os.path.exists(dst):\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "print(\"Reorganized validation images →\", OUT_VAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "840c4bf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T03:58:49.730445Z",
     "iopub.status.busy": "2025-08-17T03:58:49.730208Z",
     "iopub.status.idle": "2025-08-17T04:05:48.459471Z",
     "shell.execute_reply": "2025-08-17T04:05:48.458743Z"
    },
    "papermill": {
     "duration": 418.73674,
     "end_time": "2025-08-17T04:05:48.461084",
     "exception": false,
     "start_time": "2025-08-17T03:58:49.724344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "image_size = 64\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    RandAugment(num_ops=2, magnitude=5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random'),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "])\n",
    "\n",
    "train_dir = os.path.join(ROOT, \"train\")\n",
    "val_dir   = OUT_VAL\n",
    "\n",
    "train_set = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_set   = datasets.ImageFolder(val_dir,   transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=64, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e33c863",
   "metadata": {
    "papermill": {
     "duration": 0.00462,
     "end_time": "2025-08-17T04:05:48.471239",
     "exception": false,
     "start_time": "2025-08-17T04:05:48.466619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Overwrote train transforms with validation transforms, removing augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72926acc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:48.482481Z",
     "iopub.status.busy": "2025-08-17T04:05:48.481686Z",
     "iopub.status.idle": "2025-08-17T04:05:48.986414Z",
     "shell.execute_reply": "2025-08-17T04:05:48.984959Z"
    },
    "papermill": {
     "duration": 0.512514,
     "end_time": "2025-08-17T04:05:48.988415",
     "exception": false,
     "start_time": "2025-08-17T04:05:48.475901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val batch images shape: torch.Size([64, 3, 64, 64])\n",
      "Val batch labels shape: torch.Size([64])\n",
      "Sample labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(val_loader))\n",
    "\n",
    "print(\"Val batch images shape:\", xb.shape)   # should be (B, 3, H, W)\n",
    "print(\"Val batch labels shape:\", yb.shape)   # should be (B,)\n",
    "print(\"Sample labels:\", yb[:10].tolist())\n",
    "\n",
    "# Verify ranges and types\n",
    "assert xb.ndim == 4 and xb.size(1) == 3, \"Images should have shape (B, 3, H, W)\"\n",
    "assert torch.is_floating_point(xb), \"Images should be float tensors\"\n",
    "assert yb.ndim == 1 and yb.dtype == torch.long, \"Labels should be 1D LongTensor\"\n",
    "assert xb.min() >= -5 and xb.max() <= 5, \"Values look off; check normalization\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b52a4",
   "metadata": {
    "papermill": {
     "duration": 0.005381,
     "end_time": "2025-08-17T04:05:48.999435",
     "exception": false,
     "start_time": "2025-08-17T04:05:48.994054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Patching and Splitting Windows\n",
    "Unlike traditional CNNs that use sliding convolutional filters, Vision Transformers break the image down into a sequence of patches, treating them similarly to words in a sentence.\n",
    "\n",
    "Patchify Class: Takes an input image and converts it into patch embeddings. It uses a single convolutional layer where the kernel size and stride are equal to the patch_size. It divides the image into non overlapping patches and creating an initial vector embedding for each one.\n",
    "\n",
    "split_into_windows: Takes the patches and splits them into smaller windows. Self-attention is calculated within these windows, which is far more computationally efficient than the original ViT's approach of global attention across all patches.\n",
    "\n",
    "reverse_windows: Merges the split windows back into their original spatial layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e2fd97d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.011662Z",
     "iopub.status.busy": "2025-08-17T04:05:49.011326Z",
     "iopub.status.idle": "2025-08-17T04:05:49.017793Z",
     "shell.execute_reply": "2025-08-17T04:05:49.016873Z"
    },
    "papermill": {
     "duration": 0.014456,
     "end_time": "2025-08-17T04:05:49.019376",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.004920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Patchify(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert an image into patch embeddings using a convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        in_channels: Number of input channels (e.g., 3 for RGB).\n",
    "        embed_dim: Output embedding dimension per patch.\n",
    "        patch_size: Size of each square patch\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS, H//patch_size, W//patch_size, embed_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, embed_dim, patch_size):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (B, 3, H, W)\n",
    "        x = self.conv(x)         # (BS, embed_dim, H//patch_size, W//patch_size)\n",
    "        return x.permute(0, 2, 3, 1) # (BS, H//patch_size, W//patch_size, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1b5610a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.034110Z",
     "iopub.status.busy": "2025-08-17T04:05:49.033829Z",
     "iopub.status.idle": "2025-08-17T04:05:49.037976Z",
     "shell.execute_reply": "2025-08-17T04:05:49.037459Z"
    },
    "papermill": {
     "duration": 0.011028,
     "end_time": "2025-08-17T04:05:49.039189",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.028161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_into_windows(x, M):\n",
    "    \"\"\"\n",
    "    Splits (BS, H, W, channels) into non overlapping MxM windows.\n",
    "    Args:\n",
    "        x: Tensor of shape (BS, H, W, channels)\n",
    "        M: Window size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS * num_windows, M*M, channels)\n",
    "    \"\"\"\n",
    "    BS, H, W, channels = x.shape\n",
    "    x = x.reshape(BS, H//M, M, W//M, M, channels)\n",
    "    # Permute fixes the order so the MxM pixels are together properly\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5)     # (BS, H//M, W//M, M, M, channels)\n",
    "    return x.reshape(-1, M*M, channels) # (BS * num_windows, M*M, channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09c08328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.050168Z",
     "iopub.status.busy": "2025-08-17T04:05:49.049935Z",
     "iopub.status.idle": "2025-08-17T04:05:49.054606Z",
     "shell.execute_reply": "2025-08-17T04:05:49.053824Z"
    },
    "papermill": {
     "duration": 0.011457,
     "end_time": "2025-08-17T04:05:49.055737",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.044280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reverse_windows(x, M, H, W, channels):\n",
    "    \"\"\"\n",
    "    Reverses MxM window tokens back into the original layout.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape (BS * num_windows, M*M, channels)\n",
    "        M: Window size\n",
    "        H: Original image height\n",
    "        W: Original image width\n",
    "        channels: Number of channels\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS, H, W, channels)\n",
    "    \"\"\"\n",
    "    BS = x.shape[0] // (H//M * W//M) # Original BS\n",
    "    x = x.reshape(BS, H//M, W//M, M, M, channels)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5)      # (BS, H//M, M, W//M, M, channels)\n",
    "    return x.reshape(BS, H, W, channels) # (BS, H, W, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714aac44",
   "metadata": {
    "papermill": {
     "duration": 0.004467,
     "end_time": "2025-08-17T04:05:49.064952",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.060485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Relative Position Bias\n",
    "\n",
    "Relative Position Bias shows self-attention mechanism about the geometry of the image. It computes a learnable bias between every pair of tokens in a window, based on how far apart they are. For each pair of tokens in an M x M window, compute relative position, use that to index into a learnable bias table, add this bias to the attention logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1868115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.075844Z",
     "iopub.status.busy": "2025-08-17T04:05:49.075604Z",
     "iopub.status.idle": "2025-08-17T04:05:49.083127Z",
     "shell.execute_reply": "2025-08-17T04:05:49.082397Z"
    },
    "papermill": {
     "duration": 0.014365,
     "end_time": "2025-08-17T04:05:49.084197",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.069832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes relative position bias for self-attention.\n",
    "\n",
    "    Generates a table of learnable relative position biases between token pairs\n",
    "    within an attention window of shape (M x M). The relative position between\n",
    "    any two tokens is encoded as a bias vector per attention head, and these biases are\n",
    "    added to the attention scores in self-attention.\n",
    "\n",
    "    Args:\n",
    "        M: The height/width of the attention window. The total number of tokens is M * M.\n",
    "        nheads: Number of attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        relative_table: Learnable parameter of shape ((2M - 1)^2, nheads),\n",
    "            where each entry represents a bias value for a specific relative position and head.\n",
    "        relative_index: Lookup table of shape (M*M, M*M), where each entry is an index\n",
    "            into relative_table that maps the relative position between two tokens to a bias vector.\n",
    "\n",
    "    Forward Output:\n",
    "        Tensor of shape (nheads, M*M, M*M) containing the relative bias for each token pair\n",
    "        and each attention head. This can be directly added to attention logits.\n",
    "\n",
    "    Example:\n",
    "        relative = RelativePositionBias(M=3, nheads=4)\n",
    "        bias = relative()  # Output shape: (4, 9, 9), for 4 heads and 3x3 tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, M, nheads):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.nheads = nheads\n",
    "        # (2M-1)^2 because there are up to M-1 tokens above or below or left or right of each token.\n",
    "        # 2M - 1 possibilities for above and below, same for left and right. So (2M -1)^2 total.\n",
    "        # If M = 3 row and col would go between -2 and +2 when comparing two tokens.\n",
    "        # That gives 5^2 possible combinations. len(-2, -1, 0, 1, 2)^2\n",
    "        self.relative_table = nn.Parameter(torch.zeros(size=((2*M - 1) * (2*M -1), nheads)))\n",
    "\n",
    "        # Coordinate grid of token positions shows where each token is in the window.\n",
    "        # It gives every token a (row, col) coordinate.\n",
    "        # If M = 3: coords[0] (rows): [[0, 0, 0], [1, 1, 1], [2, 2, 2]], \n",
    "        #           coords[1] (cols): [[0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
    "        coords = torch.stack(torch.meshgrid( # Matrix style over Cartesian\n",
    "            torch.arange(M), torch.arange(M), indexing='ij'))  # (2, M, M)\n",
    "        \n",
    "        # Flatten the coordinates for token indices, so coords[:, i] is the (row, col) of token i\n",
    "        coords = coords.flatten(1) # (2, M*M)\n",
    "\n",
    "        # Compute relative positions so we have the position of a token relative to another token\n",
    "        # coords[:, :, None] shape: (2, M*M, 1), coords[:, None, :] shape: (2, 1, M*M)\n",
    "        relative = coords[:, :, None] - coords[:, None, :] # (2, M*M, M*M)\n",
    "\n",
    "        # Reformat so we can use each (row, col) as an index into a table but row/col values\n",
    "        # range from -(M-1) to (M-1) so we shift them up so they are positive: [0, 2M -2]\n",
    "        relative = relative.permute(1, 2, 0) # (M*M, M*M, 2)\n",
    "        relative[:, :, 0] += M - 1\n",
    "        relative[:, :, 1] += M - 1\n",
    "\n",
    "        # Flatten 2D positions into 1D. To convert: row * num_cols + col\n",
    "        self.register_buffer(   # Register buffer to move to GPU\n",
    "            \"relative_index\",\n",
    "            (relative[:, :, 0] * (2*M - 1) + relative[:, :, 1]).long() # (M*M, M*M)\n",
    "        )\n",
    "    def forward(self): \n",
    "        # Use index to get bias values, look up the bias vector for each token pair\n",
    "        bias = self.relative_table[self.relative_index.view(-1)] # (M*M * M*M, nheads)\n",
    "        bias = bias.reshape(self.relative_index.shape[0], self.relative_index.shape[1], self.nheads)\n",
    "        return bias.permute(2, 0, 1) # (nheads, M*M, M*M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7cb00c",
   "metadata": {
    "papermill": {
     "duration": 0.004629,
     "end_time": "2025-08-17T04:05:49.093454",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.088825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Windowed Multi-Head Self-Attention (W-MSA)\n",
    "\n",
    "Implements Windowed Multi-head Self-Attention, which is a more efficient version of the standard attention used in ViT. Instead of calculating attention across all patches in the entire image, W-MSA computes attention within M x M windows. Significantly reduces the number of calculations needed.\n",
    "\n",
    "Attention(Q, K, V) = Softmax((QK.T / √d) + B + mask) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e16ec031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.103829Z",
     "iopub.status.busy": "2025-08-17T04:05:49.103592Z",
     "iopub.status.idle": "2025-08-17T04:05:49.111674Z",
     "shell.execute_reply": "2025-08-17T04:05:49.110931Z"
    },
    "papermill": {
     "duration": 0.014641,
     "end_time": "2025-08-17T04:05:49.112801",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.098160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window-based Multihead Self-Attention with relative position bias.\n",
    "\n",
    "    Performs self-attention within non overlapping MxM windows of the input feature map.\n",
    "    It incorporates relative positional encoding and attention masks for shifted windows\n",
    "\n",
    "    Attention(Q, K, V) = Softmax((QK.T / √d) + B + mask) @ V\n",
    "    B is relative position bias.\n",
    "    Args:\n",
    "        channels: Input channels\n",
    "        M: Height and width of the attention window.\n",
    "        nheads: Number of attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        q, k, v: Linear layers for queries, keys, and values.\n",
    "        out: Output linear layer after attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, M, nheads):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.nheads = nheads\n",
    "        self.rootd = (channels // nheads) ** -0.5 # (1 / √d) == (1 / √dim_per_head)\n",
    "\n",
    "        self.q = nn.Linear(channels, channels)\n",
    "        self.k = nn.Linear(channels, channels)\n",
    "        self.v = nn.Linear(channels, channels)\n",
    "        self.out = nn.Linear(channels, channels)\n",
    "\n",
    "        self.relative = RelativePositionBias(M, nheads)\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for window based self-attention.\n",
    "\n",
    "        Args:\n",
    "            x: shape (B * nW, M*M, channels) where nW is number of windows\n",
    "            attn_mask: Attention mask used for shifted windows to prevent cross-window attention.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (B*nW, M*M, channels)\n",
    "        \"\"\"\n",
    "        # x shape = (B * nW, M*M, channels) where nW is number of windows\n",
    "        BnW, M_sq, channels = x.shape # M_sq is M*M\n",
    "\n",
    "        # d stands for dim_per_head. Permute on k so no transpose when computing attn.\n",
    "        # q: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> transpose(1, 2) --> (BnW, heads, M*M, d)\n",
    "        # k: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> permute(0, 2, 3, 1) --> (BnW, heads, d, M*M)\n",
    "        # v: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> transpose(1, 2) --> (BnW, heads, M*M, d)\n",
    "        q = self.q(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).transpose(1, 2)\n",
    "        k = self.k(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).permute(0, 2, 3, 1)\n",
    "        v = self.v(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).transpose(1, 2)\n",
    "\n",
    "        # Attention: (Q @ K.T) / √d + relative bias + optional mask\n",
    "        # k is already transposed.\n",
    "        attn = (q @ k) * self.rootd # (BnW, nheads, M*M, M*M)\n",
    "        attn = attn + self.relative()\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (M*M, M*M) --> unsqueeze(0) --> (1, M*M, M*M)\n",
    "            # Broadcasted to (BnW, nheads, M*M, M*M)\n",
    "            nW = attn_mask.shape[0]\n",
    "            BS = BnW // nW\n",
    "            attn_mask = attn_mask.to(attn.device)\n",
    "            attn = attn.view(BS, nW, self.nheads, M_sq, M_sq) + attn_mask.unsqueeze(0).unsqueeze(2)\n",
    "            attn = attn.view(BnW, self.nheads, M_sq, M_sq)\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(BnW, M_sq, channels)\n",
    "        return self.out(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eda2f08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.123227Z",
     "iopub.status.busy": "2025-08-17T04:05:49.122963Z",
     "iopub.status.idle": "2025-08-17T04:05:49.129612Z",
     "shell.execute_reply": "2025-08-17T04:05:49.128876Z"
    },
    "papermill": {
     "duration": 0.013141,
     "end_time": "2025-08-17T04:05:49.130808",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.117667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_attention_mask(H, W, M, shift):\n",
    "    \"\"\"\n",
    "    Creates an attention mask for shifted window self-attention (SW-MSA).\n",
    "\n",
    "    This function generates a mask to prevent tokens from attending across windows when \n",
    "    performing SW-MSA. It divides the feature map into distinct regions, assigns unique labels\n",
    "    to each, uses cyclic shifting, partitions it into non overlapping windows, and then\n",
    "    builds an attention mask that blocks attention between different labeled regions.\n",
    "\n",
    "    Args:\n",
    "        H: Height of the feature map.\n",
    "        W: Width of the feature map.\n",
    "        M: Window size.\n",
    "        shift: Number of pixels to cyclically shift the window. \n",
    "               If shift is 0, no mask is needed. In Swin it is M // 2\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (nW, M*M, M*M) where nW is the number of windows.\n",
    "        Or None if shift is 0.\n",
    "    \"\"\"\n",
    "    if shift == 0:\n",
    "        return None\n",
    "    \n",
    "    img_mask = torch.zeros((1, H, W, 1))  # Mask\n",
    "\n",
    "    count = 0\n",
    "    H, W = img_mask.shape[1:3]\n",
    "\n",
    "    # Split image into 9 regions\n",
    "    h_ranges = [(0, H - M), (H - M, H - shift), (H - shift, H)]\n",
    "    w_ranges = [(0, W - M), (W - M, W - shift), (W - shift, W)]\n",
    "\n",
    "    # so if H = W = 12, M = 6, shift = 3\n",
    "    # h_ranges = [(0, 6), (6, 9), (9, 12)]\n",
    "    # w_ranges = [(0, 6), (6, 9), (9, 12)]\n",
    "\n",
    "    # Fill each region with a unique integer\n",
    "    for h_start, h_end in h_ranges:\n",
    "        for w_start, w_end in w_ranges:\n",
    "            img_mask[:, h_start:h_end, w_start:w_end, :] = count\n",
    "            count += 1\n",
    "            \n",
    "    # Cyclic shift the mask\n",
    "    img_mask = torch.roll(img_mask, shifts=(-shift, -shift), dims=(1,2))\n",
    "\n",
    "    # Split into M*M windows\n",
    "    mask_windows = split_into_windows(img_mask, M)  # (nW, M*M, 1)\n",
    "    mask_windows = mask_windows.squeeze(-1)       # (nW, M*M)\n",
    "    # Create attention mask\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)  # (nW, M*M, M*M)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float('-inf')).masked_fill(attn_mask == 0, 0.0)\n",
    "    return attn_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af748d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.142070Z",
     "iopub.status.busy": "2025-08-17T04:05:49.141532Z",
     "iopub.status.idle": "2025-08-17T04:05:49.188328Z",
     "shell.execute_reply": "2025-08-17T04:05:49.187657Z"
    },
    "papermill": {
     "duration": 0.053703,
     "end_time": "2025-08-17T04:05:49.189460",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.135757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_attention_mask(12, 12, 6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc48f8b2",
   "metadata": {
    "papermill": {
     "duration": 0.005206,
     "end_time": "2025-08-17T04:05:49.199820",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.194614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stochastic Depth\n",
    "\n",
    "Stochastic depth is a regularization technique used to improve generalization and reduce overfitting. Instead of dropping individual neurons, entire residual branches are skipped during training with a given probability (drop_prob)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ab2a28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.211108Z",
     "iopub.status.busy": "2025-08-17T04:05:49.210520Z",
     "iopub.status.idle": "2025-08-17T04:05:49.215598Z",
     "shell.execute_reply": "2025-08-17T04:05:49.215081Z"
    },
    "papermill": {
     "duration": 0.011754,
     "end_time": "2025-08-17T04:05:49.216674",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.204920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stochastic_depth(x, drop_prob, training):\n",
    "    \"\"\"\n",
    "    Applies stochastic depth to the input.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        drop_prob: Probability of dropping the path.\n",
    "        training: If True, stochastic depth is applied. If False, input is returned unchanged.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor with some residual paths zeroed out.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "\n",
    "    keep_prob = 1.0 - drop_prob\n",
    "    # Create mask with shape (BS, 1, 1, ..., 1) so it broadcasts over all non batch dims\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "    # 1 with prob keep_prob, 0 with prob drop_prob\n",
    "    mask = torch.rand(shape, dtype=x.dtype, device=x.device) < keep_prob\n",
    "    # Scale\n",
    "    x = x / keep_prob\n",
    "    return x * mask\n",
    "\n",
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"\n",
    "    Module for stochastic depth.\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob):\n",
    "        super().__init__()\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return stochastic_depth(x, self.drop_prob, self.training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09c7cf",
   "metadata": {
    "papermill": {
     "duration": 0.004787,
     "end_time": "2025-08-17T04:05:49.226349",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.221562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SwinBlock (Shifted Window Transformer Block)\n",
    "\n",
    "Applies windowed self-attention over non overlapping M×M windows and alternates between non-shifted and shifted windows across consecutive blocks to enable cross-window connections. There is also an MLP with GELU at the end. Residual connections are used.\n",
    "\n",
    "![Block Architecture](figures/block_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b562dc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.237086Z",
     "iopub.status.busy": "2025-08-17T04:05:49.236797Z",
     "iopub.status.idle": "2025-08-17T04:05:49.245010Z",
     "shell.execute_reply": "2025-08-17T04:05:49.244516Z"
    },
    "papermill": {
     "duration": 0.015002,
     "end_time": "2025-08-17T04:05:49.246222",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.231220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer block with shifted/non-shifted window self-attention + MLP.\n",
    "\n",
    "    Args:\n",
    "        dim: Channel dimension of the input features.\n",
    "        H: Feature map height.\n",
    "        W: Feature map width.\n",
    "        nheads: Number of attention heads in WindowAttention.\n",
    "        M: Window size.\n",
    "        shift: Cyclic shift size (0 for non-shifted windows, M//2 for shifted).\n",
    "        ratio: Expansion ratio for the MLP hidden size (hidden dim = ratio * dim).\n",
    "        stoch_depth: stochastic depth probability for dropping residual branches.\n",
    "\n",
    "    Attributes:\n",
    "        norm1: Pre attention normalization.\n",
    "        attn (WindowAttention): Window-based multi-head self-attention.\n",
    "        drop_path: Stochastic depth module or identity if stoch_depth == 0.\n",
    "        norm2: Pre MLP normalization.\n",
    "        mlp: Two-layer feed forward network with GELU activation.\n",
    "        attn_mask: Mask for shifted attention, shape (nW, M*M, M*M) when shift > 0, else None.\n",
    "\n",
    "    Input:\n",
    "        x: Tensor of shape (BS, H, W, channels).\n",
    "\n",
    "    Output:\n",
    "        Tensor of shape (BS, H, W, Channels), same spatial shape and channels as input.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, H, W, nheads, M, shift, ratio, stoch_depth):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "        self.shift = shift\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(channels=dim, M=M, nheads=nheads)\n",
    "        if stoch_depth > 0:\n",
    "            self.stoch = StochasticDepth(stoch_depth)\n",
    "        else:\n",
    "            self.stoch = nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * ratio)), # Error I had before, forgot to wrap with int()\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * ratio), dim)\n",
    "        )\n",
    "        self.attn_mask = create_attention_mask(H, W, M, shift)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of SwinBlock.\n",
    "\n",
    "        LayerNorm --> cyclic shift --> split windows --> WindowAttention (masked if shifted)\n",
    "        --> reverse windows --> reverse shift --> residual + stochastic depth --> LayerNorm\n",
    "        --> MLP → residual +stochastic depth\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (BS, H, W, channels).\n",
    "        \"\"\"\n",
    "        BS, H, W, channels = x.shape\n",
    "\n",
    "        store = x # For Residual connection\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        if self.shift > 0:\n",
    "            x = torch.roll(x, shifts=(-self.shift, -self.shift), dims=(1, 2)) # Cyclic shift\n",
    "        x_windows = split_into_windows(x, self.M)                  # (BS*nW, M*M, channels)\n",
    "        x_windows = self.attn(x_windows, attn_mask=self.attn_mask) # (BS*nW, M*M, channels)\n",
    "        x = reverse_windows(x_windows, self.M, H, W, channels)     # (BS, H, W, channels)\n",
    "        if self.shift > 0:\n",
    "            x = torch.roll(x, shifts=(self.shift, self.shift), dims=(1, 2))\n",
    "\n",
    "        x = store + self.stoch(x)\n",
    "\n",
    "        return x + self.stoch(self.mlp(self.norm2(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ce570",
   "metadata": {
    "papermill": {
     "duration": 0.005253,
     "end_time": "2025-08-17T04:05:49.256435",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.251182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Patch Merging Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af333ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.270878Z",
     "iopub.status.busy": "2025-08-17T04:05:49.270155Z",
     "iopub.status.idle": "2025-08-17T04:05:49.277717Z",
     "shell.execute_reply": "2025-08-17T04:05:49.276836Z"
    },
    "papermill": {
     "duration": 0.017606,
     "end_time": "2025-08-17T04:05:49.279355",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.261749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    \"\"\"\n",
    "    Reduces H and W by 2x in each dimension. Doubles the channel dimension.\n",
    "    - Extract non overlapping 2x2 patches from the feature map.\n",
    "    - Concatenate features from each patch along the channel dimension.\n",
    "    - Apply LayerNorm for normalization across channels.\n",
    "    - Lower 4*channels down to 2*channels with a Linear layer.\n",
    "\n",
    "    Input:\n",
    "        x: shape (BS, H, W, channels)\n",
    "\n",
    "    Output:\n",
    "        shape (BS, H/2, W/2, 2*channels)\n",
    "\n",
    "    H and W should be even.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4*dim, 2*dim)\n",
    "        self.norm = nn.LayerNorm(4*dim) # 4 * channels --> 2 * channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (BS, H, W, channels)\n",
    "        x1 = x[:, 0::2, 0::2, :]\n",
    "        x2 = x[:, 1::2, 0::2, :]\n",
    "        x3 = x[:, 0::2, 1::2, :]\n",
    "        x4 = x[:, 1::2, 1::2, :]\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=3) # (BS, H/2, W/2, 4*channels)\n",
    "        x = self.norm(x)\n",
    "        return self.lin(x) # (BS, H/2, W/2, 2*channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76fc402",
   "metadata": {
    "papermill": {
     "duration": 0.005351,
     "end_time": "2025-08-17T04:05:49.294390",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.289039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stage (Stack of Swin Blocks + Optional Patch Merging)\n",
    "\n",
    "A Stage stacks SwinBlocks, alternating between:\n",
    "- W-MSA (non-shifted windows, shift=0)\n",
    "- SW-MSA (shifted windows, shift=M//2)\n",
    "\n",
    "Then optionally applies PatchMerging to downsample and increase channels for the next stage.\n",
    "After PatchMerging, the next stage should be constructed with dim = 2 * previous_dim and H and W halved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa278456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.310518Z",
     "iopub.status.busy": "2025-08-17T04:05:49.309662Z",
     "iopub.status.idle": "2025-08-17T04:05:49.318487Z",
     "shell.execute_reply": "2025-08-17T04:05:49.317546Z"
    },
    "papermill": {
     "duration": 0.019853,
     "end_time": "2025-08-17T04:05:49.319754",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.299901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Stage(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dim: Channel dimension.\n",
    "        H: Feature map height.\n",
    "        W: Feature map width.\n",
    "        M: Window size.\n",
    "        blocks: Number of SwinBlocks in this stage.\n",
    "        nheads: Number of attention heads per block.\n",
    "        stoch_depth_list: List of stochastic depth probabilities (len == blocks).\n",
    "        patch_merging: If True, apply PatchMerging at the end of the stage.\n",
    "        ratio: MLP expansion ratio (hidden dim = ratio * dim).\n",
    "\n",
    "    Input:\n",
    "        x: Tensor of shape (BS, H, W, channels or dim)\n",
    "\n",
    "    Output:\n",
    "        - If patch_merging is False: (BS, H, W, dim)\n",
    "        - If patch_merging is True:  (BS, H/2, W/2, 2*dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, H, W, M, blocks, nheads, stoch_depth_list, patch_merging, ratio):\n",
    "        # stoch_depth_list is a list of the stochastic depth rates for each SwinBlock.\n",
    "\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(blocks):\n",
    "            if i % 2 == 0:\n",
    "                shift = 0  # Alternate between W-MSA and SW-MSA, W-MSA has no shift.\n",
    "            else:\n",
    "                shift = M // 2\n",
    "            self.blocks.append(\n",
    "                SwinBlock(dim, H, W, nheads, M, shift, ratio, stoch_depth_list[i])\n",
    "            )\n",
    "        if patch_merging:\n",
    "            self.patch = PatchMerging(dim)\n",
    "        else:\n",
    "            self.patch = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (BS, H, W, channels)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.patch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce2ab9a",
   "metadata": {
    "papermill": {
     "duration": 0.005231,
     "end_time": "2025-08-17T04:05:49.330242",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.325011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameters\n",
    "\n",
    "- Patch size: 4 × 4\n",
    "- Base embed dim C: 96\n",
    "- Depths: [2, 2, 6, 2]\n",
    "- Num heads: [3, 6, 12, 24]\n",
    "- Window size: 7 for all blocks\n",
    "- Shift: 3 (7 // 2) for all shift blocks\n",
    "- MLP expantion ratio: 4.0 for all blocks\n",
    "- Drop path rate: 0.2 (linearly increased across all blocks)\n",
    "- Patch Merging / Downsample at the end of Stage 1, 2 and 3.\n",
    "\n",
    "\n",
    "| Stage         | Blocks | Heads | Stoch_dep | In Channels | Out Channels | Output Shape             |\n",
    "|---------------|--------|-------|-----------|-------------|--------------|--------------------------|\n",
    "| PatchEmbed    | None   | None  | None           | 3           | 96           | (BS, H/4, W/4, 96)       |\n",
    "| Stage 1       | 2      | 3     | [0.0000, 0.0182]          | 96          | 192          | (BS, H/8, W/8, 192)      |\n",
    "| Stage 2       | 2      | 6     | [0.0364, 0.0545]          | 192         | 384          | (BS, H/16, W/16, 384)    |\n",
    "| Stage 3       | 6      | 12    | [0.0727, 0.0909, 0.1091, 0.1273, 0.1455, 0.1636]          | 384         | 768          | (BS, H/32, W/32, 768)    |\n",
    "| Stage 4       | 2      | 24    | [0.1818, 0.2000]          | 768         | 768          | (BS, H/32, W/32, 768)    |\n",
    "| Head          | None   | None  | None          | 768         | num_classes  | (BS, num_classes)        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57452d",
   "metadata": {
    "papermill": {
     "duration": 0.004434,
     "end_time": "2025-08-17T04:05:49.339649",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.335215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Swin Transformer Architecture\n",
    "\n",
    "![Architecture](figures/Architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48aa7393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.350347Z",
     "iopub.status.busy": "2025-08-17T04:05:49.349859Z",
     "iopub.status.idle": "2025-08-17T04:05:49.358768Z",
     "shell.execute_reply": "2025-08-17T04:05:49.358215Z"
    },
    "papermill": {
     "duration": 0.015387,
     "end_time": "2025-08-17T04:05:49.359872",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.344485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer\n",
    "\n",
    "    Steps:\n",
    "        - Patchify to get (BS, H/patch, W/patch, emb_dim)\n",
    "        - 4 stages of Swin blocks with alternating W-MSA (shift=0) and SW-MSA (shift=M//2)\n",
    "        - Patch Merging at the end of stages 1-3\n",
    "        - Global average pooling and linear classifier head\n",
    "\n",
    "    Args:\n",
    "        img_size: Input image size\n",
    "        patch_size: Patch size for Patchify.\n",
    "        emb_dim: Base embedding dimension channel for stage 1.\n",
    "        blocks (List[int]): Number of blocks per stage.\n",
    "        nheads (List[int]): Number of attention heads per stage.\n",
    "        M: Window size for all blocks.\n",
    "        n_classes: Number of output classes for the classifier head.\n",
    "        stochastic_endpoint: Stochastic depth ratio endpoint for linspace.\n",
    "\n",
    "    Shapes:\n",
    "        Input:  (BS, 3, 224, 224)\n",
    "        After patchify: (BS, 56, 56, 96)\n",
    "        After stage1:   (BS, 28, 28, 192)\n",
    "        After stage2:   (BS, 14, 14, 384)\n",
    "        After stage3:   (BS, 7, 7, 768)\n",
    "        After stage4:   (BS, 7, 7, 768)\n",
    "        Output logits:  (BS, n_classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, emb_dim, blocks,\n",
    "                 nheads, M, n_classes, stochastic_endpoint):\n",
    "        super().__init__()\n",
    "\n",
    "        H = img_size // patch_size\n",
    "        W = img_size // patch_size\n",
    "        dims = [emb_dim, 2*emb_dim, 4*emb_dim, 8*emb_dim]\n",
    "\n",
    "        self.patchify = Patchify(3, emb_dim, patch_size)\n",
    "\n",
    "        # Linearly increase across all blocks (inclusive endpoints)\n",
    "        stoch_depth = list(np.linspace(0, stochastic_endpoint, sum(blocks)))\n",
    "        \n",
    "        # 4 Stages with stochastic depth aligned with the number of blocks in each stage.\n",
    "        ind = 0\n",
    "        self.stage1 = Stage(\n",
    "            dims[0], H, W, M[0], \n",
    "            blocks[0], nheads[0], \n",
    "            stoch_depth[ind:ind+blocks[0]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[0]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage2 = Stage(\n",
    "            dims[1], H, W, M[1], \n",
    "            blocks[1], nheads[1], \n",
    "            stoch_depth[ind:ind+blocks[1]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[1]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage3 = Stage(\n",
    "            dims[2], H, W, M[2], \n",
    "            blocks[2], nheads[2], \n",
    "            stoch_depth[ind:ind+blocks[2]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[2]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage4 = Stage(\n",
    "            dims[3], H, W, M[3], \n",
    "            blocks[3], nheads[3], \n",
    "            stoch_depth[ind:ind+blocks[3]], \n",
    "            patch_merging=False, ratio=4.0)\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[3])\n",
    "        self.head = nn.Linear(dims[3], n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (BS, 3, img_size, img_size).\n",
    "\n",
    "        Returns:\n",
    "            Class logits of shape (BS, n_classes).\n",
    "        \"\"\"\n",
    "        # x shape = (BS, 3, 224, 224)\n",
    "        x = self.patchify(x) # (BS, H//patch_size, W//patch_size, embed_dim)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.norm(x).mean(dim=(1, 2)) # Global average pooling over H, W.\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ef746",
   "metadata": {
    "papermill": {
     "duration": 0.004968,
     "end_time": "2025-08-17T04:05:49.369784",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.364816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Soft Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f030037f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.383937Z",
     "iopub.status.busy": "2025-08-17T04:05:49.383454Z",
     "iopub.status.idle": "2025-08-17T04:05:49.392314Z",
     "shell.execute_reply": "2025-08-17T04:05:49.391520Z"
    },
    "papermill": {
     "duration": 0.018509,
     "end_time": "2025-08-17T04:05:49.393431",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.374922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def one_hot(labels: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
    "    return F.one_hot(labels, num_classes=num_classes).float()\n",
    "\n",
    "class SoftCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    If target is LongTensor -> falls back to nn.CrossEntropyLoss (hard labels).\n",
    "    If target is FloatTensor (N,C) -> computes soft CE: -sum(p * log_softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, label_smoothing: float = 0.0):\n",
    "        super().__init__()\n",
    "        # If you use Mixup/CutMix, set smoothing=0.0 to avoid double softening.\n",
    "        self.ce = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        if target.dtype == torch.long:\n",
    "            return self.ce(logits, target)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(target * log_probs).sum(dim=1).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062b780d",
   "metadata": {
    "papermill": {
     "duration": 0.00974,
     "end_time": "2025-08-17T04:05:49.408976",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.399236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mixup + CutMix module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f13b69f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.425835Z",
     "iopub.status.busy": "2025-08-17T04:05:49.425324Z",
     "iopub.status.idle": "2025-08-17T04:05:49.438098Z",
     "shell.execute_reply": "2025-08-17T04:05:49.437424Z"
    },
    "papermill": {
     "duration": 0.021151,
     "end_time": "2025-08-17T04:05:49.439560",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.418409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def _rand_bbox(W: int, H: int, lam: float):\n",
    "    # CutMix box size from area ratio lam\n",
    "    cut_rat = (1.0 - lam) ** 0.5\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    cx = random.randint(0, W - 1)\n",
    "    cy = random.randint(0, H - 1)\n",
    "    x1 = max(cx - cut_w // 2, 0)\n",
    "    y1 = max(cy - cut_h // 2, 0)\n",
    "    x2 = min(cx + cut_w // 2, W)\n",
    "    y2 = min(cy + cut_h // 2, H)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "class MixupCutmix:\n",
    "    \"\"\"\n",
    "    On each batch, applies Mixup or CutMix with given probabilities.\n",
    "    Returns:\n",
    "      images: possibly mixed tensor\n",
    "      targets: either Long (no mix) or Float one-hot (mixed)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int,\n",
    "                 mixup_alpha: float = 0.8,\n",
    "                 cutmix_alpha: float = 1.0,\n",
    "                 p_mixup: float = 0.5,\n",
    "                 p_cutmix: float = 0.5):\n",
    "        self.num_classes = num_classes\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.cutmix_alpha = cutmix_alpha\n",
    "        self.p_mixup = p_mixup\n",
    "        self.p_cutmix = p_cutmix\n",
    "        self.enabled = True\n",
    "\n",
    "    def off(self):  self.enabled = False\n",
    "    def on(self):   self.enabled = True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, images: torch.Tensor, targets: torch.Tensor):\n",
    "        if (not self.enabled) or (self.p_mixup <= 0 and self.p_cutmix <= 0):\n",
    "            return images, targets  # no change\n",
    "\n",
    "        B, C, H, W = images.shape\n",
    "        # decide op\n",
    "        op = None\n",
    "        r = random.random()\n",
    "        if r < self.p_mixup:\n",
    "            op = 'mixup'\n",
    "        elif r < self.p_mixup + self.p_cutmix:\n",
    "            op = 'cutmix'\n",
    "        else:\n",
    "            return images, targets  # no change\n",
    "\n",
    "        # sample lambda from Beta\n",
    "        from torch.distributions import Beta\n",
    "\n",
    "        if op == 'mixup' and self.mixup_alpha > 0:\n",
    "            lam = Beta(self.mixup_alpha, self.mixup_alpha).sample().item()\n",
    "        elif op == 'cutmix' and self.cutmix_alpha > 0:\n",
    "            lam = Beta(self.cutmix_alpha, self.cutmix_alpha).sample().item()\n",
    "        else:\n",
    "            return images, targets\n",
    "\n",
    "        lam = max(min(lam, 0.999), 0.001)\n",
    "\n",
    "        # shuffle\n",
    "        index = torch.randperm(B, device=images.device)\n",
    "        y1 = one_hot(targets, self.num_classes).to(images.dtype)\n",
    "        y2 = one_hot(targets[index], self.num_classes).to(images.dtype)\n",
    "\n",
    "        if op == 'mixup':\n",
    "            mixed = lam * images + (1.0 - lam) * images[index]\n",
    "            y = lam * y1 + (1.0 - lam) * y2\n",
    "            return mixed, y\n",
    "\n",
    "        # CutMix\n",
    "        x1, y1b, x2, y2b = _rand_bbox(W, H, lam)\n",
    "        mixed = images.clone()\n",
    "        mixed[:, :, y1b:y2b, x1:x2] = images[index, :, y1b:y2b, x1:x2]\n",
    "\n",
    "        # adjust lam to actual area\n",
    "        box_area = (x2 - x1) * (y2b - y1b)\n",
    "        lam_adj = 1.0 - float(box_area) / float(W * H)\n",
    "        y = lam_adj * one_hot(targets, self.num_classes).to(images.dtype) + \\\n",
    "            (1.0 - lam_adj) * one_hot(targets[index], self.num_classes).to(images.dtype)\n",
    "        return mixed, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97599e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T04:05:49.456361Z",
     "iopub.status.busy": "2025-08-17T04:05:49.455903Z",
     "iopub.status.idle": "2025-08-17T09:09:55.132535Z",
     "shell.execute_reply": "2025-08-17T09:09:55.131601Z"
    },
    "papermill": {
     "duration": 18245.69704,
     "end_time": "2025-08-17T09:09:55.144540",
     "exception": false,
     "start_time": "2025-08-17T04:05:49.447500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resume] param_group 0: lr=0.000150, initial_lr=0.000150\n",
      "[resume] param_group 1: lr=0.000150, initial_lr=0.000150\n",
      "[debug] starting epoch 92 PG LRs: [7.5e-05, 7.5e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/73540678.py:66: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
      "/tmp/ipykernel_19/73540678.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
      "/tmp/ipykernel_19/73540678.py:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  92/249, Train Loss: 0.6676, Val Loss: 2.2035, Top1 Acc: 0.5034, Top5 Acc: 0.7522, LR 0.000090\n",
      "Epoch  93/249, Train Loss: 0.5689, Val Loss: 2.2665, Top1 Acc: 0.5063, Top5 Acc: 0.7496, LR 0.000105\n",
      "Epoch  94/249, Train Loss: 0.5464, Val Loss: 2.3381, Top1 Acc: 0.4945, Top5 Acc: 0.7426, LR 0.000120\n",
      "Epoch  95/249, Train Loss: 2.6923, Val Loss: 2.2479, Top1 Acc: 0.4880, Top5 Acc: 0.7311, LR 0.000135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  96/249, Train Loss: 2.6661, Val Loss: 2.2846, Top1 Acc: 0.4854, Top5 Acc: 0.7271, LR 0.000150\n",
      "Epoch  97/249, Train Loss: 2.6983, Val Loss: 2.2952, Top1 Acc: 0.4812, Top5 Acc: 0.7265, LR 0.000150\n",
      "Epoch  98/249, Train Loss: 2.6892, Val Loss: 2.2544, Top1 Acc: 0.4868, Top5 Acc: 0.7334, LR 0.000150\n",
      "Epoch  99/249, Train Loss: 2.7729, Val Loss: 2.2538, Top1 Acc: 0.4837, Top5 Acc: 0.7284, LR 0.000150\n",
      "Epoch 100/249, Train Loss: 2.7061, Val Loss: 2.2420, Top1 Acc: 0.4878, Top5 Acc: 0.7337, LR 0.000150\n",
      "Epoch 101/249, Train Loss: 2.6972, Val Loss: 2.2529, Top1 Acc: 0.4888, Top5 Acc: 0.7301, LR 0.000150\n",
      "Epoch 102/249, Train Loss: 2.6995, Val Loss: 2.2414, Top1 Acc: 0.4907, Top5 Acc: 0.7318, LR 0.000149\n",
      "Epoch 103/249, Train Loss: 2.6570, Val Loss: 2.2309, Top1 Acc: 0.4893, Top5 Acc: 0.7344, LR 0.000149\n",
      "Epoch 104/249, Train Loss: 2.6853, Val Loss: 2.2245, Top1 Acc: 0.4915, Top5 Acc: 0.7348, LR 0.000149\n",
      "Epoch 105/249, Train Loss: 2.6622, Val Loss: 2.2500, Top1 Acc: 0.4850, Top5 Acc: 0.7322, LR 0.000149\n",
      "Epoch 106/249, Train Loss: 2.6721, Val Loss: 2.2284, Top1 Acc: 0.4924, Top5 Acc: 0.7370, LR 0.000149\n",
      "Epoch 107/249, Train Loss: 2.6963, Val Loss: 2.2300, Top1 Acc: 0.4897, Top5 Acc: 0.7361, LR 0.000148\n",
      "Epoch 108/249, Train Loss: 2.6637, Val Loss: 2.2523, Top1 Acc: 0.4868, Top5 Acc: 0.7287, LR 0.000148\n",
      "Epoch 109/249, Train Loss: 2.6511, Val Loss: 2.2121, Top1 Acc: 0.4925, Top5 Acc: 0.7362, LR 0.000148\n",
      "Epoch 110/249, Train Loss: 2.6621, Val Loss: 2.2208, Top1 Acc: 0.4904, Top5 Acc: 0.7382, LR 0.000147\n",
      "Epoch 111/249, Train Loss: 2.6736, Val Loss: 2.2248, Top1 Acc: 0.4949, Top5 Acc: 0.7340, LR 0.000147\n",
      "Epoch 112/249, Train Loss: 2.6373, Val Loss: 2.2404, Top1 Acc: 0.4893, Top5 Acc: 0.7317, LR 0.000146\n",
      "Epoch 113/249, Train Loss: 2.6398, Val Loss: 2.2419, Top1 Acc: 0.4902, Top5 Acc: 0.7326, LR 0.000146\n",
      "Epoch 114/249, Train Loss: 2.6341, Val Loss: 2.2358, Top1 Acc: 0.4930, Top5 Acc: 0.7338, LR 0.000145\n",
      "Epoch 115/249, Train Loss: 2.6126, Val Loss: 2.2162, Top1 Acc: 0.5006, Top5 Acc: 0.7367, LR 0.000145\n",
      "Epoch 116/249, Train Loss: 2.5801, Val Loss: 2.2378, Top1 Acc: 0.4882, Top5 Acc: 0.7362, LR 0.000144\n",
      "Epoch 117/249, Train Loss: 2.5952, Val Loss: 2.2286, Top1 Acc: 0.4890, Top5 Acc: 0.7362, LR 0.000144\n",
      "Epoch 118/249, Train Loss: 2.6103, Val Loss: 2.2302, Top1 Acc: 0.4934, Top5 Acc: 0.7339, LR 0.000143\n",
      "Epoch 119/249, Train Loss: 2.5399, Val Loss: 2.2409, Top1 Acc: 0.4933, Top5 Acc: 0.7325, LR 0.000143\n",
      "Epoch 120/249, Train Loss: 2.6180, Val Loss: 2.2552, Top1 Acc: 0.4852, Top5 Acc: 0.7296, LR 0.000142\n",
      "Epoch 121/249, Train Loss: 2.5953, Val Loss: 2.2263, Top1 Acc: 0.4918, Top5 Acc: 0.7347, LR 0.000141\n",
      "Epoch 122/249, Train Loss: 2.5456, Val Loss: 2.2595, Top1 Acc: 0.4933, Top5 Acc: 0.7336, LR 0.000141\n",
      "Epoch 123/249, Train Loss: 2.5624, Val Loss: 2.2597, Top1 Acc: 0.4866, Top5 Acc: 0.7320, LR 0.000140\n",
      "Epoch 124/249, Train Loss: 2.5699, Val Loss: 2.2273, Top1 Acc: 0.4925, Top5 Acc: 0.7365, LR 0.000139\n",
      "Epoch 125/249, Train Loss: 2.5535, Val Loss: 2.2514, Top1 Acc: 0.4904, Top5 Acc: 0.7292, LR 0.000138\n",
      "Epoch 126/249, Train Loss: 2.5590, Val Loss: 2.2191, Top1 Acc: 0.4962, Top5 Acc: 0.7358, LR 0.000138\n",
      "Epoch 127/249, Train Loss: 2.5102, Val Loss: 2.2387, Top1 Acc: 0.4950, Top5 Acc: 0.7370, LR 0.000137\n",
      "Epoch 128/249, Train Loss: 2.5701, Val Loss: 2.2227, Top1 Acc: 0.4974, Top5 Acc: 0.7347, LR 0.000136\n",
      "Epoch 129/249, Train Loss: 2.5142, Val Loss: 2.2373, Top1 Acc: 0.4967, Top5 Acc: 0.7335, LR 0.000135\n",
      "Epoch 130/249, Train Loss: 2.5220, Val Loss: 2.1895, Top1 Acc: 0.5101, Top5 Acc: 0.7419, LR 0.000134\n",
      "Epoch 131/249, Train Loss: 2.5427, Val Loss: 2.2287, Top1 Acc: 0.4982, Top5 Acc: 0.7311, LR 0.000133\n",
      "Epoch 132/249, Train Loss: 2.4877, Val Loss: 2.2593, Top1 Acc: 0.4955, Top5 Acc: 0.7317, LR 0.000132\n",
      "Epoch 133/249, Train Loss: 2.4570, Val Loss: 2.2384, Top1 Acc: 0.4953, Top5 Acc: 0.7310, LR 0.000131\n",
      "Epoch 134/249, Train Loss: 2.5652, Val Loss: 2.2155, Top1 Acc: 0.5023, Top5 Acc: 0.7371, LR 0.000130\n",
      "Epoch 135/249, Train Loss: 2.5124, Val Loss: 2.2453, Top1 Acc: 0.4957, Top5 Acc: 0.7313, LR 0.000129\n",
      "Epoch 136/249, Train Loss: 2.4237, Val Loss: 2.2164, Top1 Acc: 0.5006, Top5 Acc: 0.7392, LR 0.000128\n",
      "Epoch 137/249, Train Loss: 2.5101, Val Loss: 2.2112, Top1 Acc: 0.4997, Top5 Acc: 0.7366, LR 0.000127\n",
      "Epoch 138/249, Train Loss: 2.4996, Val Loss: 2.2401, Top1 Acc: 0.4924, Top5 Acc: 0.7386, LR 0.000126\n",
      "Epoch 139/249, Train Loss: 2.4320, Val Loss: 2.2362, Top1 Acc: 0.4988, Top5 Acc: 0.7361, LR 0.000125\n",
      "Epoch 140/249, Train Loss: 2.4771, Val Loss: 2.2330, Top1 Acc: 0.4968, Top5 Acc: 0.7352, LR 0.000124\n",
      "Epoch 141/249, Train Loss: 2.4465, Val Loss: 2.2453, Top1 Acc: 0.4946, Top5 Acc: 0.7325, LR 0.000123\n",
      "Epoch 142/249, Train Loss: 2.4349, Val Loss: 2.2355, Top1 Acc: 0.4981, Top5 Acc: 0.7297, LR 0.000122\n",
      "Epoch 143/249, Train Loss: 2.3750, Val Loss: 2.2518, Top1 Acc: 0.4940, Top5 Acc: 0.7331, LR 0.000121\n",
      "Epoch 144/249, Train Loss: 2.4152, Val Loss: 2.2476, Top1 Acc: 0.4910, Top5 Acc: 0.7347, LR 0.000120\n",
      "Epoch 145/249, Train Loss: 2.4331, Val Loss: 2.2221, Top1 Acc: 0.5020, Top5 Acc: 0.7344, LR 0.000119\n",
      "Epoch 146/249, Train Loss: 2.4645, Val Loss: 2.2464, Top1 Acc: 0.4973, Top5 Acc: 0.7322, LR 0.000117\n",
      "Epoch 147/249, Train Loss: 2.4105, Val Loss: 2.2510, Top1 Acc: 0.4970, Top5 Acc: 0.7291, LR 0.000116\n",
      "Epoch 148/249, Train Loss: 2.4438, Val Loss: 2.2200, Top1 Acc: 0.5023, Top5 Acc: 0.7376, LR 0.000115\n",
      "Epoch 149/249, Train Loss: 2.4269, Val Loss: 2.2173, Top1 Acc: 0.5063, Top5 Acc: 0.7363, LR 0.000114\n",
      "Epoch 150/249, Train Loss: 2.4135, Val Loss: 2.2310, Top1 Acc: 0.5065, Top5 Acc: 0.7352, LR 0.000113\n",
      "Epoch 151/249, Train Loss: 2.3699, Val Loss: 2.2044, Top1 Acc: 0.5082, Top5 Acc: 0.7397, LR 0.000111\n",
      "Epoch 152/249, Train Loss: 2.3975, Val Loss: 2.2251, Top1 Acc: 0.4999, Top5 Acc: 0.7332, LR 0.000110\n",
      "Epoch 153/249, Train Loss: 2.3830, Val Loss: 2.2287, Top1 Acc: 0.5031, Top5 Acc: 0.7377, LR 0.000109\n",
      "Epoch 154/249, Train Loss: 2.3896, Val Loss: 2.2477, Top1 Acc: 0.5046, Top5 Acc: 0.7336, LR 0.000108\n",
      "Epoch 155/249, Train Loss: 2.4119, Val Loss: 2.2190, Top1 Acc: 0.5043, Top5 Acc: 0.7382, LR 0.000106\n",
      "Epoch 156/249, Train Loss: 2.3405, Val Loss: 2.2147, Top1 Acc: 0.5051, Top5 Acc: 0.7379, LR 0.000105\n",
      "Epoch 157/249, Train Loss: 2.3508, Val Loss: 2.2035, Top1 Acc: 0.5086, Top5 Acc: 0.7378, LR 0.000104\n",
      "Epoch 158/249, Train Loss: 2.3125, Val Loss: 2.2255, Top1 Acc: 0.5048, Top5 Acc: 0.7340, LR 0.000102\n",
      "Epoch 159/249, Train Loss: 2.3512, Val Loss: 2.2242, Top1 Acc: 0.5043, Top5 Acc: 0.7352, LR 0.000101\n",
      "Epoch 160/249, Train Loss: 2.3219, Val Loss: 2.2225, Top1 Acc: 0.5062, Top5 Acc: 0.7380, LR 0.000100\n",
      "Epoch 161/249, Train Loss: 2.3829, Val Loss: 2.2383, Top1 Acc: 0.5047, Top5 Acc: 0.7326, LR 0.000098\n",
      "Epoch 162/249, Train Loss: 2.3424, Val Loss: 2.2158, Top1 Acc: 0.5048, Top5 Acc: 0.7384, LR 0.000097\n",
      "Epoch 163/249, Train Loss: 2.2784, Val Loss: 2.2119, Top1 Acc: 0.5098, Top5 Acc: 0.7354, LR 0.000096\n",
      "Epoch 164/249, Train Loss: 2.2884, Val Loss: 2.2114, Top1 Acc: 0.5057, Top5 Acc: 0.7411, LR 0.000094\n",
      "Epoch 165/249, Train Loss: 2.3353, Val Loss: 2.2142, Top1 Acc: 0.5052, Top5 Acc: 0.7402, LR 0.000093\n",
      "Epoch 166/249, Train Loss: 2.2521, Val Loss: 2.2128, Top1 Acc: 0.5093, Top5 Acc: 0.7384, LR 0.000091\n",
      "Epoch 167/249, Train Loss: 2.2890, Val Loss: 2.2502, Top1 Acc: 0.5066, Top5 Acc: 0.7339, LR 0.000090\n",
      "Epoch 168/249, Train Loss: 2.2776, Val Loss: 2.2188, Top1 Acc: 0.5056, Top5 Acc: 0.7376, LR 0.000089\n",
      "Epoch 169/249, Train Loss: 2.3137, Val Loss: 2.2371, Top1 Acc: 0.5086, Top5 Acc: 0.7318, LR 0.000087\n",
      "Epoch 170/249, Train Loss: 2.2587, Val Loss: 2.2053, Top1 Acc: 0.5139, Top5 Acc: 0.7390, LR 0.000086\n",
      "Epoch 171/249, Train Loss: 2.2779, Val Loss: 2.2259, Top1 Acc: 0.5090, Top5 Acc: 0.7377, LR 0.000085\n",
      "Epoch 172/249, Train Loss: 2.2476, Val Loss: 2.2304, Top1 Acc: 0.5106, Top5 Acc: 0.7368, LR 0.000083\n",
      "Epoch 173/249, Train Loss: 2.2314, Val Loss: 2.2137, Top1 Acc: 0.5127, Top5 Acc: 0.7434, LR 0.000082\n",
      "Epoch 174/249, Train Loss: 2.2403, Val Loss: 2.2116, Top1 Acc: 0.5130, Top5 Acc: 0.7318, LR 0.000080\n",
      "Epoch 175/249, Train Loss: 2.2990, Val Loss: 2.2221, Top1 Acc: 0.5093, Top5 Acc: 0.7371, LR 0.000079\n",
      "Epoch 176/249, Train Loss: 2.2286, Val Loss: 2.2136, Top1 Acc: 0.5090, Top5 Acc: 0.7353, LR 0.000078\n",
      "Epoch 177/249, Train Loss: 2.2072, Val Loss: 2.1968, Top1 Acc: 0.5112, Top5 Acc: 0.7428, LR 0.000076\n",
      "Epoch 178/249, Train Loss: 2.2152, Val Loss: 2.2155, Top1 Acc: 0.5122, Top5 Acc: 0.7366, LR 0.000075\n",
      "Epoch 179/249, Train Loss: 2.2185, Val Loss: 2.2260, Top1 Acc: 0.5111, Top5 Acc: 0.7336, LR 0.000074\n",
      "Epoch 180/249, Train Loss: 2.2090, Val Loss: 2.2037, Top1 Acc: 0.5133, Top5 Acc: 0.7414, LR 0.000072\n",
      "Epoch 181/249, Train Loss: 2.1923, Val Loss: 2.2214, Top1 Acc: 0.5135, Top5 Acc: 0.7372, LR 0.000071\n",
      "Epoch 182/249, Train Loss: 2.2060, Val Loss: 2.2204, Top1 Acc: 0.5130, Top5 Acc: 0.7384, LR 0.000069\n",
      "Epoch 183/249, Train Loss: 2.2055, Val Loss: 2.2276, Top1 Acc: 0.5123, Top5 Acc: 0.7359, LR 0.000068\n",
      "Epoch 184/249, Train Loss: 2.2189, Val Loss: 2.2060, Top1 Acc: 0.5150, Top5 Acc: 0.7383, LR 0.000067\n",
      "Epoch 185/249, Train Loss: 2.2082, Val Loss: 2.1984, Top1 Acc: 0.5134, Top5 Acc: 0.7417, LR 0.000065\n",
      "Epoch 186/249, Train Loss: 2.1983, Val Loss: 2.2187, Top1 Acc: 0.5103, Top5 Acc: 0.7378, LR 0.000064\n",
      "Epoch 187/249, Train Loss: 2.1707, Val Loss: 2.2123, Top1 Acc: 0.5137, Top5 Acc: 0.7392, LR 0.000063\n",
      "Epoch 188/249, Train Loss: 2.1425, Val Loss: 2.1851, Top1 Acc: 0.5165, Top5 Acc: 0.7426, LR 0.000061\n",
      "Epoch 189/249, Train Loss: 2.1879, Val Loss: 2.1868, Top1 Acc: 0.5196, Top5 Acc: 0.7405, LR 0.000060\n",
      "Epoch 190/249, Train Loss: 2.1856, Val Loss: 2.2344, Top1 Acc: 0.5123, Top5 Acc: 0.7382, LR 0.000059\n",
      "Epoch 191/249, Train Loss: 2.1506, Val Loss: 2.2030, Top1 Acc: 0.5171, Top5 Acc: 0.7435, LR 0.000057\n",
      "Epoch 192/249, Train Loss: 2.1748, Val Loss: 2.1958, Top1 Acc: 0.5176, Top5 Acc: 0.7372, LR 0.000056\n",
      "Epoch 193/249, Train Loss: 2.1621, Val Loss: 2.1955, Top1 Acc: 0.5174, Top5 Acc: 0.7403, LR 0.000055\n",
      "Epoch 194/249, Train Loss: 2.1468, Val Loss: 2.2255, Top1 Acc: 0.5131, Top5 Acc: 0.7350, LR 0.000054\n",
      "Epoch 195/249, Train Loss: 2.1188, Val Loss: 2.2198, Top1 Acc: 0.5150, Top5 Acc: 0.7351, LR 0.000052\n",
      "Epoch 196/249, Train Loss: 2.1412, Val Loss: 2.1998, Top1 Acc: 0.5158, Top5 Acc: 0.7375, LR 0.000051\n",
      "Epoch 197/249, Train Loss: 2.1255, Val Loss: 2.1991, Top1 Acc: 0.5143, Top5 Acc: 0.7408, LR 0.000050\n",
      "Epoch 198/249, Train Loss: 2.1371, Val Loss: 2.2205, Top1 Acc: 0.5142, Top5 Acc: 0.7398, LR 0.000049\n",
      "Epoch 199/249, Train Loss: 2.1176, Val Loss: 2.2102, Top1 Acc: 0.5183, Top5 Acc: 0.7380, LR 0.000048\n",
      "Epoch 200/249, Train Loss: 2.1544, Val Loss: 2.2170, Top1 Acc: 0.5153, Top5 Acc: 0.7383, LR 0.000046\n",
      "Epoch 201/249, Train Loss: 2.0804, Val Loss: 2.2137, Top1 Acc: 0.5170, Top5 Acc: 0.7376, LR 0.000045\n",
      "Epoch 202/249, Train Loss: 2.1205, Val Loss: 2.1961, Top1 Acc: 0.5205, Top5 Acc: 0.7409, LR 0.000044\n",
      "Epoch 203/249, Train Loss: 2.0796, Val Loss: 2.1896, Top1 Acc: 0.5203, Top5 Acc: 0.7418, LR 0.000043\n",
      "Epoch 204/249, Train Loss: 2.1026, Val Loss: 2.2086, Top1 Acc: 0.5174, Top5 Acc: 0.7384, LR 0.000042\n",
      "Epoch 205/249, Train Loss: 2.0885, Val Loss: 2.1961, Top1 Acc: 0.5187, Top5 Acc: 0.7441, LR 0.000041\n",
      "Epoch 206/249, Train Loss: 2.0458, Val Loss: 2.1937, Top1 Acc: 0.5210, Top5 Acc: 0.7414, LR 0.000040\n",
      "Epoch 207/249, Train Loss: 2.0715, Val Loss: 2.1860, Top1 Acc: 0.5228, Top5 Acc: 0.7442, LR 0.000039\n",
      "Epoch 208/249, Train Loss: 2.0826, Val Loss: 2.1967, Top1 Acc: 0.5194, Top5 Acc: 0.7433, LR 0.000038\n",
      "Epoch 209/249, Train Loss: 2.1252, Val Loss: 2.2065, Top1 Acc: 0.5195, Top5 Acc: 0.7381, LR 0.000037\n",
      "Epoch 210/249, Train Loss: 2.0471, Val Loss: 2.1952, Top1 Acc: 0.5234, Top5 Acc: 0.7387, LR 0.000036\n",
      "Saved new best model (phase 2).\n",
      "Epoch 211/249, Train Loss: 2.0400, Val Loss: 2.1759, Top1 Acc: 0.5297, Top5 Acc: 0.7471, LR 0.000035\n",
      "Epoch 212/249, Train Loss: 2.0653, Val Loss: 2.1866, Top1 Acc: 0.5226, Top5 Acc: 0.7421, LR 0.000034\n",
      "Epoch 213/249, Train Loss: 2.0258, Val Loss: 2.1864, Top1 Acc: 0.5261, Top5 Acc: 0.7420, LR 0.000033\n",
      "Epoch 214/249, Train Loss: 2.0250, Val Loss: 2.1950, Top1 Acc: 0.5219, Top5 Acc: 0.7421, LR 0.000032\n",
      "Epoch 215/249, Train Loss: 2.0294, Val Loss: 2.1895, Top1 Acc: 0.5273, Top5 Acc: 0.7440, LR 0.000031\n",
      "Epoch 216/249, Train Loss: 2.1108, Val Loss: 2.1997, Top1 Acc: 0.5232, Top5 Acc: 0.7444, LR 0.000030\n",
      "Epoch 217/249, Train Loss: 2.0547, Val Loss: 2.1672, Top1 Acc: 0.5283, Top5 Acc: 0.7420, LR 0.000029\n",
      "Epoch 218/249, Train Loss: 2.0220, Val Loss: 2.1814, Top1 Acc: 0.5237, Top5 Acc: 0.7453, LR 0.000028\n",
      "Epoch 219/249, Train Loss: 2.0260, Val Loss: 2.1826, Top1 Acc: 0.5286, Top5 Acc: 0.7461, LR 0.000027\n",
      "Epoch 220/249, Train Loss: 2.0136, Val Loss: 2.1880, Top1 Acc: 0.5238, Top5 Acc: 0.7418, LR 0.000027\n",
      "Epoch 221/249, Train Loss: 2.0299, Val Loss: 2.2098, Top1 Acc: 0.5215, Top5 Acc: 0.7395, LR 0.000026\n",
      "Epoch 222/249, Train Loss: 2.0164, Val Loss: 2.1870, Top1 Acc: 0.5257, Top5 Acc: 0.7448, LR 0.000025\n",
      "Epoch 223/249, Train Loss: 2.0113, Val Loss: 2.1927, Top1 Acc: 0.5261, Top5 Acc: 0.7413, LR 0.000024\n",
      "Epoch 224/249, Train Loss: 1.9945, Val Loss: 2.1930, Top1 Acc: 0.5266, Top5 Acc: 0.7414, LR 0.000024\n",
      "Epoch 225/249, Train Loss: 1.9915, Val Loss: 2.1834, Top1 Acc: 0.5265, Top5 Acc: 0.7433, LR 0.000023\n",
      "Epoch 226/249, Train Loss: 2.0283, Val Loss: 2.1724, Top1 Acc: 0.5259, Top5 Acc: 0.7459, LR 0.000022\n",
      "Epoch 227/249, Train Loss: 2.0164, Val Loss: 2.1807, Top1 Acc: 0.5243, Top5 Acc: 0.7449, LR 0.000022\n",
      "Epoch 228/249, Train Loss: 1.9991, Val Loss: 2.1835, Top1 Acc: 0.5273, Top5 Acc: 0.7465, LR 0.000021\n",
      "Epoch 229/249, Train Loss: 2.0027, Val Loss: 2.1704, Top1 Acc: 0.5274, Top5 Acc: 0.7476, LR 0.000021\n",
      "Epoch 230/249, Train Loss: 1.9890, Val Loss: 2.1791, Top1 Acc: 0.5276, Top5 Acc: 0.7435, LR 0.000020\n",
      "Epoch 231/249, Train Loss: 1.9905, Val Loss: 2.2007, Top1 Acc: 0.5281, Top5 Acc: 0.7408, LR 0.000020\n",
      "Epoch 232/249, Train Loss: 1.9797, Val Loss: 2.1756, Top1 Acc: 0.5282, Top5 Acc: 0.7420, LR 0.000019\n",
      "Epoch 233/249, Train Loss: 2.0008, Val Loss: 2.1692, Top1 Acc: 0.5255, Top5 Acc: 0.7434, LR 0.000019\n",
      "Epoch 234/249, Train Loss: 1.9973, Val Loss: 2.1939, Top1 Acc: 0.5240, Top5 Acc: 0.7395, LR 0.000018\n",
      "Epoch 235/249, Train Loss: 1.9748, Val Loss: 2.1911, Top1 Acc: 0.5247, Top5 Acc: 0.7419, LR 0.000018\n",
      "Epoch 236/249, Train Loss: 2.0262, Val Loss: 2.1729, Top1 Acc: 0.5279, Top5 Acc: 0.7442, LR 0.000017\n",
      "Saved new best model (phase 2).\n",
      "Epoch 237/249, Train Loss: 1.9929, Val Loss: 2.1712, Top1 Acc: 0.5306, Top5 Acc: 0.7454, LR 0.000017\n",
      "Saved new best model (phase 2).\n",
      "Epoch 238/249, Train Loss: 1.9661, Val Loss: 2.1656, Top1 Acc: 0.5314, Top5 Acc: 0.7456, LR 0.000017\n",
      "Epoch 239/249, Train Loss: 1.9879, Val Loss: 2.1834, Top1 Acc: 0.5280, Top5 Acc: 0.7448, LR 0.000016\n",
      "Saved new best model (phase 2).\n",
      "Epoch 240/249, Train Loss: 0.1065, Val Loss: 2.1839, Top1 Acc: 0.5365, Top5 Acc: 0.7542, LR 0.000016\n",
      "Epoch 241/249, Train Loss: 0.0982, Val Loss: 2.1987, Top1 Acc: 0.5349, Top5 Acc: 0.7537, LR 0.000016\n",
      "Epoch 242/249, Train Loss: 0.0927, Val Loss: 2.2159, Top1 Acc: 0.5339, Top5 Acc: 0.7540, LR 0.000016\n",
      "Epoch 243/249, Train Loss: 0.0883, Val Loss: 2.2450, Top1 Acc: 0.5335, Top5 Acc: 0.7546, LR 0.000016\n",
      "Epoch 244/249, Train Loss: 0.0854, Val Loss: 2.2531, Top1 Acc: 0.5349, Top5 Acc: 0.7561, LR 0.000015\n",
      "Epoch 245/249, Train Loss: 0.0830, Val Loss: 2.2798, Top1 Acc: 0.5342, Top5 Acc: 0.7552, LR 0.000015\n",
      "Epoch 246/249, Train Loss: 0.0802, Val Loss: 2.2881, Top1 Acc: 0.5331, Top5 Acc: 0.7544, LR 0.000015\n",
      "Epoch 247/249, Train Loss: 0.0778, Val Loss: 2.3109, Top1 Acc: 0.5311, Top5 Acc: 0.7531, LR 0.000015\n",
      "Epoch 248/249, Train Loss: 0.0789, Val Loss: 2.3242, Top1 Acc: 0.5322, Top5 Acc: 0.7547, LR 0.000015\n",
      "Epoch 249/249, Train Loss: 0.0754, Val Loss: 2.3270, Top1 Acc: 0.5338, Top5 Acc: 0.7547, LR 0.000015\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "# ---------------- Model / Loss / Mixer ----------------\n",
    "model = SwinTransformer(\n",
    "    img_size=64, patch_size=4, emb_dim=64,\n",
    "    blocks=[2, 2, 4, 2], nheads=[2, 4, 8, 16],\n",
    "    M=[8, 8, 4, 2], n_classes=200, stochastic_endpoint=0.1\n",
    ").to(device)\n",
    "\n",
    "loss_fn = SoftCrossEntropy(label_smoothing=0.0)\n",
    "mixer   = MixupCutmix(num_classes=200, mixup_alpha=0.8, cutmix_alpha=1.0,\n",
    "                      p_mixup=0.5, p_cutmix=0.5)\n",
    "\n",
    "def param_groups_weight_decay(model, weight_decay=0.05):\n",
    "    decay, no_decay = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad: \n",
    "            continue\n",
    "        name = n.lower()\n",
    "        if p.ndim == 1 or n.endswith(\".bias\") or (\"relative\" in name) or (\"pos_embed\" in name):\n",
    "            no_decay.append(p)   # LayerNorm/bias/positional stuff: no decay\n",
    "        else:\n",
    "            decay.append(p)\n",
    "    return [{\"params\": decay, \"weight_decay\": weight_decay},\n",
    "            {\"params\": no_decay, \"weight_decay\": 0.0}]\n",
    "\n",
    "# ---------------- Load checkpoint ----------------\n",
    "ckpt_path = \"/kaggle/input/checkpoint-91st-epoch/swin_t_best.pt\"  # <- adjust if needed\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    param_groups_weight_decay(model, 0.05),\n",
    "    lr=1e-4, betas=(0.9, 0.999), fused=(device.type==\"cuda\")\n",
    ")\n",
    "if \"optimizer\" in ckpt:\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])  # bring back moments, etc.\n",
    "\n",
    "start_epoch   = int(ckpt.get(\"epoch\", 0)) + 1   # resume AFTER saved epoch\n",
    "best_val_top1 = float(ckpt.get(\"best_val_top1\", 0.0))\n",
    "best_val_top5 = float(ckpt.get(\"best_val_top5\", 0.0))\n",
    "\n",
    "total_epochs = 250\n",
    "assert start_epoch < total_epochs, f\"start_epoch={start_epoch} already >= total_epochs={total_epochs}\"\n",
    "\n",
    "# ---------------- HARD RESET LRs (fix jumping to 3e-4) ----------------\n",
    "new_base_lr = 1.5e-4   # sweet spot from your telemetry\n",
    "for i, g in enumerate(optimizer.param_groups):\n",
    "    g[\"lr\"] = new_base_lr\n",
    "    g[\"initial_lr\"] = new_base_lr   # some schedulers read this\n",
    "    print(f\"[resume] param_group {i}: lr={g['lr']:.6f}, initial_lr={g['initial_lr']:.6f}\")\n",
    "\n",
    "# ---------------- Fresh scheduler (warmup + cosine with floor) ----------------\n",
    "remaining_epochs = total_epochs - start_epoch\n",
    "warmup_epochs = min(5, max(1, remaining_epochs // 10))  # 1..5 epochs warmup\n",
    "cosine_epochs = max(1, remaining_epochs - warmup_epochs)\n",
    "\n",
    "warmup = LinearLR(optimizer, start_factor=0.5, end_factor=1.0, total_iters=warmup_epochs)\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=cosine_epochs, eta_min=new_base_lr * 0.1)\n",
    "scheduler = SequentialLR(optimizer, [warmup, cosine], milestones=[warmup_epochs])\n",
    "\n",
    "# ---------------- AMP / speed ----------------\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
    "\n",
    "# ---------------- Logging ----------------\n",
    "history_path = \"training_history_phase2.json\"\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_top1\": [], \"val_top5\": [], \"lr\": []}\n",
    "\n",
    "# Mixup/CutMix schedule: off for first 3 epochs after resume, and last 10\n",
    "def should_mix(epoch):\n",
    "    return not (start_epoch <= epoch < start_epoch + 3 or epoch >= total_epochs - 10)\n",
    "\n",
    "# --------------- TRAIN LOOP ---------------\n",
    "for epoch in range(start_epoch, total_epochs):\n",
    "    if epoch == start_epoch:\n",
    "        print(\"[debug] starting epoch\", epoch, \"PG LRs:\", [pg[\"lr\"] for pg in optimizer.param_groups])\n",
    "\n",
    "    model.train()\n",
    "    total_loss, n_samples = 0.0, 0\n",
    "\n",
    "    if should_mix(epoch):\n",
    "        mixer.on()\n",
    "    else:\n",
    "        mixer.off()\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        xb, yb = mixer(xb, yb)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "            yhat = model(xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        n_samples += bs\n",
    "\n",
    "    avg_loss = total_loss / max(1, n_samples)\n",
    "\n",
    "    # ------ Validation ------\n",
    "    model.eval()\n",
    "    val_loss, val_samples = 0.0, 0\n",
    "    val_correct_top1, val_correct_top5 = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "                yhat = model(xb)\n",
    "                loss = loss_fn(yhat, yb)\n",
    "\n",
    "            bs = xb.size(0)\n",
    "            val_loss += loss.item() * bs\n",
    "            val_samples += bs\n",
    "\n",
    "            top5 = torch.topk(yhat, k=5, dim=1).indices  # (B,5)\n",
    "            val_correct_top1 += (top5[:, 0] == yb).sum().item()\n",
    "            val_correct_top5 += top5.eq(yb.view(-1, 1)).any(dim=1).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / max(1, val_samples)\n",
    "    val_top1 = val_correct_top1 / max(1, val_samples)\n",
    "    val_top5 = val_correct_top5 / max(1, val_samples)\n",
    "\n",
    "    # Step scheduler (no epoch arg)\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    # ------ Log & Save ------\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_top1\"].append(val_top1)\n",
    "    history[\"val_top5\"].append(val_top5)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "\n",
    "    with open(history_path, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "    # Save best by Acc@1 (track Acc@5 too)\n",
    "    if val_top1 > best_val_top1:\n",
    "        best_val_top1 = val_top1\n",
    "        best_val_top5 = max(best_val_top5, val_top5)\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val_top1\": best_val_top1,\n",
    "            \"best_val_top5\": best_val_top5\n",
    "        }, \"swin_t_best_phase2.pt\")\n",
    "        print(\"Saved new best model (phase 2).\")\n",
    "        \n",
    "    print(f\"Epoch {epoch:3d}/{total_epochs-1}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Top1 Acc: {val_top1:.4f}, Top5 Acc: {val_top5:.4f}, LR {current_lr:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62ced0",
   "metadata": {
    "papermill": {
     "duration": 0.010845,
     "end_time": "2025-08-17T09:09:55.166291",
     "exception": false,
     "start_time": "2025-08-17T09:09:55.155446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "224 images by 224 was way too much for my GPU. So I went down to 64 x 64 and adjusted. I didn't want M to be 2 so I modified so each stage has a different M. Entire architecture was adapted and controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e39167",
   "metadata": {
    "papermill": {
     "duration": 0.011171,
     "end_time": "2025-08-17T09:09:55.188341",
     "exception": false,
     "start_time": "2025-08-17T09:09:55.177170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Augmentations too strong at 64×64.\n",
    "RandAugment(mag=9) + RandomResizedCrop(0.8,1.0) + RandomErasing(p=0.25) on tiny images strips too much signal. Model keeps fitting train (loss ↓) but can’t push val higher → plateau at ~37%.\n",
    "\n",
    "Relative Position Bias (RPB) was decayed.\n",
    "relative_table lived in the weight-decay group before your change. That nudges locality info toward zero over time, hurting window attention’s inductive bias → earlier “ok” gains, then flattening.\n",
    "\n",
    "Mixup/CutMix on for ~90% of training.\n",
    "Great regularizers, but at 64×64 + heavy RA they can over-regularize, making logits softer. You also validate with hard CE, so it’s common to see val_loss drift up a bit while acc stays flat (calibration mismatch).\n",
    "\n",
    "Resolution + capacity trade-off.\n",
    "At 64px, content is compressed. Your Swin-T config (C=64, windows [8,8,4,2]) is reasonable, but it simply has less separable info than 224px baselines. ~35–40% Top-1 on Tiny-IN@64 with strong regularization is plausible.\n",
    "\n",
    "Validation hygiene.\n",
    "The split is from train/ (not official val/) and not stratified in the “before” run. That adds noise and can slightly mute peak accuracy.\n",
    "\n",
    "Minor dampeners (secondary):\n",
    "\n",
    "Grad clip = 1.0 can be tight for attention; may slow learning a bit.\n",
    "\n",
    "LR schedule is fine; after ~30–40 epochs, cosine has already lowered LR a lot, making it harder to escape the regularization-limited regime."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7815483,
     "sourceId": 12394036,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8082387,
     "sourceId": 12784109,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18763.906711,
   "end_time": "2025-08-17T09:09:57.933679",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-17T03:57:14.026968",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
