{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15bca52",
   "metadata": {
    "papermill": {
     "duration": 0.005333,
     "end_time": "2025-08-15T23:23:47.598217",
     "exception": false,
     "start_time": "2025-08-15T23:23:47.592884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation and Augmentation\n",
    "\n",
    "Data augmentation techniques include RandomResizedCrop, RandomHorizontalFlip, RandAugment (applies random augmentations), and RandomErasing (randomly erases a rectangular region in an image). The pixel values are also normalized using the mean and standard deviation of the dataset. I found this in my ResNet-34 from scratch project.\n",
    "\n",
    "Dataset is then split into a 90% training set and a 10% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ac15cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:23:47.607779Z",
     "iopub.status.busy": "2025-08-15T23:23:47.607548Z",
     "iopub.status.idle": "2025-08-15T23:24:26.014760Z",
     "shell.execute_reply": "2025-08-15T23:24:26.014034Z"
    },
    "papermill": {
     "duration": 38.417097,
     "end_time": "2025-08-15T23:24:26.019817",
     "exception": false,
     "start_time": "2025-08-15T23:23:47.602720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reorganized validation images → /kaggle/working/tiny-imagenet-200-val\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, csv, pathlib\n",
    "\n",
    "ROOT = \"/kaggle/input/tiny-imagenet-200/tiny-imagenet-200\"\n",
    "VAL_DIR = os.path.join(ROOT, \"val\")\n",
    "VAL_ANN = os.path.join(VAL_DIR, \"val_annotations.txt\")\n",
    "VAL_IMAGES = os.path.join(VAL_DIR, \"images\")\n",
    "\n",
    "# Where to build an ImageFolder-compatible val/ structure\n",
    "OUT_VAL = \"/kaggle/working/tiny-imagenet-200-val\"\n",
    "\n",
    "os.makedirs(OUT_VAL, exist_ok=True)\n",
    "\n",
    "# Build mapping: filename -> wnid (class folder)\n",
    "fname_to_wnid = {}\n",
    "with open(VAL_ANN, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        fname, wnid = row[0], row[1]\n",
    "        fname_to_wnid[fname] = wnid\n",
    "\n",
    "# Copy images into OUT_VAL/<wnid>/<filename>\n",
    "for fname, wnid in fname_to_wnid.items():\n",
    "    src = os.path.join(VAL_IMAGES, fname)\n",
    "    dst_dir = os.path.join(OUT_VAL, wnid)\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    dst = os.path.join(dst_dir, fname)\n",
    "    if not os.path.exists(dst):\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "print(\"Reorganized validation images →\", OUT_VAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e27e8494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:24:26.029624Z",
     "iopub.status.busy": "2025-08-15T23:24:26.029407Z",
     "iopub.status.idle": "2025-08-15T23:26:36.826606Z",
     "shell.execute_reply": "2025-08-15T23:26:36.825923Z"
    },
    "papermill": {
     "duration": 130.80365,
     "end_time": "2025-08-15T23:26:36.828022",
     "exception": false,
     "start_time": "2025-08-15T23:24:26.024372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "image_size = 64\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.9, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    RandAugment(num_ops=2, magnitude=5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random'),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=([0.4802, 0.4481, 0.3975]),   # normalize using mean & std\n",
    "                         std=([0.2296, 0.2263, 0.2255])),\n",
    "])\n",
    "\n",
    "train_dir = os.path.join(ROOT, \"train\")\n",
    "val_dir   = OUT_VAL\n",
    "\n",
    "train_set = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_set   = datasets.ImageFolder(val_dir,   transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=64, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c534765",
   "metadata": {
    "papermill": {
     "duration": 0.004614,
     "end_time": "2025-08-15T23:26:36.837937",
     "exception": false,
     "start_time": "2025-08-15T23:26:36.833323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Overwrote train transforms with validation transforms, removing augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e31f853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:36.848784Z",
     "iopub.status.busy": "2025-08-15T23:26:36.848457Z",
     "iopub.status.idle": "2025-08-15T23:26:37.249304Z",
     "shell.execute_reply": "2025-08-15T23:26:37.247703Z"
    },
    "papermill": {
     "duration": 0.408758,
     "end_time": "2025-08-15T23:26:37.251113",
     "exception": false,
     "start_time": "2025-08-15T23:26:36.842355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val batch images shape: torch.Size([64, 3, 64, 64])\n",
      "Val batch labels shape: torch.Size([64])\n",
      "Sample labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(val_loader))\n",
    "\n",
    "print(\"Val batch images shape:\", xb.shape)   # should be (B, 3, H, W)\n",
    "print(\"Val batch labels shape:\", yb.shape)   # should be (B,)\n",
    "print(\"Sample labels:\", yb[:10].tolist())\n",
    "\n",
    "# Verify ranges and types\n",
    "assert xb.ndim == 4 and xb.size(1) == 3, \"Images should have shape (B, 3, H, W)\"\n",
    "assert torch.is_floating_point(xb), \"Images should be float tensors\"\n",
    "assert yb.ndim == 1 and yb.dtype == torch.long, \"Labels should be 1D LongTensor\"\n",
    "assert xb.min() >= -5 and xb.max() <= 5, \"Values look off; check normalization\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15d07b",
   "metadata": {
    "papermill": {
     "duration": 0.007902,
     "end_time": "2025-08-15T23:26:37.267496",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.259594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Patching and Splitting Windows\n",
    "Unlike traditional CNNs that use sliding convolutional filters, Vision Transformers break the image down into a sequence of patches, treating them similarly to words in a sentence.\n",
    "\n",
    "Patchify Class: Takes an input image and converts it into patch embeddings. It uses a single convolutional layer where the kernel size and stride are equal to the patch_size. It divides the image into non overlapping patches and creating an initial vector embedding for each one.\n",
    "\n",
    "split_into_windows: Takes the patches and splits them into smaller windows. Self-attention is calculated within these windows, which is far more computationally efficient than the original ViT's approach of global attention across all patches.\n",
    "\n",
    "reverse_windows: Merges the split windows back into their original spatial layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ba88ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.284791Z",
     "iopub.status.busy": "2025-08-15T23:26:37.284499Z",
     "iopub.status.idle": "2025-08-15T23:26:37.291240Z",
     "shell.execute_reply": "2025-08-15T23:26:37.290310Z"
    },
    "papermill": {
     "duration": 0.017748,
     "end_time": "2025-08-15T23:26:37.293064",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.275316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Patchify(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert an image into patch embeddings using a convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        in_channels: Number of input channels (e.g., 3 for RGB).\n",
    "        embed_dim: Output embedding dimension per patch.\n",
    "        patch_size: Size of each square patch\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS, H//patch_size, W//patch_size, embed_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, embed_dim, patch_size):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (B, 3, H, W)\n",
    "        x = self.conv(x)         # (BS, embed_dim, H//patch_size, W//patch_size)\n",
    "        return x.permute(0, 2, 3, 1) # (BS, H//patch_size, W//patch_size, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41eb9bb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.305147Z",
     "iopub.status.busy": "2025-08-15T23:26:37.304505Z",
     "iopub.status.idle": "2025-08-15T23:26:37.308808Z",
     "shell.execute_reply": "2025-08-15T23:26:37.308283Z"
    },
    "papermill": {
     "duration": 0.010389,
     "end_time": "2025-08-15T23:26:37.309826",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.299437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_into_windows(x, M):\n",
    "    \"\"\"\n",
    "    Splits (BS, H, W, channels) into non overlapping MxM windows.\n",
    "    Args:\n",
    "        x: Tensor of shape (BS, H, W, channels)\n",
    "        M: Window size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS * num_windows, M*M, channels)\n",
    "    \"\"\"\n",
    "    BS, H, W, channels = x.shape\n",
    "    x = x.reshape(BS, H//M, M, W//M, M, channels)\n",
    "    # Permute fixes the order so the MxM pixels are together properly\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5)     # (BS, H//M, W//M, M, M, channels)\n",
    "    return x.reshape(-1, M*M, channels) # (BS * num_windows, M*M, channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3a69fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.319432Z",
     "iopub.status.busy": "2025-08-15T23:26:37.318965Z",
     "iopub.status.idle": "2025-08-15T23:26:37.322939Z",
     "shell.execute_reply": "2025-08-15T23:26:37.322427Z"
    },
    "papermill": {
     "duration": 0.009759,
     "end_time": "2025-08-15T23:26:37.323939",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.314180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reverse_windows(x, M, H, W, channels):\n",
    "    \"\"\"\n",
    "    Reverses MxM window tokens back into the original layout.\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape (BS * num_windows, M*M, channels)\n",
    "        M: Window size\n",
    "        H: Original image height\n",
    "        W: Original image width\n",
    "        channels: Number of channels\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (BS, H, W, channels)\n",
    "    \"\"\"\n",
    "    BS = x.shape[0] // (H//M * W//M) # Original BS\n",
    "    x = x.reshape(BS, H//M, W//M, M, M, channels)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5)      # (BS, H//M, M, W//M, M, channels)\n",
    "    return x.reshape(BS, H, W, channels) # (BS, H, W, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aaf64a",
   "metadata": {
    "papermill": {
     "duration": 0.004321,
     "end_time": "2025-08-15T23:26:37.332722",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.328401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Relative Position Bias\n",
    "\n",
    "Relative Position Bias shows self-attention mechanism about the geometry of the image. It computes a learnable bias between every pair of tokens in a window, based on how far apart they are. For each pair of tokens in an M x M window, compute relative position, use that to index into a learnable bias table, add this bias to the attention logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2443b6a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.342326Z",
     "iopub.status.busy": "2025-08-15T23:26:37.341923Z",
     "iopub.status.idle": "2025-08-15T23:26:37.349005Z",
     "shell.execute_reply": "2025-08-15T23:26:37.348454Z"
    },
    "papermill": {
     "duration": 0.012842,
     "end_time": "2025-08-15T23:26:37.349893",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.337051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes relative position bias for self-attention.\n",
    "\n",
    "    Generates a table of learnable relative position biases between token pairs\n",
    "    within an attention window of shape (M x M). The relative position between\n",
    "    any two tokens is encoded as a bias vector per attention head, and these biases are\n",
    "    added to the attention scores in self-attention.\n",
    "\n",
    "    Args:\n",
    "        M: The height/width of the attention window. The total number of tokens is M * M.\n",
    "        nheads: Number of attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        relative_table: Learnable parameter of shape ((2M - 1)^2, nheads),\n",
    "            where each entry represents a bias value for a specific relative position and head.\n",
    "        relative_index: Lookup table of shape (M*M, M*M), where each entry is an index\n",
    "            into relative_table that maps the relative position between two tokens to a bias vector.\n",
    "\n",
    "    Forward Output:\n",
    "        Tensor of shape (nheads, M*M, M*M) containing the relative bias for each token pair\n",
    "        and each attention head. This can be directly added to attention logits.\n",
    "\n",
    "    Example:\n",
    "        relative = RelativePositionBias(M=3, nheads=4)\n",
    "        bias = relative()  # Output shape: (4, 9, 9), for 4 heads and 3x3 tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, M, nheads):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.nheads = nheads\n",
    "        # (2M-1)^2 because there are up to M-1 tokens above or below or left or right of each token.\n",
    "        # 2M - 1 possibilities for above and below, same for left and right. So (2M -1)^2 total.\n",
    "        # If M = 3 row and col would go between -2 and +2 when comparing two tokens.\n",
    "        # That gives 5^2 possible combinations. len(-2, -1, 0, 1, 2)^2\n",
    "        self.relative_table = nn.Parameter(torch.zeros(size=((2*M - 1) * (2*M -1), nheads)))\n",
    "\n",
    "        # Coordinate grid of token positions shows where each token is in the window.\n",
    "        # It gives every token a (row, col) coordinate.\n",
    "        # If M = 3: coords[0] (rows): [[0, 0, 0], [1, 1, 1], [2, 2, 2]], \n",
    "        #           coords[1] (cols): [[0, 1, 2], [0, 1, 2], [0, 1, 2]]\n",
    "        coords = torch.stack(torch.meshgrid( # Matrix style over Cartesian\n",
    "            torch.arange(M), torch.arange(M), indexing='ij'))  # (2, M, M)\n",
    "        \n",
    "        # Flatten the coordinates for token indices, so coords[:, i] is the (row, col) of token i\n",
    "        coords = coords.flatten(1) # (2, M*M)\n",
    "\n",
    "        # Compute relative positions so we have the position of a token relative to another token\n",
    "        # coords[:, :, None] shape: (2, M*M, 1), coords[:, None, :] shape: (2, 1, M*M)\n",
    "        relative = coords[:, :, None] - coords[:, None, :] # (2, M*M, M*M)\n",
    "\n",
    "        # Reformat so we can use each (row, col) as an index into a table but row/col values\n",
    "        # range from -(M-1) to (M-1) so we shift them up so they are positive: [0, 2M -2]\n",
    "        relative = relative.permute(1, 2, 0) # (M*M, M*M, 2)\n",
    "        relative[:, :, 0] += M - 1\n",
    "        relative[:, :, 1] += M - 1\n",
    "\n",
    "        # Flatten 2D positions into 1D. To convert: row * num_cols + col\n",
    "        self.register_buffer(   # Register buffer to move to GPU\n",
    "            \"relative_index\",\n",
    "            (relative[:, :, 0] * (2*M - 1) + relative[:, :, 1]).long() # (M*M, M*M)\n",
    "        )\n",
    "    def forward(self): \n",
    "        # Use index to get bias values, look up the bias vector for each token pair\n",
    "        bias = self.relative_table[self.relative_index.view(-1)] # (M*M * M*M, nheads)\n",
    "        bias = bias.reshape(self.relative_index.shape[0], self.relative_index.shape[1], self.nheads)\n",
    "        return bias.permute(2, 0, 1) # (nheads, M*M, M*M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0aa75b",
   "metadata": {
    "papermill": {
     "duration": 0.004382,
     "end_time": "2025-08-15T23:26:37.358609",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.354227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Windowed Multi-Head Self-Attention (W-MSA)\n",
    "\n",
    "Implements Windowed Multi-head Self-Attention, which is a more efficient version of the standard attention used in ViT. Instead of calculating attention across all patches in the entire image, W-MSA computes attention within M x M windows. Significantly reduces the number of calculations needed.\n",
    "\n",
    "Attention(Q, K, V) = Softmax((QK.T / √d) + B + mask) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09030d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.368168Z",
     "iopub.status.busy": "2025-08-15T23:26:37.367934Z",
     "iopub.status.idle": "2025-08-15T23:26:37.375371Z",
     "shell.execute_reply": "2025-08-15T23:26:37.374865Z"
    },
    "papermill": {
     "duration": 0.013458,
     "end_time": "2025-08-15T23:26:37.376392",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.362934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window-based Multihead Self-Attention with relative position bias.\n",
    "\n",
    "    Performs self-attention within non overlapping MxM windows of the input feature map.\n",
    "    It incorporates relative positional encoding and attention masks for shifted windows\n",
    "\n",
    "    Attention(Q, K, V) = Softmax((QK.T / √d) + B + mask) @ V\n",
    "    B is relative position bias.\n",
    "    Args:\n",
    "        channels: Input channels\n",
    "        M: Height and width of the attention window.\n",
    "        nheads: Number of attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        q, k, v: Linear layers for queries, keys, and values.\n",
    "        out: Output linear layer after attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, M, nheads):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.nheads = nheads\n",
    "        self.rootd = (channels // nheads) ** -0.5 # (1 / √d) == (1 / √dim_per_head)\n",
    "\n",
    "        self.q = nn.Linear(channels, channels)\n",
    "        self.k = nn.Linear(channels, channels)\n",
    "        self.v = nn.Linear(channels, channels)\n",
    "        self.out = nn.Linear(channels, channels)\n",
    "\n",
    "        self.relative = RelativePositionBias(M, nheads)\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for window based self-attention.\n",
    "\n",
    "        Args:\n",
    "            x: shape (B * nW, M*M, channels) where nW is number of windows\n",
    "            attn_mask: Attention mask used for shifted windows to prevent cross-window attention.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (B*nW, M*M, channels)\n",
    "        \"\"\"\n",
    "        # x shape = (B * nW, M*M, channels) where nW is number of windows\n",
    "        BnW, M_sq, channels = x.shape # M_sq is M*M\n",
    "\n",
    "        # d stands for dim_per_head. Permute on k so no transpose when computing attn.\n",
    "        # q: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> transpose(1, 2) --> (BnW, heads, M*M, d)\n",
    "        # k: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> permute(0, 2, 3, 1) --> (BnW, heads, d, M*M)\n",
    "        # v: (BnW, M*M, channels) --> (BnW, M*M, nheads, d) --> transpose(1, 2) --> (BnW, heads, M*M, d)\n",
    "        q = self.q(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).transpose(1, 2)\n",
    "        k = self.k(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).permute(0, 2, 3, 1)\n",
    "        v = self.v(x).reshape(BnW, M_sq, self.nheads, channels // self.nheads).transpose(1, 2)\n",
    "\n",
    "        # Attention: (Q @ K.T) / √d + relative bias + optional mask\n",
    "        # k is already transposed.\n",
    "        attn = (q @ k) * self.rootd # (BnW, nheads, M*M, M*M)\n",
    "        attn = attn + self.relative()\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (M*M, M*M) --> unsqueeze(0) --> (1, M*M, M*M)\n",
    "            # Broadcasted to (BnW, nheads, M*M, M*M)\n",
    "            nW = attn_mask.shape[0]\n",
    "            BS = BnW // nW\n",
    "            attn_mask = attn_mask.to(attn.device)\n",
    "            attn = attn.view(BS, nW, self.nheads, M_sq, M_sq) + attn_mask.unsqueeze(0).unsqueeze(2)\n",
    "            attn = attn.view(BnW, self.nheads, M_sq, M_sq)\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(BnW, M_sq, channels)\n",
    "        return self.out(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8968967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.385847Z",
     "iopub.status.busy": "2025-08-15T23:26:37.385671Z",
     "iopub.status.idle": "2025-08-15T23:26:37.391142Z",
     "shell.execute_reply": "2025-08-15T23:26:37.390613Z"
    },
    "papermill": {
     "duration": 0.011487,
     "end_time": "2025-08-15T23:26:37.392245",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.380758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_attention_mask(H, W, M, shift):\n",
    "    \"\"\"\n",
    "    Creates an attention mask for shifted window self-attention (SW-MSA).\n",
    "\n",
    "    This function generates a mask to prevent tokens from attending across windows when \n",
    "    performing SW-MSA. It divides the feature map into distinct regions, assigns unique labels\n",
    "    to each, uses cyclic shifting, partitions it into non overlapping windows, and then\n",
    "    builds an attention mask that blocks attention between different labeled regions.\n",
    "\n",
    "    Args:\n",
    "        H: Height of the feature map.\n",
    "        W: Width of the feature map.\n",
    "        M: Window size.\n",
    "        shift: Number of pixels to cyclically shift the window. \n",
    "               If shift is 0, no mask is needed. In Swin it is M // 2\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (nW, M*M, M*M) where nW is the number of windows.\n",
    "        Or None if shift is 0.\n",
    "    \"\"\"\n",
    "    if shift == 0:\n",
    "        return None\n",
    "    \n",
    "    img_mask = torch.zeros((1, H, W, 1))  # Mask\n",
    "\n",
    "    count = 0\n",
    "    H, W = img_mask.shape[1:3]\n",
    "\n",
    "    # Split image into 9 regions\n",
    "    h_ranges = [(0, H - M), (H - M, H - shift), (H - shift, H)]\n",
    "    w_ranges = [(0, W - M), (W - M, W - shift), (W - shift, W)]\n",
    "\n",
    "    # so if H = W = 12, M = 6, shift = 3\n",
    "    # h_ranges = [(0, 6), (6, 9), (9, 12)]\n",
    "    # w_ranges = [(0, 6), (6, 9), (9, 12)]\n",
    "\n",
    "    # Fill each region with a unique integer\n",
    "    for h_start, h_end in h_ranges:\n",
    "        for w_start, w_end in w_ranges:\n",
    "            img_mask[:, h_start:h_end, w_start:w_end, :] = count\n",
    "            count += 1\n",
    "            \n",
    "    # Cyclic shift the mask\n",
    "    img_mask = torch.roll(img_mask, shifts=(-shift, -shift), dims=(1,2))\n",
    "\n",
    "    # Split into M*M windows\n",
    "    mask_windows = split_into_windows(img_mask, M)  # (nW, M*M, 1)\n",
    "    mask_windows = mask_windows.squeeze(-1)       # (nW, M*M)\n",
    "    # Create attention mask\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)  # (nW, M*M, M*M)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float('-inf')).masked_fill(attn_mask == 0, 0.0)\n",
    "    return attn_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e36187bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.401882Z",
     "iopub.status.busy": "2025-08-15T23:26:37.401494Z",
     "iopub.status.idle": "2025-08-15T23:26:37.425541Z",
     "shell.execute_reply": "2025-08-15T23:26:37.425012Z"
    },
    "papermill": {
     "duration": 0.02977,
     "end_time": "2025-08-15T23:26:37.426509",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.396739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "         ...,\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.],\n",
       "         [-inf, -inf, -inf,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_attention_mask(12, 12, 6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449e186",
   "metadata": {
    "papermill": {
     "duration": 0.004351,
     "end_time": "2025-08-15T23:26:37.435479",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.431128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stochastic Depth\n",
    "\n",
    "Stochastic depth is a regularization technique used to improve generalization and reduce overfitting. Instead of dropping individual neurons, entire residual branches are skipped during training with a given probability (drop_prob)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b45494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.445478Z",
     "iopub.status.busy": "2025-08-15T23:26:37.445063Z",
     "iopub.status.idle": "2025-08-15T23:26:37.449618Z",
     "shell.execute_reply": "2025-08-15T23:26:37.449151Z"
    },
    "papermill": {
     "duration": 0.010685,
     "end_time": "2025-08-15T23:26:37.450652",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.439967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stochastic_depth(x, drop_prob, training):\n",
    "    \"\"\"\n",
    "    Applies stochastic depth to the input.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        drop_prob: Probability of dropping the path.\n",
    "        training: If True, stochastic depth is applied. If False, input is returned unchanged.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor with some residual paths zeroed out.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "\n",
    "    keep_prob = 1.0 - drop_prob\n",
    "    # Create mask with shape (BS, 1, 1, ..., 1) so it broadcasts over all non batch dims\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "    # 1 with prob keep_prob, 0 with prob drop_prob\n",
    "    mask = torch.rand(shape, dtype=x.dtype, device=x.device) < keep_prob\n",
    "    # Scale\n",
    "    x = x / keep_prob\n",
    "    return x * mask\n",
    "\n",
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"\n",
    "    Module for stochastic depth.\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob):\n",
    "        super().__init__()\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return stochastic_depth(x, self.drop_prob, self.training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87ae48",
   "metadata": {
    "papermill": {
     "duration": 0.004278,
     "end_time": "2025-08-15T23:26:37.459549",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.455271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SwinBlock (Shifted Window Transformer Block)\n",
    "\n",
    "Applies windowed self-attention over non overlapping M×M windows and alternates between non-shifted and shifted windows across consecutive blocks to enable cross-window connections. There is also an MLP with GELU at the end. Residual connections are used.\n",
    "\n",
    "![Block Architecture](figures/block_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f29737a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.469317Z",
     "iopub.status.busy": "2025-08-15T23:26:37.469118Z",
     "iopub.status.idle": "2025-08-15T23:26:37.476177Z",
     "shell.execute_reply": "2025-08-15T23:26:37.475638Z"
    },
    "papermill": {
     "duration": 0.013318,
     "end_time": "2025-08-15T23:26:37.477216",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.463898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer block with shifted/non-shifted window self-attention + MLP.\n",
    "\n",
    "    Args:\n",
    "        dim: Channel dimension of the input features.\n",
    "        H: Feature map height.\n",
    "        W: Feature map width.\n",
    "        nheads: Number of attention heads in WindowAttention.\n",
    "        M: Window size.\n",
    "        shift: Cyclic shift size (0 for non-shifted windows, M//2 for shifted).\n",
    "        ratio: Expansion ratio for the MLP hidden size (hidden dim = ratio * dim).\n",
    "        stoch_depth: stochastic depth probability for dropping residual branches.\n",
    "\n",
    "    Attributes:\n",
    "        norm1: Pre attention normalization.\n",
    "        attn (WindowAttention): Window-based multi-head self-attention.\n",
    "        drop_path: Stochastic depth module or identity if stoch_depth == 0.\n",
    "        norm2: Pre MLP normalization.\n",
    "        mlp: Two-layer feed forward network with GELU activation.\n",
    "        attn_mask: Mask for shifted attention, shape (nW, M*M, M*M) when shift > 0, else None.\n",
    "\n",
    "    Input:\n",
    "        x: Tensor of shape (BS, H, W, channels).\n",
    "\n",
    "    Output:\n",
    "        Tensor of shape (BS, H, W, Channels), same spatial shape and channels as input.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, H, W, nheads, M, shift, ratio, stoch_depth):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "        self.shift = shift\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(channels=dim, M=M, nheads=nheads)\n",
    "        if stoch_depth > 0:\n",
    "            self.stoch = StochasticDepth(stoch_depth)\n",
    "        else:\n",
    "            self.stoch = nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim * ratio)), # Error I had before, forgot to wrap with int()\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim * ratio), dim)\n",
    "        )\n",
    "        self.attn_mask = create_attention_mask(H, W, M, shift)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of SwinBlock.\n",
    "\n",
    "        LayerNorm --> cyclic shift --> split windows --> WindowAttention (masked if shifted)\n",
    "        --> reverse windows --> reverse shift --> residual + stochastic depth --> LayerNorm\n",
    "        --> MLP → residual +stochastic depth\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (BS, H, W, channels).\n",
    "        \"\"\"\n",
    "        BS, H, W, channels = x.shape\n",
    "\n",
    "        store = x # For Residual connection\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        if self.shift > 0:\n",
    "            x = torch.roll(x, shifts=(-self.shift, -self.shift), dims=(1, 2)) # Cyclic shift\n",
    "        x_windows = split_into_windows(x, self.M)                  # (BS*nW, M*M, channels)\n",
    "        x_windows = self.attn(x_windows, attn_mask=self.attn_mask) # (BS*nW, M*M, channels)\n",
    "        x = reverse_windows(x_windows, self.M, H, W, channels)     # (BS, H, W, channels)\n",
    "        if self.shift > 0:\n",
    "            x = torch.roll(x, shifts=(self.shift, self.shift), dims=(1, 2))\n",
    "\n",
    "        x = store + self.stoch(x)\n",
    "\n",
    "        return x + self.stoch(self.mlp(self.norm2(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0407a20b",
   "metadata": {
    "papermill": {
     "duration": 0.004269,
     "end_time": "2025-08-15T23:26:37.485915",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.481646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Patch Merging Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba896600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.495508Z",
     "iopub.status.busy": "2025-08-15T23:26:37.495315Z",
     "iopub.status.idle": "2025-08-15T23:26:37.500246Z",
     "shell.execute_reply": "2025-08-15T23:26:37.499720Z"
    },
    "papermill": {
     "duration": 0.010954,
     "end_time": "2025-08-15T23:26:37.501341",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.490387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    \"\"\"\n",
    "    Reduces H and W by 2x in each dimension. Doubles the channel dimension.\n",
    "    - Extract non overlapping 2x2 patches from the feature map.\n",
    "    - Concatenate features from each patch along the channel dimension.\n",
    "    - Apply LayerNorm for normalization across channels.\n",
    "    - Lower 4*channels down to 2*channels with a Linear layer.\n",
    "\n",
    "    Input:\n",
    "        x: shape (BS, H, W, channels)\n",
    "\n",
    "    Output:\n",
    "        shape (BS, H/2, W/2, 2*channels)\n",
    "\n",
    "    H and W should be even.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4*dim, 2*dim)\n",
    "        self.norm = nn.LayerNorm(4*dim) # 4 * channels --> 2 * channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (BS, H, W, channels)\n",
    "        x1 = x[:, 0::2, 0::2, :]\n",
    "        x2 = x[:, 1::2, 0::2, :]\n",
    "        x3 = x[:, 0::2, 1::2, :]\n",
    "        x4 = x[:, 1::2, 1::2, :]\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=3) # (BS, H/2, W/2, 4*channels)\n",
    "        x = self.norm(x)\n",
    "        return self.lin(x) # (BS, H/2, W/2, 2*channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda64e61",
   "metadata": {
    "papermill": {
     "duration": 0.004364,
     "end_time": "2025-08-15T23:26:37.510111",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.505747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stage (Stack of Swin Blocks + Optional Patch Merging)\n",
    "\n",
    "A Stage stacks SwinBlocks, alternating between:\n",
    "- W-MSA (non-shifted windows, shift=0)\n",
    "- SW-MSA (shifted windows, shift=M//2)\n",
    "\n",
    "Then optionally applies PatchMerging to downsample and increase channels for the next stage.\n",
    "After PatchMerging, the next stage should be constructed with dim = 2 * previous_dim and H and W halved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e116258",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.519597Z",
     "iopub.status.busy": "2025-08-15T23:26:37.519406Z",
     "iopub.status.idle": "2025-08-15T23:26:37.524423Z",
     "shell.execute_reply": "2025-08-15T23:26:37.523911Z"
    },
    "papermill": {
     "duration": 0.010944,
     "end_time": "2025-08-15T23:26:37.525442",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.514498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Stage(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dim: Channel dimension.\n",
    "        H: Feature map height.\n",
    "        W: Feature map width.\n",
    "        M: Window size.\n",
    "        blocks: Number of SwinBlocks in this stage.\n",
    "        nheads: Number of attention heads per block.\n",
    "        stoch_depth_list: List of stochastic depth probabilities (len == blocks).\n",
    "        patch_merging: If True, apply PatchMerging at the end of the stage.\n",
    "        ratio: MLP expansion ratio (hidden dim = ratio * dim).\n",
    "\n",
    "    Input:\n",
    "        x: Tensor of shape (BS, H, W, channels or dim)\n",
    "\n",
    "    Output:\n",
    "        - If patch_merging is False: (BS, H, W, dim)\n",
    "        - If patch_merging is True:  (BS, H/2, W/2, 2*dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, H, W, M, blocks, nheads, stoch_depth_list, patch_merging, ratio):\n",
    "        # stoch_depth_list is a list of the stochastic depth rates for each SwinBlock.\n",
    "\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(blocks):\n",
    "            if i % 2 == 0:\n",
    "                shift = 0  # Alternate between W-MSA and SW-MSA, W-MSA has no shift.\n",
    "            else:\n",
    "                shift = M // 2\n",
    "            self.blocks.append(\n",
    "                SwinBlock(dim, H, W, nheads, M, shift, ratio, stoch_depth_list[i])\n",
    "            )\n",
    "        if patch_merging:\n",
    "            self.patch = PatchMerging(dim)\n",
    "        else:\n",
    "            self.patch = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape = (BS, H, W, channels)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.patch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e304ba35",
   "metadata": {
    "papermill": {
     "duration": 0.00433,
     "end_time": "2025-08-15T23:26:37.534284",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.529954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameters\n",
    "\n",
    "- Patch size: 4 × 4\n",
    "- Base embed dim C: 96\n",
    "- Depths: [2, 2, 6, 2]\n",
    "- Num heads: [3, 6, 12, 24]\n",
    "- Window size: 7 for all blocks\n",
    "- Shift: 3 (7 // 2) for all shift blocks\n",
    "- MLP expantion ratio: 4.0 for all blocks\n",
    "- Drop path rate: 0.2 (linearly increased across all blocks)\n",
    "- Patch Merging / Downsample at the end of Stage 1, 2 and 3.\n",
    "\n",
    "\n",
    "| Stage         | Blocks | Heads | Stoch_dep | In Channels | Out Channels | Output Shape             |\n",
    "|---------------|--------|-------|-----------|-------------|--------------|--------------------------|\n",
    "| PatchEmbed    | None   | None  | None           | 3           | 96           | (BS, H/4, W/4, 96)       |\n",
    "| Stage 1       | 2      | 3     | [0.0000, 0.0182]          | 96          | 192          | (BS, H/8, W/8, 192)      |\n",
    "| Stage 2       | 2      | 6     | [0.0364, 0.0545]          | 192         | 384          | (BS, H/16, W/16, 384)    |\n",
    "| Stage 3       | 6      | 12    | [0.0727, 0.0909, 0.1091, 0.1273, 0.1455, 0.1636]          | 384         | 768          | (BS, H/32, W/32, 768)    |\n",
    "| Stage 4       | 2      | 24    | [0.1818, 0.2000]          | 768         | 768          | (BS, H/32, W/32, 768)    |\n",
    "| Head          | None   | None  | None          | 768         | num_classes  | (BS, num_classes)        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af89098",
   "metadata": {
    "papermill": {
     "duration": 0.004348,
     "end_time": "2025-08-15T23:26:37.543043",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.538695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Swin Transformer Architecture\n",
    "\n",
    "![Architecture](figures/Architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39bb89f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.552810Z",
     "iopub.status.busy": "2025-08-15T23:26:37.552619Z",
     "iopub.status.idle": "2025-08-15T23:26:37.560484Z",
     "shell.execute_reply": "2025-08-15T23:26:37.559921Z"
    },
    "papermill": {
     "duration": 0.014042,
     "end_time": "2025-08-15T23:26:37.561516",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.547474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer\n",
    "\n",
    "    Steps:\n",
    "        - Patchify to get (BS, H/patch, W/patch, emb_dim)\n",
    "        - 4 stages of Swin blocks with alternating W-MSA (shift=0) and SW-MSA (shift=M//2)\n",
    "        - Patch Merging at the end of stages 1-3\n",
    "        - Global average pooling and linear classifier head\n",
    "\n",
    "    Args:\n",
    "        img_size: Input image size\n",
    "        patch_size: Patch size for Patchify.\n",
    "        emb_dim: Base embedding dimension channel for stage 1.\n",
    "        blocks (List[int]): Number of blocks per stage.\n",
    "        nheads (List[int]): Number of attention heads per stage.\n",
    "        M: Window size for all blocks.\n",
    "        n_classes: Number of output classes for the classifier head.\n",
    "        stochastic_endpoint: Stochastic depth ratio endpoint for linspace.\n",
    "\n",
    "    Shapes:\n",
    "        Input:  (BS, 3, 224, 224)\n",
    "        After patchify: (BS, 56, 56, 96)\n",
    "        After stage1:   (BS, 28, 28, 192)\n",
    "        After stage2:   (BS, 14, 14, 384)\n",
    "        After stage3:   (BS, 7, 7, 768)\n",
    "        After stage4:   (BS, 7, 7, 768)\n",
    "        Output logits:  (BS, n_classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, emb_dim, blocks,\n",
    "                 nheads, M, n_classes, stochastic_endpoint):\n",
    "        super().__init__()\n",
    "\n",
    "        H = img_size // patch_size\n",
    "        W = img_size // patch_size\n",
    "        dims = [emb_dim, 2*emb_dim, 4*emb_dim, 8*emb_dim]\n",
    "\n",
    "        self.patchify = Patchify(3, emb_dim, patch_size)\n",
    "\n",
    "        # Linearly increase across all blocks (inclusive endpoints)\n",
    "        stoch_depth = list(np.linspace(0, stochastic_endpoint, sum(blocks)))\n",
    "        \n",
    "        # 4 Stages with stochastic depth aligned with the number of blocks in each stage.\n",
    "        ind = 0\n",
    "        self.stage1 = Stage(\n",
    "            dims[0], H, W, M[0], \n",
    "            blocks[0], nheads[0], \n",
    "            stoch_depth[ind:ind+blocks[0]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[0]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage2 = Stage(\n",
    "            dims[1], H, W, M[1], \n",
    "            blocks[1], nheads[1], \n",
    "            stoch_depth[ind:ind+blocks[1]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[1]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage3 = Stage(\n",
    "            dims[2], H, W, M[2], \n",
    "            blocks[2], nheads[2], \n",
    "            stoch_depth[ind:ind+blocks[2]], \n",
    "            patch_merging=True, ratio=4.0)\n",
    "        ind += blocks[2]\n",
    "        H //= 2\n",
    "        W //= 2\n",
    "        self.stage4 = Stage(\n",
    "            dims[3], H, W, M[3], \n",
    "            blocks[3], nheads[3], \n",
    "            stoch_depth[ind:ind+blocks[3]], \n",
    "            patch_merging=False, ratio=4.0)\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[3])\n",
    "        self.head = nn.Linear(dims[3], n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (BS, 3, img_size, img_size).\n",
    "\n",
    "        Returns:\n",
    "            Class logits of shape (BS, n_classes).\n",
    "        \"\"\"\n",
    "        # x shape = (BS, 3, 224, 224)\n",
    "        x = self.patchify(x) # (BS, H//patch_size, W//patch_size, embed_dim)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.norm(x).mean(dim=(1, 2)) # Global average pooling over H, W.\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa884367",
   "metadata": {
    "papermill": {
     "duration": 0.004363,
     "end_time": "2025-08-15T23:26:37.570512",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.566149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Soft Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3e67a66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.580359Z",
     "iopub.status.busy": "2025-08-15T23:26:37.580171Z",
     "iopub.status.idle": "2025-08-15T23:26:37.585048Z",
     "shell.execute_reply": "2025-08-15T23:26:37.584540Z"
    },
    "papermill": {
     "duration": 0.011147,
     "end_time": "2025-08-15T23:26:37.586136",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.574989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def one_hot(labels: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
    "    return F.one_hot(labels, num_classes=num_classes).float()\n",
    "\n",
    "class SoftCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    If target is LongTensor -> falls back to nn.CrossEntropyLoss (hard labels).\n",
    "    If target is FloatTensor (N,C) -> computes soft CE: -sum(p * log_softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, label_smoothing: float = 0.0):\n",
    "        super().__init__()\n",
    "        # If you use Mixup/CutMix, set smoothing=0.0 to avoid double softening.\n",
    "        self.ce = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        if target.dtype == torch.long:\n",
    "            return self.ce(logits, target)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(target * log_probs).sum(dim=1).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6d404",
   "metadata": {
    "papermill": {
     "duration": 0.004419,
     "end_time": "2025-08-15T23:26:37.595135",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.590716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mixup + CutMix module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef2fa367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.605316Z",
     "iopub.status.busy": "2025-08-15T23:26:37.605124Z",
     "iopub.status.idle": "2025-08-15T23:26:37.615714Z",
     "shell.execute_reply": "2025-08-15T23:26:37.615250Z"
    },
    "papermill": {
     "duration": 0.017001,
     "end_time": "2025-08-15T23:26:37.616737",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.599736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def _rand_bbox(W: int, H: int, lam: float):\n",
    "    # CutMix box size from area ratio lam\n",
    "    cut_rat = (1.0 - lam) ** 0.5\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    cx = random.randint(0, W - 1)\n",
    "    cy = random.randint(0, H - 1)\n",
    "    x1 = max(cx - cut_w // 2, 0)\n",
    "    y1 = max(cy - cut_h // 2, 0)\n",
    "    x2 = min(cx + cut_w // 2, W)\n",
    "    y2 = min(cy + cut_h // 2, H)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "class MixupCutmix:\n",
    "    \"\"\"\n",
    "    On each batch, applies Mixup or CutMix with given probabilities.\n",
    "    Returns:\n",
    "      images: possibly mixed tensor\n",
    "      targets: either Long (no mix) or Float one-hot (mixed)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int,\n",
    "                 mixup_alpha: float = 0.8,\n",
    "                 cutmix_alpha: float = 1.0,\n",
    "                 p_mixup: float = 0.5,\n",
    "                 p_cutmix: float = 0.5):\n",
    "        self.num_classes = num_classes\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.cutmix_alpha = cutmix_alpha\n",
    "        self.p_mixup = p_mixup\n",
    "        self.p_cutmix = p_cutmix\n",
    "        self.enabled = True\n",
    "\n",
    "    def off(self):  self.enabled = False\n",
    "    def on(self):   self.enabled = True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, images: torch.Tensor, targets: torch.Tensor):\n",
    "        if (not self.enabled) or (self.p_mixup <= 0 and self.p_cutmix <= 0):\n",
    "            return images, targets  # no change\n",
    "\n",
    "        B, C, H, W = images.shape\n",
    "        # decide op\n",
    "        op = None\n",
    "        r = random.random()\n",
    "        if r < self.p_mixup:\n",
    "            op = 'mixup'\n",
    "        elif r < self.p_mixup + self.p_cutmix:\n",
    "            op = 'cutmix'\n",
    "        else:\n",
    "            return images, targets  # no change\n",
    "\n",
    "        # sample lambda from Beta\n",
    "        from torch.distributions import Beta\n",
    "\n",
    "        if op == 'mixup' and self.mixup_alpha > 0:\n",
    "            lam = Beta(self.mixup_alpha, self.mixup_alpha).sample().item()\n",
    "        elif op == 'cutmix' and self.cutmix_alpha > 0:\n",
    "            lam = Beta(self.cutmix_alpha, self.cutmix_alpha).sample().item()\n",
    "        else:\n",
    "            return images, targets\n",
    "\n",
    "        lam = max(min(lam, 0.999), 0.001)\n",
    "\n",
    "        # shuffle\n",
    "        index = torch.randperm(B, device=images.device)\n",
    "        y1 = one_hot(targets, self.num_classes).to(images.dtype)\n",
    "        y2 = one_hot(targets[index], self.num_classes).to(images.dtype)\n",
    "\n",
    "        if op == 'mixup':\n",
    "            mixed = lam * images + (1.0 - lam) * images[index]\n",
    "            y = lam * y1 + (1.0 - lam) * y2\n",
    "            return mixed, y\n",
    "\n",
    "        # CutMix\n",
    "        x1, y1b, x2, y2b = _rand_bbox(W, H, lam)\n",
    "        mixed = images.clone()\n",
    "        mixed[:, :, y1b:y2b, x1:x2] = images[index, :, y1b:y2b, x1:x2]\n",
    "\n",
    "        # adjust lam to actual area\n",
    "        box_area = (x2 - x1) * (y2b - y1b)\n",
    "        lam_adj = 1.0 - float(box_area) / float(W * H)\n",
    "        y = lam_adj * one_hot(targets, self.num_classes).to(images.dtype) + \\\n",
    "            (1.0 - lam_adj) * one_hot(targets[index], self.num_classes).to(images.dtype)\n",
    "        return mixed, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da5cadfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T23:26:37.626966Z",
     "iopub.status.busy": "2025-08-15T23:26:37.626792Z",
     "iopub.status.idle": "2025-08-16T02:32:51.765925Z",
     "shell.execute_reply": "2025-08-16T02:32:51.764943Z"
    },
    "papermill": {
     "duration": 11174.155147,
     "end_time": "2025-08-16T02:32:51.776504",
     "exception": false,
     "start_time": "2025-08-15T23:26:37.621357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/695869955.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
      "/tmp/ipykernel_19/695869955.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
      "/tmp/ipykernel_19/695869955.py:104: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n",
      "Epoch 1, Train Loss: 5.1850, Val Loss: 4.9962, Top1 Acc: 0.0270, Top5 Acc: 0.1152, LR 0.000062\n",
      "Saved new best model.\n",
      "Epoch 2, Train Loss: 5.0440, Val Loss: 4.5424, Top1 Acc: 0.0723, Top5 Acc: 0.2299, LR 0.000122\n",
      "Saved new best model.\n",
      "Epoch 3, Train Loss: 4.8414, Val Loss: 4.2687, Top1 Acc: 0.1066, Top5 Acc: 0.2941, LR 0.000181\n",
      "Saved new best model.\n",
      "Epoch 4, Train Loss: 4.7136, Val Loss: 4.0153, Top1 Acc: 0.1461, Top5 Acc: 0.3607, LR 0.000241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n",
      "Epoch 5, Train Loss: 4.5841, Val Loss: 3.8532, Top1 Acc: 0.1622, Top5 Acc: 0.3987, LR 0.000300\n",
      "Saved new best model.\n",
      "Epoch 6, Train Loss: 4.5019, Val Loss: 3.7523, Top1 Acc: 0.1792, Top5 Acc: 0.4250, LR 0.000300\n",
      "Saved new best model.\n",
      "Epoch 7, Train Loss: 4.3795, Val Loss: 3.5294, Top1 Acc: 0.2119, Top5 Acc: 0.4678, LR 0.000300\n",
      "Saved new best model.\n",
      "Epoch 8, Train Loss: 4.3156, Val Loss: 3.3781, Top1 Acc: 0.2475, Top5 Acc: 0.5099, LR 0.000299\n",
      "Saved new best model.\n",
      "Epoch 9, Train Loss: 4.2338, Val Loss: 3.3297, Top1 Acc: 0.2560, Top5 Acc: 0.5195, LR 0.000299\n",
      "Saved new best model.\n",
      "Epoch 10, Train Loss: 4.1547, Val Loss: 3.2511, Top1 Acc: 0.2642, Top5 Acc: 0.5329, LR 0.000298\n",
      "Saved new best model.\n",
      "Epoch 11, Train Loss: 4.0922, Val Loss: 3.1043, Top1 Acc: 0.2942, Top5 Acc: 0.5671, LR 0.000297\n",
      "Saved new best model.\n",
      "Epoch 12, Train Loss: 4.0491, Val Loss: 3.0413, Top1 Acc: 0.3121, Top5 Acc: 0.5777, LR 0.000296\n",
      "Saved new best model.\n",
      "Epoch 13, Train Loss: 3.9664, Val Loss: 2.9725, Top1 Acc: 0.3172, Top5 Acc: 0.5956, LR 0.000295\n",
      "Saved new best model.\n",
      "Epoch 14, Train Loss: 3.9403, Val Loss: 2.9687, Top1 Acc: 0.3181, Top5 Acc: 0.5970, LR 0.000293\n",
      "Saved new best model.\n",
      "Epoch 15, Train Loss: 3.9074, Val Loss: 2.8932, Top1 Acc: 0.3350, Top5 Acc: 0.6117, LR 0.000292\n",
      "Saved new best model.\n",
      "Epoch 16, Train Loss: 3.8757, Val Loss: 2.8392, Top1 Acc: 0.3462, Top5 Acc: 0.6231, LR 0.000290\n",
      "Saved new best model.\n",
      "Epoch 17, Train Loss: 3.8317, Val Loss: 2.8012, Top1 Acc: 0.3550, Top5 Acc: 0.6277, LR 0.000288\n",
      "Saved new best model.\n",
      "Epoch 18, Train Loss: 3.8041, Val Loss: 2.7571, Top1 Acc: 0.3618, Top5 Acc: 0.6400, LR 0.000286\n",
      "Saved new best model.\n",
      "Epoch 19, Train Loss: 3.7876, Val Loss: 2.7071, Top1 Acc: 0.3701, Top5 Acc: 0.6467, LR 0.000284\n",
      "Saved new best model.\n",
      "Epoch 20, Train Loss: 3.7648, Val Loss: 2.7144, Top1 Acc: 0.3710, Top5 Acc: 0.6491, LR 0.000282\n",
      "Saved new best model.\n",
      "Epoch 21, Train Loss: 3.7314, Val Loss: 2.6356, Top1 Acc: 0.3893, Top5 Acc: 0.6621, LR 0.000279\n",
      "Epoch 22, Train Loss: 3.7045, Val Loss: 2.6630, Top1 Acc: 0.3853, Top5 Acc: 0.6577, LR 0.000277\n",
      "Saved new best model.\n",
      "Epoch 23, Train Loss: 3.6684, Val Loss: 2.6063, Top1 Acc: 0.3917, Top5 Acc: 0.6652, LR 0.000274\n",
      "Saved new best model.\n",
      "Epoch 24, Train Loss: 3.6272, Val Loss: 2.5427, Top1 Acc: 0.4093, Top5 Acc: 0.6769, LR 0.000271\n",
      "Saved new best model.\n",
      "Epoch 25, Train Loss: 3.6178, Val Loss: 2.5393, Top1 Acc: 0.4094, Top5 Acc: 0.6803, LR 0.000268\n",
      "Epoch 26, Train Loss: 3.5893, Val Loss: 2.5369, Top1 Acc: 0.4072, Top5 Acc: 0.6788, LR 0.000265\n",
      "Saved new best model.\n",
      "Epoch 27, Train Loss: 3.5326, Val Loss: 2.4871, Top1 Acc: 0.4151, Top5 Acc: 0.6907, LR 0.000262\n",
      "Saved new best model.\n",
      "Epoch 28, Train Loss: 3.5516, Val Loss: 2.4932, Top1 Acc: 0.4205, Top5 Acc: 0.6883, LR 0.000259\n",
      "Saved new best model.\n",
      "Epoch 29, Train Loss: 3.4944, Val Loss: 2.4353, Top1 Acc: 0.4255, Top5 Acc: 0.6967, LR 0.000255\n",
      "Saved new best model.\n",
      "Epoch 30, Train Loss: 3.4974, Val Loss: 2.4337, Top1 Acc: 0.4321, Top5 Acc: 0.7005, LR 0.000252\n",
      "Epoch 31, Train Loss: 3.4703, Val Loss: 2.4714, Top1 Acc: 0.4258, Top5 Acc: 0.6907, LR 0.000248\n",
      "Saved new best model.\n",
      "Epoch 32, Train Loss: 3.4586, Val Loss: 2.4034, Top1 Acc: 0.4398, Top5 Acc: 0.7021, LR 0.000244\n",
      "Epoch 33, Train Loss: 3.4244, Val Loss: 2.3944, Top1 Acc: 0.4391, Top5 Acc: 0.7022, LR 0.000240\n",
      "Saved new best model.\n",
      "Epoch 34, Train Loss: 3.4023, Val Loss: 2.3546, Top1 Acc: 0.4453, Top5 Acc: 0.7150, LR 0.000236\n",
      "Saved new best model.\n",
      "Epoch 35, Train Loss: 3.3905, Val Loss: 2.3477, Top1 Acc: 0.4530, Top5 Acc: 0.7137, LR 0.000232\n",
      "Epoch 36, Train Loss: 3.3755, Val Loss: 2.3501, Top1 Acc: 0.4510, Top5 Acc: 0.7104, LR 0.000228\n",
      "Saved new best model.\n",
      "Epoch 37, Train Loss: 3.3333, Val Loss: 2.3026, Top1 Acc: 0.4595, Top5 Acc: 0.7208, LR 0.000224\n",
      "Epoch 38, Train Loss: 3.2947, Val Loss: 2.2924, Top1 Acc: 0.4557, Top5 Acc: 0.7211, LR 0.000219\n",
      "Epoch 39, Train Loss: 3.2822, Val Loss: 2.3002, Top1 Acc: 0.4564, Top5 Acc: 0.7217, LR 0.000215\n",
      "Epoch 40, Train Loss: 3.2651, Val Loss: 2.3068, Top1 Acc: 0.4566, Top5 Acc: 0.7190, LR 0.000210\n",
      "Saved new best model.\n",
      "Epoch 41, Train Loss: 3.2323, Val Loss: 2.2748, Top1 Acc: 0.4655, Top5 Acc: 0.7235, LR 0.000206\n",
      "Epoch 42, Train Loss: 3.1968, Val Loss: 2.2733, Top1 Acc: 0.4632, Top5 Acc: 0.7257, LR 0.000201\n",
      "Epoch 43, Train Loss: 3.2006, Val Loss: 2.2636, Top1 Acc: 0.4618, Top5 Acc: 0.7265, LR 0.000196\n",
      "Saved new best model.\n",
      "Epoch 44, Train Loss: 3.1807, Val Loss: 2.2228, Top1 Acc: 0.4720, Top5 Acc: 0.7319, LR 0.000192\n",
      "Saved new best model.\n",
      "Epoch 45, Train Loss: 3.1543, Val Loss: 2.2105, Top1 Acc: 0.4740, Top5 Acc: 0.7384, LR 0.000187\n",
      "Saved new best model.\n",
      "Epoch 46, Train Loss: 3.1284, Val Loss: 2.2209, Top1 Acc: 0.4750, Top5 Acc: 0.7314, LR 0.000182\n",
      "Saved new best model.\n",
      "Epoch 47, Train Loss: 3.1276, Val Loss: 2.1894, Top1 Acc: 0.4803, Top5 Acc: 0.7375, LR 0.000177\n",
      "Epoch 48, Train Loss: 3.1184, Val Loss: 2.2199, Top1 Acc: 0.4752, Top5 Acc: 0.7333, LR 0.000172\n",
      "Saved new best model.\n",
      "Epoch 49, Train Loss: 3.0607, Val Loss: 2.1537, Top1 Acc: 0.4856, Top5 Acc: 0.7465, LR 0.000167\n",
      "Epoch 50, Train Loss: 3.0610, Val Loss: 2.1873, Top1 Acc: 0.4818, Top5 Acc: 0.7398, LR 0.000162\n",
      "Saved new best model.\n",
      "Epoch 51, Train Loss: 3.0110, Val Loss: 2.1677, Top1 Acc: 0.4914, Top5 Acc: 0.7413, LR 0.000157\n",
      "Epoch 52, Train Loss: 2.9864, Val Loss: 2.1551, Top1 Acc: 0.4886, Top5 Acc: 0.7411, LR 0.000152\n",
      "Saved new best model.\n",
      "Epoch 53, Train Loss: 3.0069, Val Loss: 2.1457, Top1 Acc: 0.4947, Top5 Acc: 0.7436, LR 0.000148\n",
      "Saved new best model.\n",
      "Epoch 54, Train Loss: 2.9555, Val Loss: 2.1223, Top1 Acc: 0.4995, Top5 Acc: 0.7490, LR 0.000143\n",
      "Epoch 55, Train Loss: 2.9576, Val Loss: 2.1281, Top1 Acc: 0.4969, Top5 Acc: 0.7464, LR 0.000138\n",
      "Epoch 56, Train Loss: 2.8908, Val Loss: 2.1365, Top1 Acc: 0.4975, Top5 Acc: 0.7489, LR 0.000133\n",
      "Epoch 57, Train Loss: 2.9320, Val Loss: 2.1367, Top1 Acc: 0.4935, Top5 Acc: 0.7461, LR 0.000128\n",
      "Epoch 58, Train Loss: 2.8852, Val Loss: 2.1386, Top1 Acc: 0.4960, Top5 Acc: 0.7437, LR 0.000123\n",
      "Saved new best model.\n",
      "Epoch 59, Train Loss: 2.8877, Val Loss: 2.1255, Top1 Acc: 0.5021, Top5 Acc: 0.7479, LR 0.000118\n",
      "Saved new best model.\n",
      "Epoch 60, Train Loss: 2.8238, Val Loss: 2.1055, Top1 Acc: 0.5045, Top5 Acc: 0.7530, LR 0.000113\n",
      "Epoch 61, Train Loss: 2.8619, Val Loss: 2.1179, Top1 Acc: 0.5018, Top5 Acc: 0.7504, LR 0.000108\n",
      "Epoch 62, Train Loss: 2.7725, Val Loss: 2.1321, Top1 Acc: 0.5013, Top5 Acc: 0.7496, LR 0.000104\n",
      "Epoch 63, Train Loss: 2.8120, Val Loss: 2.1074, Top1 Acc: 0.5044, Top5 Acc: 0.7507, LR 0.000099\n",
      "Saved new best model.\n",
      "Epoch 64, Train Loss: 2.7786, Val Loss: 2.1054, Top1 Acc: 0.5049, Top5 Acc: 0.7509, LR 0.000094\n",
      "Epoch 65, Train Loss: 2.7739, Val Loss: 2.1329, Top1 Acc: 0.5023, Top5 Acc: 0.7494, LR 0.000090\n",
      "Epoch 66, Train Loss: 2.7694, Val Loss: 2.1082, Top1 Acc: 0.5006, Top5 Acc: 0.7505, LR 0.000085\n",
      "Epoch 67, Train Loss: 2.7445, Val Loss: 2.1128, Top1 Acc: 0.5018, Top5 Acc: 0.7511, LR 0.000081\n",
      "Saved new best model.\n",
      "Epoch 68, Train Loss: 2.7204, Val Loss: 2.1019, Top1 Acc: 0.5059, Top5 Acc: 0.7507, LR 0.000076\n",
      "Epoch 69, Train Loss: 2.6826, Val Loss: 2.0930, Top1 Acc: 0.5052, Top5 Acc: 0.7545, LR 0.000072\n",
      "Saved new best model.\n",
      "Epoch 70, Train Loss: 2.6558, Val Loss: 2.0790, Top1 Acc: 0.5080, Top5 Acc: 0.7574, LR 0.000068\n",
      "Saved new best model.\n",
      "Epoch 71, Train Loss: 2.6683, Val Loss: 2.0998, Top1 Acc: 0.5103, Top5 Acc: 0.7522, LR 0.000064\n",
      "Saved new best model.\n",
      "Epoch 72, Train Loss: 2.6149, Val Loss: 2.1013, Top1 Acc: 0.5116, Top5 Acc: 0.7557, LR 0.000060\n",
      "Saved new best model.\n",
      "Epoch 73, Train Loss: 2.6729, Val Loss: 2.0972, Top1 Acc: 0.5149, Top5 Acc: 0.7532, LR 0.000056\n",
      "Saved new best model.\n",
      "Epoch 74, Train Loss: 2.6588, Val Loss: 2.0850, Top1 Acc: 0.5163, Top5 Acc: 0.7547, LR 0.000052\n",
      "Epoch 75, Train Loss: 2.5652, Val Loss: 2.0841, Top1 Acc: 0.5119, Top5 Acc: 0.7575, LR 0.000048\n",
      "Epoch 76, Train Loss: 2.5717, Val Loss: 2.1101, Top1 Acc: 0.5079, Top5 Acc: 0.7542, LR 0.000045\n",
      "Epoch 77, Train Loss: 2.6003, Val Loss: 2.0927, Top1 Acc: 0.5130, Top5 Acc: 0.7565, LR 0.000041\n",
      "Saved new best model.\n",
      "Epoch 78, Train Loss: 2.5288, Val Loss: 2.0978, Top1 Acc: 0.5166, Top5 Acc: 0.7551, LR 0.000038\n",
      "Epoch 79, Train Loss: 2.5245, Val Loss: 2.0959, Top1 Acc: 0.5140, Top5 Acc: 0.7549, LR 0.000035\n",
      "Saved new best model.\n",
      "Epoch 80, Train Loss: 2.5161, Val Loss: 2.0901, Top1 Acc: 0.5173, Top5 Acc: 0.7557, LR 0.000032\n",
      "Epoch 81, Train Loss: 2.5053, Val Loss: 2.0885, Top1 Acc: 0.5165, Top5 Acc: 0.7559, LR 0.000029\n",
      "Epoch 82, Train Loss: 2.5079, Val Loss: 2.0977, Top1 Acc: 0.5155, Top5 Acc: 0.7557, LR 0.000026\n",
      "Epoch 83, Train Loss: 2.5122, Val Loss: 2.0936, Top1 Acc: 0.5170, Top5 Acc: 0.7569, LR 0.000023\n",
      "Epoch 84, Train Loss: 2.5408, Val Loss: 2.0938, Top1 Acc: 0.5173, Top5 Acc: 0.7552, LR 0.000021\n",
      "Saved new best model.\n",
      "Epoch 85, Train Loss: 2.5265, Val Loss: 2.0833, Top1 Acc: 0.5192, Top5 Acc: 0.7564, LR 0.000018\n",
      "Epoch 86, Train Loss: 2.4448, Val Loss: 2.0843, Top1 Acc: 0.5175, Top5 Acc: 0.7586, LR 0.000016\n",
      "Epoch 87, Train Loss: 2.4942, Val Loss: 2.0886, Top1 Acc: 0.5191, Top5 Acc: 0.7574, LR 0.000014\n",
      "Saved new best model.\n",
      "Epoch 88, Train Loss: 2.4952, Val Loss: 2.0867, Top1 Acc: 0.5203, Top5 Acc: 0.7577, LR 0.000012\n",
      "Epoch 89, Train Loss: 2.5173, Val Loss: 2.0866, Top1 Acc: 0.5193, Top5 Acc: 0.7567, LR 0.000010\n",
      "Saved new best model.\n",
      "Epoch 90, Train Loss: 2.4888, Val Loss: 2.0808, Top1 Acc: 0.5209, Top5 Acc: 0.7550, LR 0.000008\n",
      "Saved new best model.\n",
      "Epoch 91, Train Loss: 0.5399, Val Loss: 2.0396, Top1 Acc: 0.5290, Top5 Acc: 0.7657, LR 0.000007\n",
      "Saved new best model.\n",
      "Epoch 92, Train Loss: 0.5086, Val Loss: 2.0378, Top1 Acc: 0.5295, Top5 Acc: 0.7681, LR 0.000005\n",
      "Epoch 93, Train Loss: 0.4910, Val Loss: 2.0408, Top1 Acc: 0.5287, Top5 Acc: 0.7690, LR 0.000004\n",
      "Epoch 94, Train Loss: 0.4864, Val Loss: 2.0413, Top1 Acc: 0.5292, Top5 Acc: 0.7677, LR 0.000003\n",
      "Epoch 95, Train Loss: 0.4777, Val Loss: 2.0458, Top1 Acc: 0.5270, Top5 Acc: 0.7685, LR 0.000002\n",
      "Epoch 96, Train Loss: 0.4654, Val Loss: 2.0459, Top1 Acc: 0.5278, Top5 Acc: 0.7682, LR 0.000001\n",
      "Epoch 97, Train Loss: 0.4649, Val Loss: 2.0457, Top1 Acc: 0.5280, Top5 Acc: 0.7686, LR 0.000001\n",
      "Epoch 98, Train Loss: 0.4638, Val Loss: 2.0466, Top1 Acc: 0.5295, Top5 Acc: 0.7682, LR 0.000000\n",
      "Epoch 99, Train Loss: 0.4612, Val Loss: 2.0466, Top1 Acc: 0.5290, Top5 Acc: 0.7679, LR 0.000000\n",
      "Epoch 100, Train Loss: 0.4604, Val Loss: 2.0465, Top1 Acc: 0.5291, Top5 Acc: 0.7681, LR 0.000000\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "import json\n",
    "\n",
    "model = SwinTransformer(\n",
    "    img_size=64, patch_size=4, emb_dim=64,\n",
    "    blocks=[2, 2, 4, 2], nheads=[2, 4, 8, 16],\n",
    "    M=[8, 8, 4, 2], n_classes=200, stochastic_endpoint=0.1).to(device)\n",
    "\n",
    "loss_fn = SoftCrossEntropy(label_smoothing=0.0)\n",
    "mixer = MixupCutmix(num_classes=200, mixup_alpha=0.8, cutmix_alpha=1.0,\n",
    "                    p_mixup=0.5, p_cutmix=0.5)\n",
    "# Scheduler Epochs\n",
    "warmup_epochs = 5  # Gradually increase lr for the first 5 epochs\n",
    "cosine_epochs = 95 # Cosine-anneal lr for the remaining 95 epochs\n",
    "\n",
    "def param_groups_weight_decay(model, weight_decay=0.05):\n",
    "    decay, no_decay = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        name = n.lower()\n",
    "        if (\n",
    "            p.ndim == 1\n",
    "            or n.endswith(\".bias\")\n",
    "            or \"relative\" in name\n",
    "            or \"pos_embed\" in name\n",
    "        ):\n",
    "            no_decay.append(p)\n",
    "        else:\n",
    "            decay.append(p)\n",
    "    return [\n",
    "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "# AdamW optimizer, weight decay taken directly from the paper.\n",
    "# fused=True uses a fused CUDA kernel\n",
    "optimizer = torch.optim.AdamW(param_groups_weight_decay(model, 0.05), lr=3e-4, betas=(0.9, 0.999), fused=True)\n",
    "\n",
    "# Linear warmup scheduler. Start at 1% of lr and gradually fo up to 100%\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_epochs)\n",
    "# Cosine annealing scheduler. Decay lr following a cosine curve\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=cosine_epochs)\n",
    "# Run warmup_scheduler first, then cosine_scheduler\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
    "# GradScaler for Automatic Mixed Precision (AMP). Saves VRAM.\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
    "\n",
    "best_top1 = 0.0\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_top1\": [],\n",
    "    \"val_top5\": [],\n",
    "    \"lr\": []\n",
    "}\n",
    "\n",
    "for epoch in range(warmup_epochs + cosine_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    if epoch < 1 or epoch >= warmup_epochs + cosine_epochs - 10:\n",
    "        mixer.off()\n",
    "    else:\n",
    "        mixer.on()\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        xb, yb = mixer(xb, yb)  # labels now (N) or (N,C)\n",
    "        optimizer.zero_grad(set_to_none=True) # set_to_none saves a bit of memory.\n",
    "        # Forward and loss in mixed precision\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "            yhat = model(xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "        # Backprop with scaled loss\n",
    "        scaler.scale(loss).backward()\n",
    "        # unscale before clipping, then clip\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # Step optimizer only goes if gradients are finite\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        n_samples += bs\n",
    "\n",
    "    # Safe averages\n",
    "    avg_loss = total_loss / n_samples\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_samples = 0\n",
    "    val_correct_top1 = 0\n",
    "    val_correct_top5 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "                yhat = model(xb)\n",
    "                loss = loss_fn(yhat, yb)\n",
    "            bs = xb.size(0)\n",
    "            val_loss += loss.item() * bs\n",
    "            val_samples += bs\n",
    "            \n",
    "            topk = torch.topk(yhat, k=5, dim=1).indices  # (B,5)\n",
    "            val_correct_top1 += (topk[:, 0] == yb).sum().item()\n",
    "            val_correct_top5 += topk.eq(yb.view(-1, 1)).any(dim=1).sum().item()\n",
    "            \n",
    "    avg_val_loss = val_loss / val_samples\n",
    "    val_top1 = val_correct_top1 / val_samples\n",
    "    val_top5 = val_correct_top5 / val_samples\n",
    "    \n",
    "    # Get current learning rate. I had this wrong before, I was grabbing the past lr instead of current\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_top1\"].append(val_top1)\n",
    "    history[\"val_top5\"].append(val_top5)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "\n",
    "    with open(\"training_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "    if val_top1 > best_top1:\n",
    "        best_top1 = val_top1\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val_top1\": best_top1,\n",
    "            \"best_val_top5\": val_top5\n",
    "        }, \"swin_t_best.pt\")\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Top1 Acc: {val_top1:.4f}, Top5 Acc: {val_top5:.4f}, LR {current_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745e4d4",
   "metadata": {
    "papermill": {
     "duration": 0.008768,
     "end_time": "2025-08-16T02:32:51.794472",
     "exception": false,
     "start_time": "2025-08-16T02:32:51.785704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "224 images by 224 was way too much for my GPU. So I went down to 64 x 64 and adjusted. I didn't want M to be 2 so I modified so each stage has a different M. Entire architecture was adapted and controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c564481",
   "metadata": {
    "papermill": {
     "duration": 0.008986,
     "end_time": "2025-08-16T02:32:51.812307",
     "exception": false,
     "start_time": "2025-08-16T02:32:51.803321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Augmentations too strong at 64×64.\n",
    "RandAugment(mag=9) + RandomResizedCrop(0.8,1.0) + RandomErasing(p=0.25) on tiny images strips too much signal. Model keeps fitting train (loss ↓) but can’t push val higher → plateau at ~37%.\n",
    "\n",
    "Relative Position Bias (RPB) was decayed.\n",
    "relative_table lived in the weight-decay group before your change. That nudges locality info toward zero over time, hurting window attention’s inductive bias → earlier “ok” gains, then flattening.\n",
    "\n",
    "Mixup/CutMix on for ~90% of training.\n",
    "Great regularizers, but at 64×64 + heavy RA they can over-regularize, making logits softer. You also validate with hard CE, so it’s common to see val_loss drift up a bit while acc stays flat (calibration mismatch).\n",
    "\n",
    "Resolution + capacity trade-off.\n",
    "At 64px, content is compressed. Your Swin-T config (C=64, windows [8,8,4,2]) is reasonable, but it simply has less separable info than 224px baselines. ~35–40% Top-1 on Tiny-IN@64 with strong regularization is plausible.\n",
    "\n",
    "Validation hygiene.\n",
    "The split is from train/ (not official val/) and not stratified in the “before” run. That adds noise and can slightly mute peak accuracy.\n",
    "\n",
    "Minor dampeners (secondary):\n",
    "\n",
    "Grad clip = 1.0 can be tight for attention; may slow learning a bit.\n",
    "\n",
    "LR schedule is fine; after ~30–40 epochs, cosine has already lowered LR a lot, making it harder to escape the regularization-limited regime.\n",
    "\n",
    "Because I kept the dimensions at 64x64, the Swin Transformer did not scale as well. Swin's are very good with large amounts of data, but when you only have 100k images with 64x64 images, it does not scale nearly as well as it should scale. I could train on ImageNets 1000k images, but I do not have the computational resources to train a model that large. I think Cosine Annealing caused the learning rate to drop a bit too fast. I will train a few hundred more epochs with a normal linear LR starting from ~0.000200, dropping to around ~0.0000100. Adding a few things caused training speed to increase by around 5x. Continuing off of epoch 91, because that had the highest validation accuracy. Definitely has not converged yet, LR was around 0.000007, it moved in the wrong directions and couldnt make up for it in the remaining 9 epochs."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7815483,
     "sourceId": 12394036,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11350.970673,
   "end_time": "2025-08-16T02:32:54.549262",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-15T23:23:43.578589",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
