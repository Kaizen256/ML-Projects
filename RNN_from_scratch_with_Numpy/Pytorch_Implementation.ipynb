{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e294dd",
   "metadata": {},
   "source": [
    "Pytorch Implementation of the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c190e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 233.7712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     78\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(output, targets[j]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m---> 80\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     82\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m seq_length\n",
      "File \u001b[1;32mc:\\Users\\rowek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rowek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rowek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. Data Preparation\n",
    "# ----------------------------------------------------\n",
    "text = \"\"\"To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles,\n",
    "And by opposing end them.\"\"\"\n",
    "\n",
    "# Make lowercase and build character vocabulary\n",
    "chars = sorted(list(set(text.lower())))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# char to index mappings\n",
    "char_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Encode the text as indices\n",
    "data = torch.tensor([char_to_idx[ch] for ch in text.lower()], dtype=torch.long)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. Hyperparameters\n",
    "# ----------------------------------------------------\n",
    "embed_size = 16\n",
    "hidden_size = 64\n",
    "lr = 0.01\n",
    "epochs = 500\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Model\n",
    "# ----------------------------------------------------\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.i2h = nn.Linear(embed_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, vocab_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        emb = self.embed(x)\n",
    "        combined = torch.cat((emb, hidden), dim=1)\n",
    "        hidden = self.tanh(self.i2h(combined))\n",
    "        output = self.h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, hidden_size)\n",
    "\n",
    "model = CharRNN(vocab_size, embed_size, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. Training\n",
    "# ----------------------------------------------------\n",
    "seq_length = 30  # number of characters to unroll\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    hidden = model.init_hidden()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # run through data in steps of seq_length\n",
    "    for i in range(0, len(data) - seq_length):\n",
    "        inputs = data[i:i+seq_length]\n",
    "        targets = data[i+1:i+seq_length+1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = 0\n",
    "        for j in range(seq_length):\n",
    "            output, hidden = model(inputs[j].unsqueeze(0), hidden)\n",
    "            hidden = hidden.detach()\n",
    "            loss += criterion(output, targets[j].unsqueeze(0))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() / seq_length\n",
    "    \n",
    "    if epoch % 50 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5. Sampling (text generation)\n",
    "# ----------------------------------------------------\n",
    "def sample(start_str=\"to \", length=200):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden()\n",
    "    input_char = torch.tensor([char_to_idx[ch] for ch in start_str], dtype=torch.long)\n",
    "    result = start_str\n",
    "    \n",
    "    # warm-up on the initial string\n",
    "    for ch in input_char:\n",
    "        output, hidden = model(ch.unsqueeze(0), hidden)\n",
    "    \n",
    "    idx = torch.multinomial(torch.softmax(output, dim=1), 1).item()\n",
    "    result += idx_to_char[idx]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        output, hidden = model(torch.tensor([[idx]]), hidden)\n",
    "        idx = torch.multinomial(torch.softmax(output, dim=1), 1).item()\n",
    "        result += idx_to_char[idx]\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"\\nSample generation:\\n\")\n",
    "print(sample(\"to \", 300))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c7c27",
   "metadata": {},
   "source": [
    "Using embedding for practice instead of one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "70533f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "20769b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "output_size = len(l)\n",
    "vocab_size = 22\n",
    "sequence_length = 20\n",
    "\n",
    "lr = 0.0005\n",
    "epochs = 250\n",
    "\n",
    "model = RNN(vocab_size, hidden_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "619fba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss.backward()\n",
    "#torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "#optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab8a1fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 3.0964248180389404\n",
      "Epoch 1 loss: 3.0840961933135986\n",
      "Epoch 2 loss: 3.072006940841675\n",
      "Epoch 3 loss: 3.060065269470215\n",
      "Epoch 4 loss: 3.0481855869293213\n",
      "Epoch 5 loss: 3.036295175552368\n",
      "Epoch 6 loss: 3.0243287086486816\n",
      "Epoch 7 loss: 3.0122263431549072\n",
      "Epoch 8 loss: 2.999936580657959\n",
      "Epoch 9 loss: 2.9874114990234375\n",
      "Epoch 10 loss: 2.974623203277588\n",
      "Epoch 11 loss: 2.9615511894226074\n",
      "Epoch 12 loss: 2.948195457458496\n",
      "Epoch 13 loss: 2.9345884323120117\n",
      "Epoch 14 loss: 2.9207844734191895\n",
      "Epoch 15 loss: 2.906874418258667\n",
      "Epoch 16 loss: 2.8929929733276367\n",
      "Epoch 17 loss: 2.8792922496795654\n",
      "Epoch 18 loss: 2.8659517765045166\n",
      "Epoch 19 loss: 2.8531391620635986\n",
      "Epoch 20 loss: 2.841012954711914\n",
      "Epoch 21 loss: 2.8296446800231934\n",
      "Epoch 22 loss: 2.8190829753875732\n",
      "Epoch 23 loss: 2.8093295097351074\n",
      "Epoch 24 loss: 2.8003342151641846\n",
      "Epoch 25 loss: 2.7920169830322266\n",
      "Epoch 26 loss: 2.7842965126037598\n",
      "Epoch 27 loss: 2.777127981185913\n",
      "Epoch 28 loss: 2.7704594135284424\n",
      "Epoch 29 loss: 2.7642486095428467\n",
      "Epoch 30 loss: 2.7584609985351562\n",
      "Epoch 31 loss: 2.753079891204834\n",
      "Epoch 32 loss: 2.7480804920196533\n",
      "Epoch 33 loss: 2.7434463500976562\n",
      "Epoch 34 loss: 2.7391481399536133\n",
      "Epoch 35 loss: 2.7351701259613037\n",
      "Epoch 36 loss: 2.7314929962158203\n",
      "Epoch 37 loss: 2.7280941009521484\n",
      "Epoch 38 loss: 2.7249481678009033\n",
      "Epoch 39 loss: 2.7220335006713867\n",
      "Epoch 40 loss: 2.7193329334259033\n",
      "Epoch 41 loss: 2.7168214321136475\n",
      "Epoch 42 loss: 2.714482069015503\n",
      "Epoch 43 loss: 2.7123007774353027\n",
      "Epoch 44 loss: 2.7102630138397217\n",
      "Epoch 45 loss: 2.708348035812378\n",
      "Epoch 46 loss: 2.706550121307373\n",
      "Epoch 47 loss: 2.7048532962799072\n",
      "Epoch 48 loss: 2.703242301940918\n",
      "Epoch 49 loss: 2.701712131500244\n",
      "Epoch 50 loss: 2.7002546787261963\n",
      "Epoch 51 loss: 2.6988606452941895\n",
      "Epoch 52 loss: 2.6975257396698\n",
      "Epoch 53 loss: 2.6962406635284424\n",
      "Epoch 54 loss: 2.6949996948242188\n",
      "Epoch 55 loss: 2.6937971115112305\n",
      "Epoch 56 loss: 2.692634344100952\n",
      "Epoch 57 loss: 2.691498041152954\n",
      "Epoch 58 loss: 2.6903905868530273\n",
      "Epoch 59 loss: 2.68930721282959\n",
      "Epoch 60 loss: 2.6882483959198\n",
      "Epoch 61 loss: 2.6872076988220215\n",
      "Epoch 62 loss: 2.686185836791992\n",
      "Epoch 63 loss: 2.6851747035980225\n",
      "Epoch 64 loss: 2.6841795444488525\n",
      "Epoch 65 loss: 2.683194160461426\n",
      "Epoch 66 loss: 2.6822187900543213\n",
      "Epoch 67 loss: 2.681251287460327\n",
      "Epoch 68 loss: 2.6802897453308105\n",
      "Epoch 69 loss: 2.6793360710144043\n",
      "Epoch 70 loss: 2.6783840656280518\n",
      "Epoch 71 loss: 2.6774380207061768\n",
      "Epoch 72 loss: 2.6764912605285645\n",
      "Epoch 73 loss: 2.6755480766296387\n",
      "Epoch 74 loss: 2.674604654312134\n",
      "Epoch 75 loss: 2.6736631393432617\n",
      "Epoch 76 loss: 2.672719955444336\n",
      "Epoch 77 loss: 2.6717751026153564\n",
      "Epoch 78 loss: 2.670827627182007\n",
      "Epoch 79 loss: 2.6698782444000244\n",
      "Epoch 80 loss: 2.6689293384552\n",
      "Epoch 81 loss: 2.6679766178131104\n",
      "Epoch 82 loss: 2.6670162677764893\n",
      "Epoch 83 loss: 2.666055679321289\n",
      "Epoch 84 loss: 2.6650898456573486\n",
      "Epoch 85 loss: 2.6641180515289307\n",
      "Epoch 86 loss: 2.663142204284668\n",
      "Epoch 87 loss: 2.6621615886688232\n",
      "Epoch 88 loss: 2.6611735820770264\n",
      "Epoch 89 loss: 2.660181999206543\n",
      "Epoch 90 loss: 2.659181833267212\n",
      "Epoch 91 loss: 2.658174991607666\n",
      "Epoch 92 loss: 2.657162666320801\n",
      "Epoch 93 loss: 2.656144380569458\n",
      "Epoch 94 loss: 2.655117988586426\n",
      "Epoch 95 loss: 2.654085636138916\n",
      "Epoch 96 loss: 2.653043031692505\n",
      "Epoch 97 loss: 2.651993751525879\n",
      "Epoch 98 loss: 2.650937557220459\n",
      "Epoch 99 loss: 2.649871349334717\n",
      "Epoch 100 loss: 2.6487998962402344\n",
      "Epoch 101 loss: 2.6477174758911133\n",
      "Epoch 102 loss: 2.6466245651245117\n",
      "Epoch 103 loss: 2.6455273628234863\n",
      "Epoch 104 loss: 2.644418716430664\n",
      "Epoch 105 loss: 2.643301248550415\n",
      "Epoch 106 loss: 2.6421754360198975\n",
      "Epoch 107 loss: 2.6410398483276367\n",
      "Epoch 108 loss: 2.6398935317993164\n",
      "Epoch 109 loss: 2.6387369632720947\n",
      "Epoch 110 loss: 2.6375715732574463\n",
      "Epoch 111 loss: 2.6363959312438965\n",
      "Epoch 112 loss: 2.63520884513855\n",
      "Epoch 113 loss: 2.6340160369873047\n",
      "Epoch 114 loss: 2.6328110694885254\n",
      "Epoch 115 loss: 2.631592273712158\n",
      "Epoch 116 loss: 2.6303632259368896\n",
      "Epoch 117 loss: 2.6291258335113525\n",
      "Epoch 118 loss: 2.627876043319702\n",
      "Epoch 119 loss: 2.6266164779663086\n",
      "Epoch 120 loss: 2.6253445148468018\n",
      "Epoch 121 loss: 2.6240592002868652\n",
      "Epoch 122 loss: 2.622765064239502\n",
      "Epoch 123 loss: 2.6214585304260254\n",
      "Epoch 124 loss: 2.620140314102173\n",
      "Epoch 125 loss: 2.618809461593628\n",
      "Epoch 126 loss: 2.6174659729003906\n",
      "Epoch 127 loss: 2.6161091327667236\n",
      "Epoch 128 loss: 2.6147429943084717\n",
      "Epoch 129 loss: 2.6133596897125244\n",
      "Epoch 130 loss: 2.611968517303467\n",
      "Epoch 131 loss: 2.610557794570923\n",
      "Epoch 132 loss: 2.6091415882110596\n",
      "Epoch 133 loss: 2.6077091693878174\n",
      "Epoch 134 loss: 2.606261730194092\n",
      "Epoch 135 loss: 2.6048007011413574\n",
      "Epoch 136 loss: 2.603325843811035\n",
      "Epoch 137 loss: 2.6018383502960205\n",
      "Epoch 138 loss: 2.600336790084839\n",
      "Epoch 139 loss: 2.5988171100616455\n",
      "Epoch 140 loss: 2.5972867012023926\n",
      "Epoch 141 loss: 2.5957367420196533\n",
      "Epoch 142 loss: 2.5941789150238037\n",
      "Epoch 143 loss: 2.592602014541626\n",
      "Epoch 144 loss: 2.5910115242004395\n",
      "Epoch 145 loss: 2.5894064903259277\n",
      "Epoch 146 loss: 2.587782144546509\n",
      "Epoch 147 loss: 2.5861451625823975\n",
      "Epoch 148 loss: 2.584491491317749\n",
      "Epoch 149 loss: 2.5828206539154053\n",
      "Epoch 150 loss: 2.5811338424682617\n",
      "Epoch 151 loss: 2.5794291496276855\n",
      "Epoch 152 loss: 2.5777108669281006\n",
      "Epoch 153 loss: 2.5759685039520264\n",
      "Epoch 154 loss: 2.5742173194885254\n",
      "Epoch 155 loss: 2.572442054748535\n",
      "Epoch 156 loss: 2.5706565380096436\n",
      "Epoch 157 loss: 2.5688517093658447\n",
      "Epoch 158 loss: 2.5670268535614014\n",
      "Epoch 159 loss: 2.5651838779449463\n",
      "Epoch 160 loss: 2.563323974609375\n",
      "Epoch 161 loss: 2.5614373683929443\n",
      "Epoch 162 loss: 2.55954647064209\n",
      "Epoch 163 loss: 2.557631731033325\n",
      "Epoch 164 loss: 2.555697441101074\n",
      "Epoch 165 loss: 2.553741693496704\n",
      "Epoch 166 loss: 2.551769256591797\n",
      "Epoch 167 loss: 2.5497772693634033\n",
      "Epoch 168 loss: 2.547762393951416\n",
      "Epoch 169 loss: 2.54573392868042\n",
      "Epoch 170 loss: 2.5436830520629883\n",
      "Epoch 171 loss: 2.541612148284912\n",
      "Epoch 172 loss: 2.5395219326019287\n",
      "Epoch 173 loss: 2.537409782409668\n",
      "Epoch 174 loss: 2.535280704498291\n",
      "Epoch 175 loss: 2.5331292152404785\n",
      "Epoch 176 loss: 2.5309596061706543\n",
      "Epoch 177 loss: 2.5287678241729736\n",
      "Epoch 178 loss: 2.5265562534332275\n",
      "Epoch 179 loss: 2.524325370788574\n",
      "Epoch 180 loss: 2.522073745727539\n",
      "Epoch 181 loss: 2.519801378250122\n",
      "Epoch 182 loss: 2.5175089836120605\n",
      "Epoch 183 loss: 2.5151963233947754\n",
      "Epoch 184 loss: 2.512864589691162\n",
      "Epoch 185 loss: 2.5105111598968506\n",
      "Epoch 186 loss: 2.508138656616211\n",
      "Epoch 187 loss: 2.5057475566864014\n",
      "Epoch 188 loss: 2.5033369064331055\n",
      "Epoch 189 loss: 2.500905990600586\n",
      "Epoch 190 loss: 2.498457431793213\n",
      "Epoch 191 loss: 2.495985507965088\n",
      "Epoch 192 loss: 2.4935009479522705\n",
      "Epoch 193 loss: 2.4909937381744385\n",
      "Epoch 194 loss: 2.488469123840332\n",
      "Epoch 195 loss: 2.485927104949951\n",
      "Epoch 196 loss: 2.4833693504333496\n",
      "Epoch 197 loss: 2.4807920455932617\n",
      "Epoch 198 loss: 2.4781973361968994\n",
      "Epoch 199 loss: 2.47558856010437\n",
      "Epoch 200 loss: 2.4729626178741455\n",
      "Epoch 201 loss: 2.470319986343384\n",
      "Epoch 202 loss: 2.467663288116455\n",
      "Epoch 203 loss: 2.4649930000305176\n",
      "Epoch 204 loss: 2.4623069763183594\n",
      "Epoch 205 loss: 2.459606409072876\n",
      "Epoch 206 loss: 2.4568958282470703\n",
      "Epoch 207 loss: 2.454169988632202\n",
      "Epoch 208 loss: 2.451430559158325\n",
      "Epoch 209 loss: 2.4486868381500244\n",
      "Epoch 210 loss: 2.4459266662597656\n",
      "Epoch 211 loss: 2.443157434463501\n",
      "Epoch 212 loss: 2.4403791427612305\n",
      "Epoch 213 loss: 2.437593936920166\n",
      "Epoch 214 loss: 2.4347991943359375\n",
      "Epoch 215 loss: 2.431995153427124\n",
      "Epoch 216 loss: 2.4291865825653076\n",
      "Epoch 217 loss: 2.4263696670532227\n",
      "Epoch 218 loss: 2.4235503673553467\n",
      "Epoch 219 loss: 2.4207241535186768\n",
      "Epoch 220 loss: 2.4178926944732666\n",
      "Epoch 221 loss: 2.4150607585906982\n",
      "Epoch 222 loss: 2.4122254848480225\n",
      "Epoch 223 loss: 2.40938663482666\n",
      "Epoch 224 loss: 2.4065499305725098\n",
      "Epoch 225 loss: 2.403709650039673\n",
      "Epoch 226 loss: 2.400869607925415\n",
      "Epoch 227 loss: 2.398030996322632\n",
      "Epoch 228 loss: 2.3951923847198486\n",
      "Epoch 229 loss: 2.3923563957214355\n",
      "Epoch 230 loss: 2.3895225524902344\n",
      "Epoch 231 loss: 2.3866918087005615\n",
      "Epoch 232 loss: 2.3838624954223633\n",
      "Epoch 233 loss: 2.381035327911377\n",
      "Epoch 234 loss: 2.3782122135162354\n",
      "Epoch 235 loss: 2.375392198562622\n",
      "Epoch 236 loss: 2.372574806213379\n",
      "Epoch 237 loss: 2.3697633743286133\n",
      "Epoch 238 loss: 2.366952657699585\n",
      "Epoch 239 loss: 2.364147186279297\n",
      "Epoch 240 loss: 2.361344575881958\n",
      "Epoch 241 loss: 2.358546495437622\n",
      "Epoch 242 loss: 2.3557496070861816\n",
      "Epoch 243 loss: 2.352954149246216\n",
      "Epoch 244 loss: 2.350163221359253\n",
      "Epoch 245 loss: 2.3473734855651855\n",
      "Epoch 246 loss: 2.3445849418640137\n",
      "Epoch 247 loss: 2.341797113418579\n",
      "Epoch 248 loss: 2.3390138149261475\n",
      "Epoch 249 loss: 2.336228847503662\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    for xb, yb in loader:\n",
    "        logits = model(xb)\n",
    "        logits = logits.reshape(20, 22)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(logits, yb.reshape(20))\n",
    "        total_loss += loss\n",
    "        count += 1\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} loss: {total_loss / count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86ebbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "wx = np.random.rand(64, 22) *0.01   # (64, 22)\n",
    "x = np.random.rand(22,)\n",
    "(x @ wx.T).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
