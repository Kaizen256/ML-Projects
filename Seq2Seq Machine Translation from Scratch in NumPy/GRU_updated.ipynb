{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8496e7b",
   "metadata": {
    "papermill": {
     "duration": 0.005403,
     "end_time": "2025-08-05T23:54:21.584142",
     "exception": false,
     "start_time": "2025-08-05T23:54:21.578739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GRU-based Encoder–Decoder for Machine Translation (NumPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073add2",
   "metadata": {
    "papermill": {
     "duration": 0.003813,
     "end_time": "2025-08-05T23:54:21.592657",
     "exception": false,
     "start_time": "2025-08-05T23:54:21.588844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this project, I created a machine translation model using an encoder–decoder architecture built entirely from scratch in NumPy, without using any deep learning frameworks. The model translates from English to Japanese using a GRU (Gated Recurrent Unit).\n",
    "- Dataset: English Japanese sentence pairs selected from the Tatoeba Project\n",
    "- Architecture: Encoder–Decoder with GRU\n",
    "- Objective: Generate Japanese translations using only NumPy operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc77ca",
   "metadata": {
    "papermill": {
     "duration": 0.00372,
     "end_time": "2025-08-05T23:54:21.600540",
     "exception": false,
     "start_time": "2025-08-05T23:54:21.596820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Architecture Overview\n",
    "- **Embedding**: English and Japanese character tokens are first mapped to integer indices. Each index is embedded into a dense vector using separate embedding matrices.\n",
    "- **Encoder**: A GRU processes the full embedded English input sequence, the final hidden state is returned and represents the context of the entire input sentence. \n",
    "- **Decoder**: The decoder is another GRU that predicts the Japanese output one character at a time. It takes the embedded previous character and the hidden state from the previous step. The initial hidden state is the encoder's final hidden state.\n",
    "- **Output**: Immediately after running the decoder GRU on a time step, apply an FC layer to the hidden state at that time step to get output logits. Store those logits and at the end apply softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81291c0",
   "metadata": {
    "papermill": {
     "duration": 0.003684,
     "end_time": "2025-08-05T23:54:21.608158",
     "exception": false,
     "start_time": "2025-08-05T23:54:21.604474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![Encoder Decoder Architecture](figures/ED_arch.png)\n",
    "\n",
    "Image source: Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. - https://github.com/d2l-ai/d2l-en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8b7e5",
   "metadata": {
    "papermill": {
     "duration": 0.003653,
     "end_time": "2025-08-05T23:54:21.615745",
     "exception": false,
     "start_time": "2025-08-05T23:54:21.612092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Loading and Filtering Sentence Pairs\n",
    "\n",
    "Read English Japanese sentence pairs from the `jpn.txt` file. Because training a model from scratch using only NumPy means we can't use a gpu, we need to make the dataset smaller. We only use sentences with 3 or less words. lowercase the English text, and clean both languages using regular expressions. \n",
    "\n",
    "For English, we remove all non-alphanumeric characters. For Japanese, we preserve Hiragana, Katakana, Kanji, and a few key punctuation symbols (like 。 and 、). Duplicate English sentences are also removed to avoid redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090e9604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T23:54:21.625846Z",
     "iopub.status.busy": "2025-08-05T23:54:21.625393Z",
     "iopub.status.idle": "2025-08-05T23:54:22.310668Z",
     "shell.execute_reply": "2025-08-05T23:54:22.309683Z"
    },
    "papermill": {
     "duration": 0.693246,
     "end_time": "2025-08-05T23:54:22.312891",
     "exception": false,
     "start_time": "2025-08-05T23:54:21.619645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentences: ['go', 'hi', 'run', 'who', 'wow', 'duck', 'fire', 'help', 'hide', 'jump']\n",
      "English Sentences Length: 6627\n",
      "Japanese Sentences: ['行け。', 'こんにちは。', '走れ。', '誰？', 'すごい！', '頭を下げろ！', '火事だ！', '助けて！', '隠れろ。', '飛び越えろ！']\n",
      "Japanese Sentences Length: 6627\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Lists for cleaned sentences\n",
    "eng_sentences = []\n",
    "jpn_sentences = []\n",
    "\n",
    "# To avoid duplicates\n",
    "seen = set()\n",
    "\n",
    "with open('jpn.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "\n",
    "        # Extract English and Japanese parts\n",
    "        eng = parts[0].strip().lower()\n",
    "        jpn = parts[1].strip().lower()\n",
    "\n",
    "        # Clean English: keep lowercase letters, numbers, and spaces\n",
    "        eng = re.sub(r\"[^a-z0-9\\s]\", \"\", eng)\n",
    "\n",
    "        # Clean Japanese: keep the punctuation\n",
    "        jpn = re.sub(r\"[^\\u3040-\\u30ff\\u4e00-\\u9fff。、！？\\s]\", \"\", jpn)\n",
    "        # Filtering out sentences, max 3 words\n",
    "        if len(eng.split()) <= 3 and len(jpn.split()) <= 3 and eng not in seen:\n",
    "            eng_sentences.append(eng)\n",
    "            jpn_sentences.append(jpn)\n",
    "            seen.add(eng)\n",
    "\n",
    "print(f\"English Sentences: {eng_sentences[0:10]}\")\n",
    "print(f\"English Sentences Length: {len(eng_sentences)}\")\n",
    "print(f\"Japanese Sentences: {jpn_sentences[0:10]}\")\n",
    "print(f\"Japanese Sentences Length: {len(jpn_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392c374",
   "metadata": {
    "papermill": {
     "duration": 0.00407,
     "end_time": "2025-08-05T23:54:22.321587",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.317517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vocabulary Construction\n",
    "\n",
    "We now build vocabularies for both English and Japanese.\n",
    "\n",
    "- English is tokenized at word level. We assign indices to all words that appear more than twice.\n",
    "- Japanese is tokenized at the character level, since words are not separated by spaces. Characters appearing more than twice are included in the vocabulary.\n",
    "\n",
    "We also add special tokens:\n",
    "\n",
    "| Token | Meaning         |\n",
    "|-------|------------------|\n",
    "| `<pad>` | Padding token     |\n",
    "| `<unk>` | Unknown token     |\n",
    "| `<bos>` | Beginning of sequence |\n",
    "| `<eos>` | End of sequence     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c4a9f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T23:54:22.331427Z",
     "iopub.status.busy": "2025-08-05T23:54:22.331089Z",
     "iopub.status.idle": "2025-08-05T23:54:22.363744Z",
     "shell.execute_reply": "2025-08-05T23:54:22.362265Z"
    },
    "papermill": {
     "duration": 0.039699,
     "end_time": "2025-08-05T23:54:22.365522",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.325823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 971\n",
      "Japanese Vocabulary Size: 847\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "# Count word frequency\n",
    "for sent in eng_sentences:\n",
    "    counter.update(sent.split())\n",
    "\n",
    "# English to index mapping\n",
    "eng_to_ind = {'<pad>': 0, '<unk>':1}\n",
    "ind = 2\n",
    "\n",
    "# Include words that appear 3+ times.\n",
    "for word in counter:\n",
    "    if counter[word] > 2:\n",
    "        eng_to_ind[word] = ind\n",
    "        ind += 1\n",
    "        \n",
    "print(\"English Vocabulary Size:\", len(eng_to_ind))\n",
    "\n",
    "jcounter = Counter()\n",
    "\n",
    "# Count character frequency across Japanese sentences\n",
    "for sent in jpn_sentences:\n",
    "    jcounter.update(list(sent))\n",
    "\n",
    "# Japanese to index and index to Japanese mapping\n",
    "jpn_to_ind = {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3}\n",
    "ind_to_jpn = {0: '<pad>', 1: '<unk>', 2: '<bos>', 3: '<eos>'}\n",
    "ind = 4\n",
    "\n",
    "# Include characters that appear 3+ times\n",
    "for char, freq in jcounter.most_common():\n",
    "    if freq > 2:\n",
    "        jpn_to_ind[char] = ind\n",
    "        ind_to_jpn[ind] = char\n",
    "        ind += 1\n",
    "\n",
    "print(\"Japanese Vocabulary Size:\", len(jpn_to_ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d643b",
   "metadata": {
    "papermill": {
     "duration": 0.004523,
     "end_time": "2025-08-05T23:54:22.375011",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.370488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Encoding Sentences\n",
    "\n",
    "Now convert each sentence into a sequence of integer indices.\n",
    "\n",
    "- English Encoding: \n",
    "  - Tokenized at the word level.\n",
    "  - Each sentence is padded to 3 tokens using the `<pad>` (index 0).\n",
    "  - Unknown words are mapped to `<unk>` (index 1).\n",
    "\n",
    "- Japanese Encoding:\n",
    "  - Tokenized at the character level.\n",
    "  - Each sentence starts with `<bos>` (index 2) and ends with `<eos>` (index 3).\n",
    "  - Limit to 12 tokens total to include both `<bos>` and `<eos>`.\n",
    "  - Padding (`<pad>`) is added at the end if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421f85df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T23:54:22.384882Z",
     "iopub.status.busy": "2025-08-05T23:54:22.384583Z",
     "iopub.status.idle": "2025-08-05T23:54:22.433681Z",
     "shell.execute_reply": "2025-08-05T23:54:22.432616Z"
    },
    "papermill": {
     "duration": 0.056384,
     "end_time": "2025-08-05T23:54:22.435728",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.379344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentences Encoded: [[2, 0, 0], [3, 0, 0], [4, 0, 0], [5, 0, 0], [6, 0, 0], [1, 0, 0], [7, 0, 0], [8, 0, 0], [9, 0, 0], [10, 0, 0]]\n",
      "English Sentences Encoded Length: 6627\n",
      "Japanese Sentences Encoded: [[2, 71, 47, 4, 3, 0, 0, 0, 0, 0, 0, 0], [2, 29, 23, 27, 42, 6, 4, 3, 0, 0, 0, 0], [2, 213, 28, 4, 3, 0, 0, 0, 0, 0, 0, 0], [2, 65, 26, 3, 0, 0, 0, 0, 0, 0, 0, 0], [2, 12, 79, 5, 53, 3, 0, 0, 0, 0, 0, 0]]\n",
      "Japanese Sentences Encoded Length: 6627\n"
     ]
    }
   ],
   "source": [
    "eng_encoded = []\n",
    "\n",
    "# Encode each English sentence at word level\n",
    "# Maximum of 3 words per sentence\n",
    "for sentence in eng_sentences:\n",
    "    s = []\n",
    "    for word in sentence.split():\n",
    "        if word in eng_to_ind:\n",
    "            s.append(eng_to_ind[word])\n",
    "        else:\n",
    "            s.append(1)\n",
    "    while(len(s) < 3):\n",
    "        s.append(0) # Padding\n",
    "    eng_encoded.append(s)\n",
    "\n",
    "jpn_encoded = []\n",
    "\n",
    "# Encode each Japanese sentence character level\n",
    "# Maximum of twelve characters per sentence including <bos> and <eos>\n",
    "for sentence in jpn_sentences:\n",
    "    s = [2] # 2 is <bos>\n",
    "    for ch in sentence:\n",
    "        if len(s) > 10:\n",
    "            break # We need room to fit <eos> token.\n",
    "        if ch in jpn_to_ind:\n",
    "            s.append(jpn_to_ind[ch])\n",
    "        else:\n",
    "            s.append(1)   # 1 is <unk>\n",
    "    s.append(3)           # 3 is <eos>\n",
    "    while(len(s) < 12):\n",
    "        s.append(0)       # 0 is <pad>\n",
    "    jpn_encoded.append(s)\n",
    "\n",
    "print(f\"English Sentences Encoded: {eng_encoded[0:10]}\")\n",
    "print(f\"English Sentences Encoded Length: {len(eng_encoded)}\")\n",
    "print(f\"Japanese Sentences Encoded: {jpn_encoded[0:5]}\")\n",
    "print(f\"Japanese Sentences Encoded Length: {len(jpn_encoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558ad63",
   "metadata": {
    "papermill": {
     "duration": 0.004125,
     "end_time": "2025-08-05T23:54:22.445368",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.441243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating Embedding Matrices\n",
    "\n",
    "- Vocabulary Size: Approximately 900 for each language.\n",
    "- Embedding Dimension: I estimated using int(min(600, 1.6 * vocab_size ** 0.56)) = 72  \n",
    "  I wanted to use a power of 2, so I rounded down to 64.\n",
    "\n",
    "Each word or character will be embedded into a 64 dimensional dense vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5aac688",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T23:54:22.455500Z",
     "iopub.status.busy": "2025-08-05T23:54:22.455141Z",
     "iopub.status.idle": "2025-08-05T23:54:22.470763Z",
     "shell.execute_reply": "2025-08-05T23:54:22.469400Z"
    },
    "papermill": {
     "duration": 0.023102,
     "end_time": "2025-08-05T23:54:22.472731",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.449629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English embedding shape: (971, 64)\n",
      "Japanese embedding shape: (847, 64)\n",
      "English input shape: (3, 64)\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 64\n",
    "V_eng = len(eng_to_ind)  # English vocab size\n",
    "V_jpn = len(jpn_to_ind)  # Japanese vocab size\n",
    "\n",
    "eng_embedding = np.random.randn(V_eng, embedding_size) * 0.1\n",
    "jpn_embedding = np.random.randn(V_jpn, embedding_size) * 0.1\n",
    "\n",
    "print(f\"English embedding shape: {eng_embedding.shape}\")\n",
    "print(f\"Japanese embedding shape: {jpn_embedding.shape}\")\n",
    "\n",
    "# Example\n",
    "eng_sentence = [5, 23, 8]\n",
    "embedded_sentence = eng_embedding[eng_sentence]  # Shape (3, 64)\n",
    "print(f\"English input shape: {embedded_sentence.shape}\")  # (seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f600fa7",
   "metadata": {
    "papermill": {
     "duration": 0.004438,
     "end_time": "2025-08-05T23:54:22.482301",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.477863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Activation and Loss Functions\n",
    "\n",
    "- Sigmoid: Used in GRU gates.\n",
    "- Tanh: Used for the candidate hidden state.\n",
    "- Softmax: Converts decoder logits into probabilities.\n",
    "- Cross Entropy Loss: Measures prediction error between predicted softmax outputs and true labels.\n",
    "\n",
    "Documentation generated by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "933be495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T23:54:22.494326Z",
     "iopub.status.busy": "2025-08-05T23:54:22.493939Z",
     "iopub.status.idle": "2025-08-05T23:54:22.504058Z",
     "shell.execute_reply": "2025-08-05T23:54:22.502780Z"
    },
    "papermill": {
     "duration": 0.018035,
     "end_time": "2025-08-05T23:54:22.505913",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.487878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Element-wise Sigmoid activation function.\n",
    "    Used in GRU for update/reset gates.\n",
    "    \n",
    "    Clips input to avoid overflow in exp.\n",
    "    \"\"\"\n",
    "    x = np.clip(x, -50, 50)  # Prevent overflow\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    \"\"\"\n",
    "    Derivative of the Sigmoid function.\n",
    "    Used during GRU backpropagation.\n",
    "    \"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Element-wise hyperbolic tangent activation.\n",
    "    Used in GRU for candidate hidden state.\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    \"\"\"\n",
    "    Derivative of tanh function.\n",
    "    Used in GRU backpropagation.\n",
    "    \"\"\"\n",
    "    return 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "def softmax(Z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply softmax over last dimension (class logits).\n",
    "    \n",
    "    Parameters:\n",
    "    - Z: np.ndarray of shape (batch_size, num_classes)\n",
    "      Raw logits for classification.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray of same shape: softmax probabilities.\n",
    "    \"\"\"\n",
    "    Z = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    exp_Z = np.exp(Z)\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "def CrossEntropy(yhat: np.ndarray, y: np.ndarray, eps: float = 1e-15) -> float:\n",
    "    \"\"\"\n",
    "    Compute mean cross-entropy loss.\n",
    "\n",
    "    Parameters:\n",
    "    - yhat: np.ndarray of shape (batch_size, num_classes)\n",
    "        Predicted probabilities (after softmax)\n",
    "    - y: np.ndarray of shape (batch_size,)\n",
    "        True class indices\n",
    "    - eps: float\n",
    "        Small constant to avoid log(0)\n",
    "\n",
    "    Returns:\n",
    "    - float: mean loss across the batch\n",
    "    \"\"\"\n",
    "    yhat = np.clip(yhat, eps, 1 - eps)  # Prevent log(0)\n",
    "    correct_probs = yhat[np.arange(len(y)), y]\n",
    "    return -np.mean(np.log(correct_probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd83b39a",
   "metadata": {
    "papermill": {
     "duration": 0.004132,
     "end_time": "2025-08-05T23:54:22.514580",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.510448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GRU Architecture and Implementation\n",
    "\n",
    "The Gated Recurrent Unit (GRU) is an RNN cell designed to assist with the vanishing/exploding gradient problem and capture long range dependencies more efficiently than a vanilla RNN. Below is a overview of the implementation.\n",
    "\n",
    "- Reset Gate (R): Determines how much of the past hidden state to forget.\n",
    "- Update Gate (Z): Determines how much of the new candidate state to use versus retaining the past hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86faa9",
   "metadata": {
    "papermill": {
     "duration": 0.004108,
     "end_time": "2025-08-05T23:54:22.523245",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.519137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GRU Architecture and Implementation\n",
    "\n",
    "The Gated Recurrent Unit (GRU) is an RNN cell designed to assist with the vanishing/exploding gradient problem and capture long range dependencies more efficiently than a vanilla RNN. Below is a overview of the implementation.\n",
    "\n",
    "- Reset Gate (R): Determines how much of the past hidden state to forget.\n",
    "- Update Gate (Z): Determines how much of the new candidate state to use versus retaining the past hidden state.\n",
    "Rₜ = σ(xₜ @ W_r.T + hₜ₋₁ @ U_r + b_r)\n",
    "### Forward Pass (per time step \\(t\\))\n",
    "1. Compute gates\n",
    "    - Rₜ = σ(xₜ @ W_r.T + hₜ₋₁ @ U_r + b_r)\n",
    "    - Zₜ = σ(xₜ @ W_z.T + hₜ₋₁ @ U_z + b_z)\n",
    "2. Candidate state \n",
    "    - Cₜ = tanh(xₜ @ W_c.T + (Rₜ ⊙ hₜ₋₁) @ U_c + b_c)\n",
    "3. Hidden state update\n",
    "    - xₜ = zₜ ⊙ hₜ₋₁ + (1 - Zₜ) ⊙ Cₜ\n",
    "\n",
    "Store (xₜ, hₜ₋₁, Rₜ, Zₜ, Cₜ, hₜ) at each step to use during backpropagation.\n",
    "\n",
    "![GRU Architecture](figures/GRU_arch.png)\n",
    "\n",
    "### Gradient Calculations\n",
    "\n",
    "Calculating the gradients is a tedious process, below are my calculations to get this model to work. There were a lot of problems.\n",
    "\n",
    "![Backpropagation](figures/Backprop.png)\n",
    "\n",
    "### Integration into Encoder–Decoder\n",
    "- Encoder GRU: Processes the input sequence and returns the final hidden state \\(h_{enc}\\).  \n",
    "- Decoder GRU: Initialized with \\(h_{enc}\\), generates the target sequence by feeding its own previous output back as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "083682d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T23:54:22.533798Z",
     "iopub.status.busy": "2025-08-05T23:54:22.533481Z",
     "iopub.status.idle": "2025-08-05T23:54:22.554434Z",
     "shell.execute_reply": "2025-08-05T23:54:22.553425Z"
    },
    "papermill": {
     "duration": 0.028608,
     "end_time": "2025-08-05T23:54:22.556225",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.527617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Xavier Normal std\n",
    "        std = np.sqrt(2 / (input_size + hidden_size))\n",
    "\n",
    "        # Reset gate (R)\n",
    "        self.r_w = np.random.normal(0, std, size=(hidden_size, input_size))\n",
    "        self.r_u = np.random.normal(0, std, size=(hidden_size, hidden_size))\n",
    "        self.r_b = np.zeros(hidden_size)\n",
    "\n",
    "        # Update gate (Z)\n",
    "        self.z_w = np.random.normal(0, std, size=(hidden_size, input_size))\n",
    "        self.z_u = np.random.normal(0, std, size=(hidden_size, hidden_size))\n",
    "        self.z_b = np.zeros(hidden_size)\n",
    "\n",
    "        # Candidate hidden state (C)\n",
    "        self.c_w = np.random.normal(0, std, size=(hidden_size, input_size))\n",
    "        self.c_u = np.random.normal(0, std, size=(hidden_size, hidden_size))\n",
    "        self.c_b = np.zeros(hidden_size)\n",
    "    \n",
    "    def forward(self, X, H=None):\n",
    "        batch_size, seq_len, embed_dim = X.shape # (N, T, D)\n",
    "        if H is None:\n",
    "            H = np.zeros(shape=(batch_size, self.hidden_size)) # (N, H)\n",
    "\n",
    "        hidden_states = []\n",
    "        store = [] # For backpropagation\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            x_t = X[:, i, :]  # (N, D)\n",
    "            R = sigmoid(x_t @ self.r_w.T + H @ self.r_u + self.r_b)  # (N, H)\n",
    "            Z = sigmoid(x_t @ self.z_w.T + H @ self.z_u + self.z_b)  # (N, H)\n",
    "            C = tanh(x_t @ self.c_w.T + (R * H) @ self.c_u + self.c_b)  # (N, H)\n",
    "\n",
    "            H_new = Z * H + (1 - Z) * C  # (N, H)\n",
    "\n",
    "            store.append((x_t, H, R, Z, C, H_new))\n",
    "            H = H_new\n",
    "            hidden_states.append(H[:, np.newaxis, :])\n",
    "            \n",
    "\n",
    "        return H, np.concatenate(hidden_states, axis=1), store\n",
    "\n",
    "    def backward(self, dH_all, store, lr):\n",
    "\n",
    "        batch, seq_len, _ = dH_all.shape\n",
    "\n",
    "        # Initialize accumulators for parameter gradients to zeros\n",
    "        dr_w = np.zeros_like(self.r_w)  # ∂L/∂R_w, shape (H, D)\n",
    "        dr_u = np.zeros_like(self.r_u)  # ∂L/∂R_u, shape (H, H)\n",
    "        dr_b = np.zeros_like(self.r_b)  # ∂L/∂R_b,  shape (H,)\n",
    "\n",
    "        dz_w = np.zeros_like(self.z_w)  # ∂L/∂Z_w\n",
    "        dz_u = np.zeros_like(self.z_u)  # ∂L/∂Z_u\n",
    "        dz_b = np.zeros_like(self.z_b)  # ∂L/∂Z_b\n",
    "\n",
    "        dc_w = np.zeros_like(self.c_w)  # ∂L/∂C_w\n",
    "        dc_u = np.zeros_like(self.c_u)  # ∂L/∂C_u\n",
    "        dc_b = np.zeros_like(self.c_b)  # ∂L/∂C_b\n",
    "\n",
    "        # dh accumulates the gradient flowing from future time steps\n",
    "        dH = np.zeros((batch, self.hidden_size)) # (N, H)\n",
    "        \n",
    "        # Will hold gradient w.r.t. the input embeddings at each time step\n",
    "        dx = np.zeros((batch, seq_len, self.input_size))  # (N, T, D)\n",
    "\n",
    "        # Loop backwards through time: t = seq_len-1 ... 0\n",
    "        for t in reversed(range(seq_len)):\n",
    "            # Unpack stored values from forward pass\n",
    "            # x_t:   (N, D) input at time t\n",
    "            # h_prev:(N, H) previous hidden state\n",
    "            # r_t:   (N, H) reset gate\n",
    "            # z_t:   (N, H) update gate\n",
    "            # c_t:   (N, H) candidate hidden state\n",
    "            # h_t:   (N, H) final hidden state at time t\n",
    "            x_t, h_prev, r_t, z_t, c_t, h_t = store[t]\n",
    "\n",
    "            # dh: (N, H) total gradient wrt current hidden state h_t\n",
    "            # Includes gradient from output loss and from next time step dH\n",
    "            dh = dH_all[:, t, :] + dH\n",
    "            \n",
    "            # Gradients through update gate z_t and candidate c_t\n",
    "            # ∂L/∂z_t = dh ⊙ (h_prev - c_t)\n",
    "            dz = dh * (h_prev - c_t) # (N, H)\n",
    "            # ∂L/∂c_t = dh ⊙ (1 - z_t)\n",
    "            dc = dh * (1 - z_t)      # (N, H)\n",
    "            # ∂L/∂h_prev = dh ⊙ z_t, one part of ∂L/∂h_{t-1}\n",
    "            dh_prev = dh * z_t       # (N, H)\n",
    "\n",
    "            # Backprop through tanh: ∂L/∂c_t_preact = dc ⊙ (1 - c_t²)\n",
    "            dc_preact = dc * (1 - c_t**2)   # (N, H)\n",
    "            \n",
    "            # Accumulate parameter gradients for candidate weights\n",
    "            # ∂L/∂C_w = dc_preact.T @ x_t\n",
    "            dc_w += dc_preact.T @ x_t              # (H, D)\n",
    "            # ∂L/∂C_u = (r_t ⊙ h_prev).T @ dc_preact\n",
    "            dc_u += (r_t * h_prev).T @ dc_preact   # (H, H)\n",
    "            # ∂L/∂C_b = sum over batch of dc_preact\n",
    "            dc_b += np.sum(dc_preact, axis=0)      # (H,)\n",
    "\n",
    "            # Reset gate gradient from C preact's hidden side\n",
    "            # ∂L/∂r_t = ∂L/(∂C preact) ⊙ ∂(C preact)/∂r_t\n",
    "            # ∂L/∂r_t = (dc_preact @ C_u.T) ⊙ h_prev\n",
    "            dr = (dc_preact @ self.c_u.T) * h_prev   # (N, H)\n",
    "            # Backprop through sigmoid: ∂L/∂r_preact = dr ⊙ r_t ⊙ (1 - r_t)\n",
    "            dr_preact = dr * (r_t * (1 - r_t))       # (N, H)\n",
    "            # Accumulate Reset gate gradients\n",
    "            dr_w += dr_preact.T @ x_t                # (H, D)\n",
    "            dr_u += h_prev.T @ dr_preact             # (H, H)\n",
    "            dr_b += np.sum(dr_preact, axis=0)        # (H,)\n",
    "\n",
    "            # Backprop through update gate sigmoid\n",
    "            dz_preact = dz * z_t * (1 - z_t)         # (N, H)\n",
    "\n",
    "            # Accumulate update gate gradients\n",
    "            dz_w += dz_preact.T @ x_t                # (H, D)\n",
    "            dz_u += h_prev.T @ dz_preact             # (H, H)\n",
    "            dz_b += np.sum(dz_preact, axis=0)        # (H,)\n",
    "\n",
    "            # Add to total gradient wrt h_prev\n",
    "            # dh_prev = sum of all partials wrt h_prev from C, R, Z\n",
    "            dh_prev += (dc_preact @ self.c_u.T) * r_t        # (N, H)\n",
    "            dh_prev += dr_preact @ self.r_u.T                # (N, H)\n",
    "            dh_prev += dz_preact @ self.z_u.T                # (N, H)\n",
    "            \n",
    "\n",
    "            # Set dH for next iteration (t-1)\n",
    "            dH = dh_prev  # (N, H)\n",
    "\n",
    "            # Gradient wrt. input x_t combines contributions from each gate\n",
    "            # Paths: dr_preact, dz_preact, dc_preact\n",
    "            dx[:, t, :] = (\n",
    "                dr_preact @ self.r_w +  # via reset gate input weights\n",
    "                dz_preact @ self.z_w +  # via update gate input weights\n",
    "                dc_preact @ self.c_w    # via candidate input weights\n",
    "            )  # shape (N, D)\n",
    "\n",
    "\n",
    "        grads = [dc_w, dc_u, dc_b, dr_w, dr_u, dr_b, dz_w, dz_u, dz_b]\n",
    "\n",
    "        for i in range(len(grads)):\n",
    "            np.clip(grads[i], -1.0, 1.0, out=grads[i])  # Gradient Clipping\n",
    "\n",
    "        self.c_w -= lr * dc_w\n",
    "        self.c_u -= lr * dc_u\n",
    "        self.c_b -= lr * dc_b\n",
    "\n",
    "        self.r_w -= lr * dr_w\n",
    "        self.r_u -= lr * dr_u\n",
    "        self.r_b -= lr * dr_b\n",
    "\n",
    "        self.z_w -= lr * dz_w\n",
    "        self.z_u -= lr * dz_u\n",
    "        self.z_b -= lr * dz_b\n",
    "\n",
    "        return dH, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d46a3f",
   "metadata": {
    "papermill": {
     "duration": 0.004161,
     "end_time": "2025-08-05T23:54:22.564951",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.560790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Saving parameters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc6001ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T23:54:22.575286Z",
     "iopub.status.busy": "2025-08-05T23:54:22.574905Z",
     "iopub.status.idle": "2025-08-05T23:54:22.581835Z",
     "shell.execute_reply": "2025-08-05T23:54:22.580778Z"
    },
    "papermill": {
     "duration": 0.013976,
     "end_time": "2025-08-05T23:54:22.583513",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.569537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_parameters(path, Encoder_GRU, Decoder_GRU, eng_embedding, jpn_embedding, o_w, o_b):\n",
    "\n",
    "    np.savez(\n",
    "        path,\n",
    "        eng_embedding=eng_embedding,\n",
    "        jpn_embedding=jpn_embedding,\n",
    "        o_w=o_w, o_b=o_b,\n",
    "        enc_r_w=Encoder_GRU.r_w, enc_r_u=Encoder_GRU.r_u, enc_r_b=Encoder_GRU.r_b,\n",
    "        enc_z_w=Encoder_GRU.z_w, enc_z_u=Encoder_GRU.z_u, enc_z_b=Encoder_GRU.z_b,\n",
    "        enc_c_w=Encoder_GRU.c_w, enc_c_u=Encoder_GRU.c_u, enc_c_b=Encoder_GRU.c_b,\n",
    "        dec_r_w=Decoder_GRU.r_w, dec_r_u=Decoder_GRU.r_u, dec_r_b=Decoder_GRU.r_b,\n",
    "        dec_z_w=Decoder_GRU.z_w, dec_z_u=Decoder_GRU.z_u, dec_z_b=Decoder_GRU.z_b,\n",
    "        dec_c_w=Decoder_GRU.c_w, dec_c_u=Decoder_GRU.c_u, dec_c_b=Decoder_GRU.c_b\n",
    "    )\n",
    "    print(\"Saved Parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbfb1a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T23:54:22.596782Z",
     "iopub.status.busy": "2025-08-05T23:54:22.596162Z",
     "iopub.status.idle": "2025-08-06T02:04:44.216645Z",
     "shell.execute_reply": "2025-08-06T02:04:44.215443Z"
    },
    "papermill": {
     "duration": 7821.630278,
     "end_time": "2025-08-06T02:04:44.218535",
     "exception": false,
     "start_time": "2025-08-05T23:54:22.588257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 6.7080\n",
      "Epoch 2 | Loss: 6.6384\n",
      "Epoch 3 | Loss: 6.5638\n",
      "Epoch 4 | Loss: 6.4726\n",
      "Epoch 5 | Loss: 6.3211\n",
      "Epoch 6 | Loss: 5.8865\n",
      "Epoch 7 | Loss: 5.3120\n",
      "Epoch 8 | Loss: 4.9587\n",
      "Epoch 9 | Loss: 4.7263\n",
      "Epoch 10 | Loss: 4.5756\n",
      "Epoch 11 | Loss: 4.4725\n",
      "Epoch 12 | Loss: 4.3903\n",
      "Epoch 13 | Loss: 4.3177\n",
      "Epoch 14 | Loss: 4.2517\n",
      "Epoch 15 | Loss: 4.1934\n",
      "Epoch 16 | Loss: 4.1426\n",
      "Epoch 17 | Loss: 4.0993\n",
      "Epoch 18 | Loss: 4.0629\n",
      "Epoch 19 | Loss: 4.0315\n",
      "Epoch 20 | Loss: 4.0041\n",
      "Epoch 21 | Loss: 3.9799\n",
      "Epoch 22 | Loss: 3.9580\n",
      "Epoch 23 | Loss: 3.9396\n",
      "Epoch 24 | Loss: 3.9228\n",
      "Epoch 25 | Loss: 3.9081\n",
      "Epoch 26 | Loss: 3.8936\n",
      "Epoch 27 | Loss: 3.8810\n",
      "Epoch 28 | Loss: 3.8690\n",
      "Epoch 29 | Loss: 3.8577\n",
      "Epoch 30 | Loss: 3.8475\n",
      "Epoch 31 | Loss: 3.8375\n",
      "Epoch 32 | Loss: 3.8283\n",
      "Epoch 33 | Loss: 3.8195\n",
      "Epoch 34 | Loss: 3.8111\n",
      "Epoch 35 | Loss: 3.8029\n",
      "Epoch 36 | Loss: 3.7953\n",
      "Epoch 37 | Loss: 3.7881\n",
      "Epoch 38 | Loss: 3.7809\n",
      "Epoch 39 | Loss: 3.7742\n",
      "Epoch 40 | Loss: 3.7681\n",
      "Epoch 41 | Loss: 3.7613\n",
      "Epoch 42 | Loss: 3.7555\n",
      "Epoch 43 | Loss: 3.7491\n",
      "Epoch 44 | Loss: 3.7431\n",
      "Epoch 45 | Loss: 3.7372\n",
      "Epoch 46 | Loss: 3.7314\n",
      "Epoch 47 | Loss: 3.7260\n",
      "Epoch 48 | Loss: 3.7206\n",
      "Epoch 49 | Loss: 3.7148\n",
      "Epoch 50 | Loss: 3.7095\n",
      "Epoch 51 | Loss: 3.7036\n",
      "Epoch 52 | Loss: 3.6987\n",
      "Epoch 53 | Loss: 3.6936\n",
      "Epoch 54 | Loss: 3.6876\n",
      "Epoch 55 | Loss: 3.6822\n",
      "Epoch 56 | Loss: 3.6776\n",
      "Epoch 57 | Loss: 3.6719\n",
      "Epoch 58 | Loss: 3.6666\n",
      "Epoch 59 | Loss: 3.6611\n",
      "Epoch 60 | Loss: 3.6557\n",
      "Epoch 61 | Loss: 3.6502\n",
      "Epoch 62 | Loss: 3.6448\n",
      "Epoch 63 | Loss: 3.6392\n",
      "Epoch 64 | Loss: 3.6339\n",
      "Epoch 65 | Loss: 3.6288\n",
      "Epoch 66 | Loss: 3.6226\n",
      "Epoch 67 | Loss: 3.6169\n",
      "Epoch 68 | Loss: 3.6116\n",
      "Epoch 69 | Loss: 3.6060\n",
      "Epoch 70 | Loss: 3.6005\n",
      "Epoch 71 | Loss: 3.5949\n",
      "Epoch 72 | Loss: 3.5894\n",
      "Epoch 73 | Loss: 3.5833\n",
      "Epoch 74 | Loss: 3.5785\n",
      "Epoch 75 | Loss: 3.5725\n",
      "Epoch 76 | Loss: 3.5669\n",
      "Epoch 77 | Loss: 3.5611\n",
      "Epoch 78 | Loss: 3.5563\n",
      "Epoch 79 | Loss: 3.5504\n",
      "Epoch 80 | Loss: 3.5448\n",
      "Epoch 81 | Loss: 3.5396\n",
      "Epoch 82 | Loss: 3.5334\n",
      "Epoch 83 | Loss: 3.5284\n",
      "Epoch 84 | Loss: 3.5235\n",
      "Epoch 85 | Loss: 3.5179\n",
      "Epoch 86 | Loss: 3.5128\n",
      "Epoch 87 | Loss: 3.5074\n",
      "Epoch 88 | Loss: 3.5026\n",
      "Epoch 89 | Loss: 3.4977\n",
      "Epoch 90 | Loss: 3.4926\n",
      "Epoch 91 | Loss: 3.4874\n",
      "Epoch 92 | Loss: 3.4825\n",
      "Epoch 93 | Loss: 3.4776\n",
      "Epoch 94 | Loss: 3.4729\n",
      "Epoch 95 | Loss: 3.4689\n",
      "Epoch 96 | Loss: 3.4645\n",
      "Epoch 97 | Loss: 3.4594\n",
      "Epoch 98 | Loss: 3.4552\n",
      "Epoch 99 | Loss: 3.4507\n",
      "Epoch 100 | Loss: 3.4468\n",
      "Epoch 101 | Loss: 3.4428\n",
      "Epoch 102 | Loss: 3.4380\n",
      "Epoch 103 | Loss: 3.4342\n",
      "Epoch 104 | Loss: 3.4301\n",
      "Epoch 105 | Loss: 3.4258\n",
      "Epoch 106 | Loss: 3.4224\n",
      "Epoch 107 | Loss: 3.4182\n",
      "Epoch 108 | Loss: 3.4146\n",
      "Epoch 109 | Loss: 3.4107\n",
      "Epoch 110 | Loss: 3.4065\n",
      "Epoch 111 | Loss: 3.4032\n",
      "Epoch 112 | Loss: 3.3993\n",
      "Epoch 113 | Loss: 3.3952\n",
      "Epoch 114 | Loss: 3.3923\n",
      "Epoch 115 | Loss: 3.3882\n",
      "Epoch 116 | Loss: 3.3848\n",
      "Epoch 117 | Loss: 3.3813\n",
      "Epoch 118 | Loss: 3.3779\n",
      "Epoch 119 | Loss: 3.3739\n",
      "Epoch 120 | Loss: 3.3708\n",
      "Epoch 121 | Loss: 3.3675\n",
      "Epoch 122 | Loss: 3.3642\n",
      "Epoch 123 | Loss: 3.3607\n",
      "Epoch 124 | Loss: 3.3572\n",
      "Epoch 125 | Loss: 3.3537\n",
      "Epoch 126 | Loss: 3.3514\n",
      "Epoch 127 | Loss: 3.3474\n",
      "Epoch 128 | Loss: 3.3444\n",
      "Epoch 129 | Loss: 3.3412\n",
      "Epoch 130 | Loss: 3.3377\n",
      "Epoch 131 | Loss: 3.3345\n",
      "Epoch 132 | Loss: 3.3319\n",
      "Epoch 133 | Loss: 3.3287\n",
      "Epoch 134 | Loss: 3.3255\n",
      "Epoch 135 | Loss: 3.3226\n",
      "Epoch 136 | Loss: 3.3191\n",
      "Epoch 137 | Loss: 3.3162\n",
      "Epoch 138 | Loss: 3.3143\n",
      "Epoch 139 | Loss: 3.3105\n",
      "Epoch 140 | Loss: 3.3073\n",
      "Epoch 141 | Loss: 3.3048\n",
      "Epoch 142 | Loss: 3.3011\n",
      "Epoch 143 | Loss: 3.2984\n",
      "Epoch 144 | Loss: 3.2955\n",
      "Epoch 145 | Loss: 3.2924\n",
      "Epoch 146 | Loss: 3.2899\n",
      "Epoch 147 | Loss: 3.2868\n",
      "Epoch 148 | Loss: 3.2836\n",
      "Epoch 149 | Loss: 3.2814\n",
      "Epoch 150 | Loss: 3.2785\n",
      "Epoch 151 | Loss: 3.2758\n",
      "Epoch 152 | Loss: 3.2727\n",
      "Epoch 153 | Loss: 3.2699\n",
      "Epoch 154 | Loss: 3.2679\n",
      "Epoch 155 | Loss: 3.2642\n",
      "Epoch 156 | Loss: 3.2617\n",
      "Epoch 157 | Loss: 3.2592\n",
      "Epoch 158 | Loss: 3.2563\n",
      "Epoch 159 | Loss: 3.2538\n",
      "Epoch 160 | Loss: 3.2514\n",
      "Epoch 161 | Loss: 3.2485\n",
      "Epoch 162 | Loss: 3.2455\n",
      "Epoch 163 | Loss: 3.2426\n",
      "Epoch 164 | Loss: 3.2401\n",
      "Epoch 165 | Loss: 3.2379\n",
      "Epoch 166 | Loss: 3.2354\n",
      "Epoch 167 | Loss: 3.2324\n",
      "Epoch 168 | Loss: 3.2297\n",
      "Epoch 169 | Loss: 3.2277\n",
      "Epoch 170 | Loss: 3.2249\n",
      "Epoch 171 | Loss: 3.2226\n",
      "Epoch 172 | Loss: 3.2198\n",
      "Epoch 173 | Loss: 3.2169\n",
      "Epoch 174 | Loss: 3.2148\n",
      "Epoch 175 | Loss: 3.2123\n",
      "Epoch 176 | Loss: 3.2092\n",
      "Epoch 177 | Loss: 3.2075\n",
      "Epoch 178 | Loss: 3.2049\n",
      "Epoch 179 | Loss: 3.2020\n",
      "Epoch 180 | Loss: 3.2000\n",
      "Epoch 181 | Loss: 3.1974\n",
      "Epoch 182 | Loss: 3.1946\n",
      "Epoch 183 | Loss: 3.1923\n",
      "Epoch 184 | Loss: 3.1899\n",
      "Epoch 185 | Loss: 3.1879\n",
      "Epoch 186 | Loss: 3.1848\n",
      "Epoch 187 | Loss: 3.1825\n",
      "Epoch 188 | Loss: 3.1792\n",
      "Epoch 189 | Loss: 3.1774\n",
      "Epoch 190 | Loss: 3.1750\n",
      "Epoch 191 | Loss: 3.1728\n",
      "Epoch 192 | Loss: 3.1708\n",
      "Epoch 193 | Loss: 3.1676\n",
      "Epoch 194 | Loss: 3.1659\n",
      "Epoch 195 | Loss: 3.1627\n",
      "Epoch 196 | Loss: 3.1609\n",
      "Epoch 197 | Loss: 3.1579\n",
      "Epoch 198 | Loss: 3.1556\n",
      "Epoch 199 | Loss: 3.1535\n",
      "Epoch 200 | Loss: 3.1518\n",
      "Epoch 201 | Loss: 3.1491\n",
      "Epoch 202 | Loss: 3.1463\n",
      "Epoch 203 | Loss: 3.1441\n",
      "Epoch 204 | Loss: 3.1416\n",
      "Epoch 205 | Loss: 3.1395\n",
      "Epoch 206 | Loss: 3.1372\n",
      "Epoch 207 | Loss: 3.1351\n",
      "Epoch 208 | Loss: 3.1328\n",
      "Epoch 209 | Loss: 3.1310\n",
      "Epoch 210 | Loss: 3.1280\n",
      "Epoch 211 | Loss: 3.1259\n",
      "Epoch 212 | Loss: 3.1235\n",
      "Epoch 213 | Loss: 3.1210\n",
      "Epoch 214 | Loss: 3.1190\n",
      "Epoch 215 | Loss: 3.1163\n",
      "Epoch 216 | Loss: 3.1147\n",
      "Epoch 217 | Loss: 3.1124\n",
      "Epoch 218 | Loss: 3.1100\n",
      "Epoch 219 | Loss: 3.1079\n",
      "Epoch 220 | Loss: 3.1052\n",
      "Epoch 221 | Loss: 3.1031\n",
      "Epoch 222 | Loss: 3.1008\n",
      "Epoch 223 | Loss: 3.0986\n",
      "Epoch 224 | Loss: 3.0977\n",
      "Epoch 225 | Loss: 3.0934\n",
      "Epoch 226 | Loss: 3.0924\n",
      "Epoch 227 | Loss: 3.0893\n",
      "Epoch 228 | Loss: 3.0881\n",
      "Epoch 229 | Loss: 3.0849\n",
      "Epoch 230 | Loss: 3.0828\n",
      "Epoch 231 | Loss: 3.0803\n",
      "Epoch 232 | Loss: 3.0783\n",
      "Epoch 233 | Loss: 3.0756\n",
      "Epoch 234 | Loss: 3.0736\n",
      "Epoch 235 | Loss: 3.0708\n",
      "Epoch 236 | Loss: 3.0695\n",
      "Epoch 237 | Loss: 3.0669\n",
      "Epoch 238 | Loss: 3.0641\n",
      "Epoch 239 | Loss: 3.0614\n",
      "Epoch 240 | Loss: 3.0592\n",
      "Epoch 241 | Loss: 3.0568\n",
      "Epoch 242 | Loss: 3.0547\n",
      "Epoch 243 | Loss: 3.0531\n",
      "Epoch 244 | Loss: 3.0501\n",
      "Epoch 245 | Loss: 3.0473\n",
      "Epoch 246 | Loss: 3.0447\n",
      "Epoch 247 | Loss: 3.0421\n",
      "Epoch 248 | Loss: 3.0396\n",
      "Epoch 249 | Loss: 3.0370\n",
      "Epoch 250 | Loss: 3.0349\n",
      "Epoch 251 | Loss: 3.0326\n",
      "Epoch 252 | Loss: 3.0300\n",
      "Epoch 253 | Loss: 3.0281\n",
      "Epoch 254 | Loss: 3.0251\n",
      "Epoch 255 | Loss: 3.0227\n",
      "Epoch 256 | Loss: 3.0198\n",
      "Epoch 257 | Loss: 3.0178\n",
      "Epoch 258 | Loss: 3.0151\n",
      "Epoch 259 | Loss: 3.0134\n",
      "Epoch 260 | Loss: 3.0107\n",
      "Epoch 261 | Loss: 3.0089\n",
      "Epoch 262 | Loss: 3.0059\n",
      "Epoch 263 | Loss: 3.0039\n",
      "Epoch 264 | Loss: 3.0013\n",
      "Epoch 265 | Loss: 2.9997\n",
      "Epoch 266 | Loss: 2.9973\n",
      "Epoch 267 | Loss: 2.9955\n",
      "Epoch 268 | Loss: 2.9929\n",
      "Epoch 269 | Loss: 2.9909\n",
      "Epoch 270 | Loss: 2.9895\n",
      "Epoch 271 | Loss: 2.9860\n",
      "Epoch 272 | Loss: 2.9852\n",
      "Epoch 273 | Loss: 2.9823\n",
      "Epoch 274 | Loss: 2.9807\n",
      "Epoch 275 | Loss: 2.9783\n",
      "Epoch 276 | Loss: 2.9758\n",
      "Epoch 277 | Loss: 2.9743\n",
      "Epoch 278 | Loss: 2.9722\n",
      "Epoch 279 | Loss: 2.9711\n",
      "Epoch 280 | Loss: 2.9688\n",
      "Epoch 281 | Loss: 2.9667\n",
      "Epoch 282 | Loss: 2.9646\n",
      "Epoch 283 | Loss: 2.9632\n",
      "Epoch 284 | Loss: 2.9605\n",
      "Epoch 285 | Loss: 2.9593\n",
      "Epoch 286 | Loss: 2.9575\n",
      "Epoch 287 | Loss: 2.9553\n",
      "Epoch 288 | Loss: 2.9539\n",
      "Epoch 289 | Loss: 2.9518\n",
      "Epoch 290 | Loss: 2.9502\n",
      "Epoch 291 | Loss: 2.9476\n",
      "Epoch 292 | Loss: 2.9459\n",
      "Epoch 293 | Loss: 2.9441\n",
      "Epoch 294 | Loss: 2.9423\n",
      "Epoch 295 | Loss: 2.9403\n",
      "Epoch 296 | Loss: 2.9399\n",
      "Epoch 297 | Loss: 2.9371\n",
      "Epoch 298 | Loss: 2.9350\n",
      "Epoch 299 | Loss: 2.9335\n",
      "Epoch 300 | Loss: 2.9328\n",
      "Epoch 301 | Loss: 2.9298\n",
      "Epoch 302 | Loss: 2.9278\n",
      "Epoch 303 | Loss: 2.9263\n",
      "Epoch 304 | Loss: 2.9242\n",
      "Epoch 305 | Loss: 2.9229\n",
      "Epoch 306 | Loss: 2.9214\n",
      "Epoch 307 | Loss: 2.9190\n",
      "Epoch 308 | Loss: 2.9174\n",
      "Epoch 309 | Loss: 2.9156\n",
      "Epoch 310 | Loss: 2.9142\n",
      "Epoch 311 | Loss: 2.9123\n",
      "Epoch 312 | Loss: 2.9112\n",
      "Epoch 313 | Loss: 2.9086\n",
      "Epoch 314 | Loss: 2.9065\n",
      "Epoch 315 | Loss: 2.9055\n",
      "Epoch 316 | Loss: 2.9038\n",
      "Epoch 317 | Loss: 2.9020\n",
      "Epoch 318 | Loss: 2.9007\n",
      "Epoch 319 | Loss: 2.8986\n",
      "Epoch 320 | Loss: 2.8970\n",
      "Epoch 321 | Loss: 2.8955\n",
      "Epoch 322 | Loss: 2.8941\n",
      "Epoch 323 | Loss: 2.8922\n",
      "Epoch 324 | Loss: 2.8901\n",
      "Epoch 325 | Loss: 2.8891\n",
      "Epoch 326 | Loss: 2.8865\n",
      "Epoch 327 | Loss: 2.8852\n",
      "Epoch 328 | Loss: 2.8834\n",
      "Epoch 329 | Loss: 2.8819\n",
      "Epoch 330 | Loss: 2.8805\n",
      "Epoch 331 | Loss: 2.8788\n",
      "Epoch 332 | Loss: 2.8774\n",
      "Epoch 333 | Loss: 2.8754\n",
      "Epoch 334 | Loss: 2.8737\n",
      "Epoch 335 | Loss: 2.8724\n",
      "Epoch 336 | Loss: 2.8700\n",
      "Epoch 337 | Loss: 2.8696\n",
      "Epoch 338 | Loss: 2.8671\n",
      "Epoch 339 | Loss: 2.8659\n",
      "Epoch 340 | Loss: 2.8647\n",
      "Epoch 341 | Loss: 2.8629\n",
      "Epoch 342 | Loss: 2.8608\n",
      "Epoch 343 | Loss: 2.8596\n",
      "Epoch 344 | Loss: 2.8575\n",
      "Epoch 345 | Loss: 2.8559\n",
      "Epoch 346 | Loss: 2.8548\n",
      "Epoch 347 | Loss: 2.8530\n",
      "Epoch 348 | Loss: 2.8510\n",
      "Epoch 349 | Loss: 2.8497\n",
      "Epoch 350 | Loss: 2.8479\n",
      "Epoch 351 | Loss: 2.8464\n",
      "Epoch 352 | Loss: 2.8442\n",
      "Epoch 353 | Loss: 2.8436\n",
      "Epoch 354 | Loss: 2.8420\n",
      "Epoch 355 | Loss: 2.8402\n",
      "Epoch 356 | Loss: 2.8389\n",
      "Epoch 357 | Loss: 2.8371\n",
      "Epoch 358 | Loss: 2.8358\n",
      "Epoch 359 | Loss: 2.8339\n",
      "Epoch 360 | Loss: 2.8319\n",
      "Epoch 361 | Loss: 2.8303\n",
      "Epoch 362 | Loss: 2.8293\n",
      "Epoch 363 | Loss: 2.8279\n",
      "Epoch 364 | Loss: 2.8264\n",
      "Epoch 365 | Loss: 2.8248\n",
      "Epoch 366 | Loss: 2.8236\n",
      "Epoch 367 | Loss: 2.8222\n",
      "Epoch 368 | Loss: 2.8199\n",
      "Epoch 369 | Loss: 2.8191\n",
      "Epoch 370 | Loss: 2.8181\n",
      "Epoch 371 | Loss: 2.8154\n",
      "Epoch 372 | Loss: 2.8143\n",
      "Epoch 373 | Loss: 2.8128\n",
      "Epoch 374 | Loss: 2.8115\n",
      "Epoch 375 | Loss: 2.8095\n",
      "Epoch 376 | Loss: 2.8079\n",
      "Epoch 377 | Loss: 2.8068\n",
      "Epoch 378 | Loss: 2.8050\n",
      "Epoch 379 | Loss: 2.8040\n",
      "Epoch 380 | Loss: 2.8022\n",
      "Epoch 381 | Loss: 2.8005\n",
      "Epoch 382 | Loss: 2.7989\n",
      "Epoch 383 | Loss: 2.7980\n",
      "Epoch 384 | Loss: 2.7961\n",
      "Epoch 385 | Loss: 2.7943\n",
      "Epoch 386 | Loss: 2.7931\n",
      "Epoch 387 | Loss: 2.7915\n",
      "Epoch 388 | Loss: 2.7905\n",
      "Epoch 389 | Loss: 2.7885\n",
      "Epoch 390 | Loss: 2.7877\n",
      "Epoch 391 | Loss: 2.7862\n",
      "Epoch 392 | Loss: 2.7840\n",
      "Epoch 393 | Loss: 2.7826\n",
      "Epoch 394 | Loss: 2.7815\n",
      "Epoch 395 | Loss: 2.7823\n",
      "Epoch 396 | Loss: 2.7789\n",
      "Epoch 397 | Loss: 2.7771\n",
      "Epoch 398 | Loss: 2.7757\n",
      "Epoch 399 | Loss: 2.7752\n",
      "Epoch 400 | Loss: 2.7726\n",
      "Epoch 401 | Loss: 2.7714\n",
      "Epoch 402 | Loss: 2.7703\n",
      "Epoch 403 | Loss: 2.7688\n",
      "Epoch 404 | Loss: 2.7672\n",
      "Epoch 405 | Loss: 2.7678\n",
      "Epoch 406 | Loss: 2.7654\n",
      "Epoch 407 | Loss: 2.7629\n",
      "Epoch 408 | Loss: 2.7619\n",
      "Epoch 409 | Loss: 2.7601\n",
      "Epoch 410 | Loss: 2.7587\n",
      "Epoch 411 | Loss: 2.7575\n",
      "Epoch 412 | Loss: 2.7566\n",
      "Epoch 413 | Loss: 2.7556\n",
      "Epoch 414 | Loss: 2.7535\n",
      "Epoch 415 | Loss: 2.7523\n",
      "Epoch 416 | Loss: 2.7517\n",
      "Epoch 417 | Loss: 2.7499\n",
      "Epoch 418 | Loss: 2.7491\n",
      "Epoch 419 | Loss: 2.7467\n",
      "Epoch 420 | Loss: 2.7457\n",
      "Epoch 421 | Loss: 2.7436\n",
      "Epoch 422 | Loss: 2.7432\n",
      "Epoch 423 | Loss: 2.7417\n",
      "Epoch 424 | Loss: 2.7402\n",
      "Epoch 425 | Loss: 2.7385\n",
      "Epoch 426 | Loss: 2.7375\n",
      "Epoch 427 | Loss: 2.7362\n",
      "Epoch 428 | Loss: 2.7349\n",
      "Epoch 429 | Loss: 2.7338\n",
      "Epoch 430 | Loss: 2.7327\n",
      "Epoch 431 | Loss: 2.7313\n",
      "Epoch 432 | Loss: 2.7299\n",
      "Epoch 433 | Loss: 2.7289\n",
      "Epoch 434 | Loss: 2.7272\n",
      "Epoch 435 | Loss: 2.7258\n",
      "Epoch 436 | Loss: 2.7252\n",
      "Epoch 437 | Loss: 2.7239\n",
      "Epoch 438 | Loss: 2.7217\n",
      "Epoch 439 | Loss: 2.7209\n",
      "Epoch 440 | Loss: 2.7196\n",
      "Epoch 441 | Loss: 2.7183\n",
      "Epoch 442 | Loss: 2.7176\n",
      "Epoch 443 | Loss: 2.7161\n",
      "Epoch 444 | Loss: 2.7144\n",
      "Epoch 445 | Loss: 2.7137\n",
      "Epoch 446 | Loss: 2.7121\n",
      "Epoch 447 | Loss: 2.7108\n",
      "Epoch 448 | Loss: 2.7093\n",
      "Epoch 449 | Loss: 2.7082\n",
      "Epoch 450 | Loss: 2.7071\n",
      "Epoch 451 | Loss: 2.7064\n",
      "Epoch 452 | Loss: 2.7062\n",
      "Epoch 453 | Loss: 2.7035\n",
      "Epoch 454 | Loss: 2.7023\n",
      "Epoch 455 | Loss: 2.7007\n",
      "Epoch 456 | Loss: 2.6999\n",
      "Epoch 457 | Loss: 2.6986\n",
      "Epoch 458 | Loss: 2.6973\n",
      "Epoch 459 | Loss: 2.6960\n",
      "Epoch 460 | Loss: 2.6951\n",
      "Epoch 461 | Loss: 2.6934\n",
      "Epoch 462 | Loss: 2.6924\n",
      "Epoch 463 | Loss: 2.6923\n",
      "Epoch 464 | Loss: 2.6901\n",
      "Epoch 465 | Loss: 2.6887\n",
      "Epoch 466 | Loss: 2.6880\n",
      "Epoch 467 | Loss: 2.6868\n",
      "Epoch 468 | Loss: 2.6857\n",
      "Epoch 469 | Loss: 2.6837\n",
      "Epoch 470 | Loss: 2.6830\n",
      "Epoch 471 | Loss: 2.6823\n",
      "Epoch 472 | Loss: 2.6810\n",
      "Epoch 473 | Loss: 2.6792\n",
      "Epoch 474 | Loss: 2.6780\n",
      "Epoch 475 | Loss: 2.6772\n",
      "Epoch 476 | Loss: 2.6760\n",
      "Epoch 477 | Loss: 2.6752\n",
      "Epoch 478 | Loss: 2.6741\n",
      "Epoch 479 | Loss: 2.6728\n",
      "Epoch 480 | Loss: 2.6714\n",
      "Epoch 481 | Loss: 2.6702\n",
      "Epoch 482 | Loss: 2.6692\n",
      "Epoch 483 | Loss: 2.6680\n",
      "Epoch 484 | Loss: 2.6683\n",
      "Epoch 485 | Loss: 2.6658\n",
      "Epoch 486 | Loss: 2.6641\n",
      "Epoch 487 | Loss: 2.6635\n",
      "Epoch 488 | Loss: 2.6623\n",
      "Epoch 489 | Loss: 2.6616\n",
      "Epoch 490 | Loss: 2.6605\n",
      "Epoch 491 | Loss: 2.6593\n",
      "Epoch 492 | Loss: 2.6582\n",
      "Epoch 493 | Loss: 2.6571\n",
      "Epoch 494 | Loss: 2.6556\n",
      "Epoch 495 | Loss: 2.6543\n",
      "Epoch 496 | Loss: 2.6534\n",
      "Epoch 497 | Loss: 2.6525\n",
      "Epoch 498 | Loss: 2.6513\n",
      "Epoch 499 | Loss: 2.6505\n",
      "Epoch 500 | Loss: 2.6494\n",
      "Epoch 501 | Loss: 2.6479\n",
      "Epoch 502 | Loss: 2.6476\n",
      "Epoch 503 | Loss: 2.6455\n",
      "Epoch 504 | Loss: 2.6448\n",
      "Epoch 505 | Loss: 2.6436\n",
      "Epoch 506 | Loss: 2.6418\n",
      "Epoch 507 | Loss: 2.6415\n",
      "Epoch 508 | Loss: 2.6403\n",
      "Epoch 509 | Loss: 2.6399\n",
      "Epoch 510 | Loss: 2.6379\n",
      "Epoch 511 | Loss: 2.6374\n",
      "Epoch 512 | Loss: 2.6360\n",
      "Epoch 513 | Loss: 2.6350\n",
      "Epoch 514 | Loss: 2.6339\n",
      "Epoch 515 | Loss: 2.6329\n",
      "Epoch 516 | Loss: 2.6321\n",
      "Epoch 517 | Loss: 2.6311\n",
      "Epoch 518 | Loss: 2.6293\n",
      "Epoch 519 | Loss: 2.6285\n",
      "Epoch 520 | Loss: 2.6276\n",
      "Epoch 521 | Loss: 2.6265\n",
      "Epoch 522 | Loss: 2.6253\n",
      "Epoch 523 | Loss: 2.6234\n",
      "Epoch 524 | Loss: 2.6234\n",
      "Epoch 525 | Loss: 2.6220\n",
      "Epoch 526 | Loss: 2.6216\n",
      "Epoch 527 | Loss: 2.6204\n",
      "Epoch 528 | Loss: 2.6192\n",
      "Epoch 529 | Loss: 2.6179\n",
      "Epoch 530 | Loss: 2.6172\n",
      "Epoch 531 | Loss: 2.6157\n",
      "Epoch 532 | Loss: 2.6146\n",
      "Epoch 533 | Loss: 2.6130\n",
      "Epoch 534 | Loss: 2.6124\n",
      "Epoch 535 | Loss: 2.6114\n",
      "Epoch 536 | Loss: 2.6103\n",
      "Epoch 537 | Loss: 2.6094\n",
      "Epoch 538 | Loss: 2.6081\n",
      "Epoch 539 | Loss: 2.6076\n",
      "Epoch 540 | Loss: 2.6070\n",
      "Epoch 541 | Loss: 2.6057\n",
      "Epoch 542 | Loss: 2.6043\n",
      "Epoch 543 | Loss: 2.6027\n",
      "Epoch 544 | Loss: 2.6020\n",
      "Epoch 545 | Loss: 2.6016\n",
      "Epoch 546 | Loss: 2.5999\n",
      "Epoch 547 | Loss: 2.5989\n",
      "Epoch 548 | Loss: 2.5981\n",
      "Epoch 549 | Loss: 2.5966\n",
      "Epoch 550 | Loss: 2.5964\n",
      "Epoch 551 | Loss: 2.5952\n",
      "Epoch 552 | Loss: 2.5940\n",
      "Epoch 553 | Loss: 2.5930\n",
      "Epoch 554 | Loss: 2.5919\n",
      "Epoch 555 | Loss: 2.5912\n",
      "Epoch 556 | Loss: 2.5899\n",
      "Epoch 557 | Loss: 2.5891\n",
      "Epoch 558 | Loss: 2.5883\n",
      "Epoch 559 | Loss: 2.5871\n",
      "Epoch 560 | Loss: 2.5860\n",
      "Epoch 561 | Loss: 2.5848\n",
      "Epoch 562 | Loss: 2.5841\n",
      "Epoch 563 | Loss: 2.5831\n",
      "Epoch 564 | Loss: 2.5821\n",
      "Epoch 565 | Loss: 2.5807\n",
      "Epoch 566 | Loss: 2.5795\n",
      "Epoch 567 | Loss: 2.5791\n",
      "Epoch 568 | Loss: 2.5781\n",
      "Epoch 569 | Loss: 2.5769\n",
      "Epoch 570 | Loss: 2.5763\n",
      "Epoch 571 | Loss: 2.5751\n",
      "Epoch 572 | Loss: 2.5737\n",
      "Epoch 573 | Loss: 2.5731\n",
      "Epoch 574 | Loss: 2.5720\n",
      "Epoch 575 | Loss: 2.5714\n",
      "Epoch 576 | Loss: 2.5704\n",
      "Epoch 577 | Loss: 2.5687\n",
      "Epoch 578 | Loss: 2.5685\n",
      "Epoch 579 | Loss: 2.5676\n",
      "Epoch 580 | Loss: 2.5657\n",
      "Epoch 581 | Loss: 2.5649\n",
      "Epoch 582 | Loss: 2.5642\n",
      "Epoch 583 | Loss: 2.5632\n",
      "Epoch 584 | Loss: 2.5619\n",
      "Epoch 585 | Loss: 2.5631\n",
      "Epoch 586 | Loss: 2.5603\n",
      "Epoch 587 | Loss: 2.5592\n",
      "Epoch 588 | Loss: 2.5580\n",
      "Epoch 589 | Loss: 2.5577\n",
      "Epoch 590 | Loss: 2.5563\n",
      "Epoch 591 | Loss: 2.5564\n",
      "Epoch 592 | Loss: 2.5550\n",
      "Epoch 593 | Loss: 2.5534\n",
      "Epoch 594 | Loss: 2.5528\n",
      "Epoch 595 | Loss: 2.5521\n",
      "Epoch 596 | Loss: 2.5511\n",
      "Epoch 597 | Loss: 2.5498\n",
      "Epoch 598 | Loss: 2.5487\n",
      "Epoch 599 | Loss: 2.5476\n",
      "Epoch 600 | Loss: 2.5468\n",
      "Epoch 601 | Loss: 2.5482\n",
      "Epoch 602 | Loss: 2.5452\n",
      "Epoch 603 | Loss: 2.5441\n",
      "Epoch 604 | Loss: 2.5426\n",
      "Epoch 605 | Loss: 2.5419\n",
      "Epoch 606 | Loss: 2.5413\n",
      "Epoch 607 | Loss: 2.5405\n",
      "Epoch 608 | Loss: 2.5388\n",
      "Epoch 609 | Loss: 2.5381\n",
      "Epoch 610 | Loss: 2.5371\n",
      "Epoch 611 | Loss: 2.5359\n",
      "Epoch 612 | Loss: 2.5351\n",
      "Epoch 613 | Loss: 2.5352\n",
      "Epoch 614 | Loss: 2.5332\n",
      "Epoch 615 | Loss: 2.5331\n",
      "Epoch 616 | Loss: 2.5318\n",
      "Epoch 617 | Loss: 2.5307\n",
      "Epoch 618 | Loss: 2.5297\n",
      "Epoch 619 | Loss: 2.5288\n",
      "Epoch 620 | Loss: 2.5279\n",
      "Epoch 621 | Loss: 2.5272\n",
      "Epoch 622 | Loss: 2.5257\n",
      "Epoch 623 | Loss: 2.5254\n",
      "Epoch 624 | Loss: 2.5243\n",
      "Epoch 625 | Loss: 2.5230\n",
      "Epoch 626 | Loss: 2.5228\n",
      "Epoch 627 | Loss: 2.5211\n",
      "Epoch 628 | Loss: 2.5203\n",
      "Epoch 629 | Loss: 2.5191\n",
      "Epoch 630 | Loss: 2.5187\n",
      "Epoch 631 | Loss: 2.5179\n",
      "Epoch 632 | Loss: 2.5163\n",
      "Epoch 633 | Loss: 2.5160\n",
      "Epoch 634 | Loss: 2.5163\n",
      "Epoch 635 | Loss: 2.5138\n",
      "Epoch 636 | Loss: 2.5134\n",
      "Epoch 637 | Loss: 2.5125\n",
      "Epoch 638 | Loss: 2.5114\n",
      "Epoch 639 | Loss: 2.5106\n",
      "Epoch 640 | Loss: 2.5096\n",
      "Epoch 641 | Loss: 2.5086\n",
      "Epoch 642 | Loss: 2.5075\n",
      "Epoch 643 | Loss: 2.5065\n",
      "Epoch 644 | Loss: 2.5060\n",
      "Epoch 645 | Loss: 2.5050\n",
      "Epoch 646 | Loss: 2.5043\n",
      "Epoch 647 | Loss: 2.5030\n",
      "Epoch 648 | Loss: 2.5022\n",
      "Epoch 649 | Loss: 2.5010\n",
      "Epoch 650 | Loss: 2.5006\n",
      "Epoch 651 | Loss: 2.4996\n",
      "Epoch 652 | Loss: 2.4987\n",
      "Epoch 653 | Loss: 2.4981\n",
      "Epoch 654 | Loss: 2.4966\n",
      "Epoch 655 | Loss: 2.4955\n",
      "Epoch 656 | Loss: 2.4947\n",
      "Epoch 657 | Loss: 2.4942\n",
      "Epoch 658 | Loss: 2.4931\n",
      "Epoch 659 | Loss: 2.4926\n",
      "Epoch 660 | Loss: 2.4906\n",
      "Epoch 661 | Loss: 2.4906\n",
      "Epoch 662 | Loss: 2.4897\n",
      "Epoch 663 | Loss: 2.4887\n",
      "Epoch 664 | Loss: 2.4881\n",
      "Epoch 665 | Loss: 2.4870\n",
      "Epoch 666 | Loss: 2.4861\n",
      "Epoch 667 | Loss: 2.4854\n",
      "Epoch 668 | Loss: 2.4841\n",
      "Epoch 669 | Loss: 2.4834\n",
      "Epoch 670 | Loss: 2.4828\n",
      "Epoch 671 | Loss: 2.4818\n",
      "Epoch 672 | Loss: 2.4805\n",
      "Epoch 673 | Loss: 2.4795\n",
      "Epoch 674 | Loss: 2.4787\n",
      "Epoch 675 | Loss: 2.4781\n",
      "Epoch 676 | Loss: 2.4773\n",
      "Epoch 677 | Loss: 2.4768\n",
      "Epoch 678 | Loss: 2.4760\n",
      "Epoch 679 | Loss: 2.4746\n",
      "Epoch 680 | Loss: 2.4735\n",
      "Epoch 681 | Loss: 2.4725\n",
      "Epoch 682 | Loss: 2.4722\n",
      "Epoch 683 | Loss: 2.4715\n",
      "Epoch 684 | Loss: 2.4699\n",
      "Epoch 685 | Loss: 2.4690\n",
      "Epoch 686 | Loss: 2.4682\n",
      "Epoch 687 | Loss: 2.4675\n",
      "Epoch 688 | Loss: 2.4665\n",
      "Epoch 689 | Loss: 2.4663\n",
      "Epoch 690 | Loss: 2.4653\n",
      "Epoch 691 | Loss: 2.4641\n",
      "Epoch 692 | Loss: 2.4630\n",
      "Epoch 693 | Loss: 2.4621\n",
      "Epoch 694 | Loss: 2.4616\n",
      "Epoch 695 | Loss: 2.4606\n",
      "Epoch 696 | Loss: 2.4596\n",
      "Epoch 697 | Loss: 2.4593\n",
      "Epoch 698 | Loss: 2.4578\n",
      "Epoch 699 | Loss: 2.4571\n",
      "Epoch 700 | Loss: 2.4566\n",
      "Epoch 701 | Loss: 2.4555\n",
      "Epoch 702 | Loss: 2.4546\n",
      "Epoch 703 | Loss: 2.4543\n",
      "Epoch 704 | Loss: 2.4523\n",
      "Epoch 705 | Loss: 2.4524\n",
      "Epoch 706 | Loss: 2.4510\n",
      "Epoch 707 | Loss: 2.4503\n",
      "Epoch 708 | Loss: 2.4492\n",
      "Epoch 709 | Loss: 2.4486\n",
      "Epoch 710 | Loss: 2.4475\n",
      "Epoch 711 | Loss: 2.4466\n",
      "Epoch 712 | Loss: 2.4461\n",
      "Epoch 713 | Loss: 2.4452\n",
      "Epoch 714 | Loss: 2.4445\n",
      "Epoch 715 | Loss: 2.4434\n",
      "Epoch 716 | Loss: 2.4431\n",
      "Epoch 717 | Loss: 2.4413\n",
      "Epoch 718 | Loss: 2.4409\n",
      "Epoch 719 | Loss: 2.4403\n",
      "Epoch 720 | Loss: 2.4399\n",
      "Epoch 721 | Loss: 2.4382\n",
      "Epoch 722 | Loss: 2.4377\n",
      "Epoch 723 | Loss: 2.4367\n",
      "Epoch 724 | Loss: 2.4356\n",
      "Epoch 725 | Loss: 2.4352\n",
      "Epoch 726 | Loss: 2.4342\n",
      "Epoch 727 | Loss: 2.4333\n",
      "Epoch 728 | Loss: 2.4327\n",
      "Epoch 729 | Loss: 2.4319\n",
      "Epoch 730 | Loss: 2.4310\n",
      "Epoch 731 | Loss: 2.4299\n",
      "Epoch 732 | Loss: 2.4291\n",
      "Epoch 733 | Loss: 2.4289\n",
      "Epoch 734 | Loss: 2.4269\n",
      "Epoch 735 | Loss: 2.4267\n",
      "Epoch 736 | Loss: 2.4253\n",
      "Epoch 737 | Loss: 2.4246\n",
      "Epoch 738 | Loss: 2.4241\n",
      "Epoch 739 | Loss: 2.4233\n",
      "Epoch 740 | Loss: 2.4229\n",
      "Epoch 741 | Loss: 2.4218\n",
      "Epoch 742 | Loss: 2.4211\n",
      "Epoch 743 | Loss: 2.4199\n",
      "Epoch 744 | Loss: 2.4191\n",
      "Epoch 745 | Loss: 2.4184\n",
      "Epoch 746 | Loss: 2.4173\n",
      "Epoch 747 | Loss: 2.4167\n",
      "Epoch 748 | Loss: 2.4161\n",
      "Epoch 749 | Loss: 2.4152\n",
      "Epoch 750 | Loss: 2.4143\n",
      "Epoch 751 | Loss: 2.4135\n",
      "Epoch 752 | Loss: 2.4133\n",
      "Epoch 753 | Loss: 2.4120\n",
      "Epoch 754 | Loss: 2.4111\n",
      "Epoch 755 | Loss: 2.4097\n",
      "Epoch 756 | Loss: 2.4093\n",
      "Epoch 757 | Loss: 2.4084\n",
      "Epoch 758 | Loss: 2.4081\n",
      "Epoch 759 | Loss: 2.4072\n",
      "Epoch 760 | Loss: 2.4059\n",
      "Epoch 761 | Loss: 2.4054\n",
      "Epoch 762 | Loss: 2.4046\n",
      "Epoch 763 | Loss: 2.4038\n",
      "Epoch 764 | Loss: 2.4023\n",
      "Epoch 765 | Loss: 2.4020\n",
      "Epoch 766 | Loss: 2.4010\n",
      "Epoch 767 | Loss: 2.4005\n",
      "Epoch 768 | Loss: 2.3999\n",
      "Epoch 769 | Loss: 2.3990\n",
      "Epoch 770 | Loss: 2.3977\n",
      "Epoch 771 | Loss: 2.3970\n",
      "Epoch 772 | Loss: 2.3962\n",
      "Epoch 773 | Loss: 2.3956\n",
      "Epoch 774 | Loss: 2.3945\n",
      "Epoch 775 | Loss: 2.3940\n",
      "Epoch 776 | Loss: 2.3933\n",
      "Epoch 777 | Loss: 2.3926\n",
      "Epoch 778 | Loss: 2.3911\n",
      "Epoch 779 | Loss: 2.3909\n",
      "Epoch 780 | Loss: 2.3901\n",
      "Epoch 781 | Loss: 2.3891\n",
      "Epoch 782 | Loss: 2.3882\n",
      "Epoch 783 | Loss: 2.3876\n",
      "Epoch 784 | Loss: 2.3862\n",
      "Epoch 785 | Loss: 2.3863\n",
      "Epoch 786 | Loss: 2.3850\n",
      "Epoch 787 | Loss: 2.3848\n",
      "Epoch 788 | Loss: 2.3837\n",
      "Epoch 789 | Loss: 2.3828\n",
      "Epoch 790 | Loss: 2.3819\n",
      "Epoch 791 | Loss: 2.3808\n",
      "Epoch 792 | Loss: 2.3802\n",
      "Epoch 793 | Loss: 2.3793\n",
      "Epoch 794 | Loss: 2.3791\n",
      "Epoch 795 | Loss: 2.3776\n",
      "Epoch 796 | Loss: 2.3779\n",
      "Epoch 797 | Loss: 2.3762\n",
      "Epoch 798 | Loss: 2.3758\n",
      "Epoch 799 | Loss: 2.3751\n",
      "Epoch 800 | Loss: 2.3743\n",
      "Epoch 801 | Loss: 2.3733\n",
      "Epoch 802 | Loss: 2.3727\n",
      "Epoch 803 | Loss: 2.3737\n",
      "Epoch 804 | Loss: 2.3709\n",
      "Epoch 805 | Loss: 2.3700\n",
      "Epoch 806 | Loss: 2.3694\n",
      "Epoch 807 | Loss: 2.3685\n",
      "Epoch 808 | Loss: 2.3675\n",
      "Epoch 809 | Loss: 2.3671\n",
      "Epoch 810 | Loss: 2.3664\n",
      "Epoch 811 | Loss: 2.3656\n",
      "Epoch 812 | Loss: 2.3646\n",
      "Epoch 813 | Loss: 2.3640\n",
      "Epoch 814 | Loss: 2.3634\n",
      "Epoch 815 | Loss: 2.3629\n",
      "Epoch 816 | Loss: 2.3617\n",
      "Epoch 817 | Loss: 2.3607\n",
      "Epoch 818 | Loss: 2.3597\n",
      "Epoch 819 | Loss: 2.3594\n",
      "Epoch 820 | Loss: 2.3585\n",
      "Epoch 821 | Loss: 2.3573\n",
      "Epoch 822 | Loss: 2.3573\n",
      "Epoch 823 | Loss: 2.3565\n",
      "Epoch 824 | Loss: 2.3556\n",
      "Epoch 825 | Loss: 2.3545\n",
      "Epoch 826 | Loss: 2.3535\n",
      "Epoch 827 | Loss: 2.3528\n",
      "Epoch 828 | Loss: 2.3524\n",
      "Epoch 829 | Loss: 2.3514\n",
      "Epoch 830 | Loss: 2.3510\n",
      "Epoch 831 | Loss: 2.3502\n",
      "Epoch 832 | Loss: 2.3498\n",
      "Epoch 833 | Loss: 2.3483\n",
      "Epoch 834 | Loss: 2.3481\n",
      "Epoch 835 | Loss: 2.3470\n",
      "Epoch 836 | Loss: 2.3465\n",
      "Epoch 837 | Loss: 2.3455\n",
      "Epoch 838 | Loss: 2.3454\n",
      "Epoch 839 | Loss: 2.3447\n",
      "Epoch 840 | Loss: 2.3437\n",
      "Epoch 841 | Loss: 2.3424\n",
      "Epoch 842 | Loss: 2.3420\n",
      "Epoch 843 | Loss: 2.3410\n",
      "Epoch 844 | Loss: 2.3404\n",
      "Epoch 845 | Loss: 2.3399\n",
      "Epoch 846 | Loss: 2.3387\n",
      "Epoch 847 | Loss: 2.3380\n",
      "Epoch 848 | Loss: 2.3373\n",
      "Epoch 849 | Loss: 2.3362\n",
      "Epoch 850 | Loss: 2.3358\n",
      "Epoch 851 | Loss: 2.3353\n",
      "Epoch 852 | Loss: 2.3344\n",
      "Epoch 853 | Loss: 2.3334\n",
      "Epoch 854 | Loss: 2.3325\n",
      "Epoch 855 | Loss: 2.3322\n",
      "Epoch 856 | Loss: 2.3312\n",
      "Epoch 857 | Loss: 2.3301\n",
      "Epoch 858 | Loss: 2.3292\n",
      "Epoch 859 | Loss: 2.3291\n",
      "Epoch 860 | Loss: 2.3279\n",
      "Epoch 861 | Loss: 2.3268\n",
      "Epoch 862 | Loss: 2.3264\n",
      "Epoch 863 | Loss: 2.3252\n",
      "Epoch 864 | Loss: 2.3247\n",
      "Epoch 865 | Loss: 2.3249\n",
      "Epoch 866 | Loss: 2.3235\n",
      "Epoch 867 | Loss: 2.3224\n",
      "Epoch 868 | Loss: 2.3221\n",
      "Epoch 869 | Loss: 2.3212\n",
      "Epoch 870 | Loss: 2.3206\n",
      "Epoch 871 | Loss: 2.3209\n",
      "Epoch 872 | Loss: 2.3192\n",
      "Epoch 873 | Loss: 2.3185\n",
      "Epoch 874 | Loss: 2.3173\n",
      "Epoch 875 | Loss: 2.3170\n",
      "Epoch 876 | Loss: 2.3158\n",
      "Epoch 877 | Loss: 2.3152\n",
      "Epoch 878 | Loss: 2.3145\n",
      "Epoch 879 | Loss: 2.3139\n",
      "Epoch 880 | Loss: 2.3129\n",
      "Epoch 881 | Loss: 2.3129\n",
      "Epoch 882 | Loss: 2.3119\n",
      "Epoch 883 | Loss: 2.3112\n",
      "Epoch 884 | Loss: 2.3104\n",
      "Epoch 885 | Loss: 2.3099\n",
      "Epoch 886 | Loss: 2.3091\n",
      "Epoch 887 | Loss: 2.3083\n",
      "Epoch 888 | Loss: 2.3072\n",
      "Epoch 889 | Loss: 2.3074\n",
      "Epoch 890 | Loss: 2.3065\n",
      "Epoch 891 | Loss: 2.3055\n",
      "Epoch 892 | Loss: 2.3053\n",
      "Epoch 893 | Loss: 2.3032\n",
      "Epoch 894 | Loss: 2.3032\n",
      "Epoch 895 | Loss: 2.3023\n",
      "Epoch 896 | Loss: 2.3031\n",
      "Epoch 897 | Loss: 2.3012\n",
      "Epoch 898 | Loss: 2.2998\n",
      "Epoch 899 | Loss: 2.2993\n",
      "Epoch 900 | Loss: 2.2988\n",
      "Epoch 901 | Loss: 2.2982\n",
      "Epoch 902 | Loss: 2.2973\n",
      "Epoch 903 | Loss: 2.2966\n",
      "Epoch 904 | Loss: 2.2962\n",
      "Epoch 905 | Loss: 2.2949\n",
      "Epoch 906 | Loss: 2.2943\n",
      "Epoch 907 | Loss: 2.2932\n",
      "Epoch 908 | Loss: 2.2928\n",
      "Epoch 909 | Loss: 2.2922\n",
      "Epoch 910 | Loss: 2.2918\n",
      "Epoch 911 | Loss: 2.2908\n",
      "Epoch 912 | Loss: 2.2903\n",
      "Epoch 913 | Loss: 2.2889\n",
      "Epoch 914 | Loss: 2.2885\n",
      "Epoch 915 | Loss: 2.2876\n",
      "Epoch 916 | Loss: 2.2871\n",
      "Epoch 917 | Loss: 2.2866\n",
      "Epoch 918 | Loss: 2.2859\n",
      "Epoch 919 | Loss: 2.2851\n",
      "Epoch 920 | Loss: 2.2848\n",
      "Epoch 921 | Loss: 2.2834\n",
      "Epoch 922 | Loss: 2.2830\n",
      "Epoch 923 | Loss: 2.2825\n",
      "Epoch 924 | Loss: 2.2817\n",
      "Epoch 925 | Loss: 2.2808\n",
      "Epoch 926 | Loss: 2.2801\n",
      "Epoch 927 | Loss: 2.2808\n",
      "Epoch 928 | Loss: 2.2788\n",
      "Epoch 929 | Loss: 2.2775\n",
      "Epoch 930 | Loss: 2.2773\n",
      "Epoch 931 | Loss: 2.2763\n",
      "Epoch 932 | Loss: 2.2753\n",
      "Epoch 933 | Loss: 2.2749\n",
      "Epoch 934 | Loss: 2.2742\n",
      "Epoch 935 | Loss: 2.2738\n",
      "Epoch 936 | Loss: 2.2731\n",
      "Epoch 937 | Loss: 2.2721\n",
      "Epoch 938 | Loss: 2.2714\n",
      "Epoch 939 | Loss: 2.2712\n",
      "Epoch 940 | Loss: 2.2705\n",
      "Epoch 941 | Loss: 2.2699\n",
      "Epoch 942 | Loss: 2.2711\n",
      "Epoch 943 | Loss: 2.2684\n",
      "Epoch 944 | Loss: 2.2687\n",
      "Epoch 945 | Loss: 2.2666\n",
      "Epoch 946 | Loss: 2.2669\n",
      "Epoch 947 | Loss: 2.2655\n",
      "Epoch 948 | Loss: 2.2649\n",
      "Epoch 949 | Loss: 2.2639\n",
      "Epoch 950 | Loss: 2.2632\n",
      "Epoch 951 | Loss: 2.2629\n",
      "Epoch 952 | Loss: 2.2616\n",
      "Epoch 953 | Loss: 2.2614\n",
      "Epoch 954 | Loss: 2.2606\n",
      "Epoch 955 | Loss: 2.2596\n",
      "Epoch 956 | Loss: 2.2594\n",
      "Epoch 957 | Loss: 2.2580\n",
      "Epoch 958 | Loss: 2.2578\n",
      "Epoch 959 | Loss: 2.2569\n",
      "Epoch 960 | Loss: 2.2559\n",
      "Epoch 961 | Loss: 2.2555\n",
      "Epoch 962 | Loss: 2.2553\n",
      "Epoch 963 | Loss: 2.2541\n",
      "Epoch 964 | Loss: 2.2533\n",
      "Epoch 965 | Loss: 2.2529\n",
      "Epoch 966 | Loss: 2.2526\n",
      "Epoch 967 | Loss: 2.2514\n",
      "Epoch 968 | Loss: 2.2506\n",
      "Epoch 969 | Loss: 2.2502\n",
      "Epoch 970 | Loss: 2.2498\n",
      "Epoch 971 | Loss: 2.2486\n",
      "Epoch 972 | Loss: 2.2482\n",
      "Epoch 973 | Loss: 2.2477\n",
      "Epoch 974 | Loss: 2.2468\n",
      "Epoch 975 | Loss: 2.2463\n",
      "Epoch 976 | Loss: 2.2448\n",
      "Epoch 977 | Loss: 2.2449\n",
      "Epoch 978 | Loss: 2.2442\n",
      "Epoch 979 | Loss: 2.2436\n",
      "Epoch 980 | Loss: 2.2427\n",
      "Epoch 981 | Loss: 2.2424\n",
      "Epoch 982 | Loss: 2.2413\n",
      "Epoch 983 | Loss: 2.2409\n",
      "Epoch 984 | Loss: 2.2401\n",
      "Epoch 985 | Loss: 2.2397\n",
      "Epoch 986 | Loss: 2.2390\n",
      "Epoch 987 | Loss: 2.2378\n",
      "Epoch 988 | Loss: 2.2377\n",
      "Epoch 989 | Loss: 2.2369\n",
      "Epoch 990 | Loss: 2.2366\n",
      "Epoch 991 | Loss: 2.2365\n",
      "Epoch 992 | Loss: 2.2346\n",
      "Epoch 993 | Loss: 2.2340\n",
      "Epoch 994 | Loss: 2.2337\n",
      "Epoch 995 | Loss: 2.2328\n",
      "Epoch 996 | Loss: 2.2322\n",
      "Epoch 997 | Loss: 2.2319\n",
      "Epoch 998 | Loss: 2.2308\n",
      "Epoch 999 | Loss: 2.2313\n",
      "Epoch 1000 | Loss: 2.2292\n",
      "Saved Parameters\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 3000\n",
    "lr = 0.01\n",
    "encoder_len = 3        # English length\n",
    "target_len = 12        # Japanese length\n",
    "hidden_size = 128\n",
    "\n",
    "bos_id, pad_id, eos_id = 2, 0, 3\n",
    "\n",
    "Encoder_GRU = GRU(64, hidden_size)\n",
    "Decoder_GRU = GRU(64, hidden_size)\n",
    "\n",
    "X = np.array(eng_encoded, dtype=int)\n",
    "y = np.array(jpn_encoded, dtype=int)\n",
    "\n",
    "V_jpn = jpn_embedding.shape[0]\n",
    "o_w = np.random.randn(V_jpn, hidden_size) * 0.01\n",
    "o_b = np.zeros(V_jpn)\n",
    "\n",
    "past_loss = []\n",
    "break_loop = False\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    X_train = X[perm]\n",
    "    y_train = y[perm]\n",
    "\n",
    "    for batch in range(0, X.shape[0], batch_size):\n",
    "        end = min(batch + batch_size, X_train.shape[0])\n",
    "        X_ids = X_train[batch:end]\n",
    "        y_train_batch = y_train[batch:end]\n",
    "        batch_len = X_ids.shape[0]\n",
    "        \n",
    "        # Encoder forward\n",
    "        X_emb = eng_embedding[X_ids]\n",
    "        h_enc, enc_h_all, enc_store = Encoder_GRU.forward(X_emb)\n",
    "\n",
    "        decoder_input = np.concatenate(\n",
    "            [np.full((batch_len, 1), bos_id, dtype=int), y_train_batch[:, :-1]],\n",
    "            axis=1\n",
    "        )\n",
    "        decoder_target = y_train_batch\n",
    "\n",
    "        dec_in_emb = jpn_embedding[decoder_input]        # (N, T, D)\n",
    "        _, h_dec_all, dec_store = Decoder_GRU.forward(dec_in_emb, h_enc)\n",
    "\n",
    "        logits_flat = (h_dec_all @ o_w.T + o_b).reshape(-1, V_jpn) # (bs*seq_len, V_jpn)\n",
    "        targets_flat = decoder_target.reshape(-1)\n",
    "\n",
    "        mask = (targets_flat != pad_id)\n",
    "        probs = softmax(logits_flat)\n",
    "        probs = np.clip(probs, 1e-12, 1 - 1e-12)\n",
    "        \n",
    "        loss = -np.sum(np.log(probs[np.where(mask)[0], targets_flat[mask]])) / mask.sum()\n",
    "        total_loss += loss * batch_len\n",
    "\n",
    "        # Backprop through softmax + cross entropy\n",
    "        dlogits = probs\n",
    "        dlogits[np.where(mask)[0], targets_flat[mask]] -= 1\n",
    "        dlogits /= mask.sum()  # average over non pad tokens\n",
    "\n",
    "        # Final linear layer grad\n",
    "        h_dec_all_flat = h_dec_all.reshape(-1, hidden_size)\n",
    "        do_w = dlogits.T @ h_dec_all_flat    # (V_jpn, H)\n",
    "        do_b = np.sum(dlogits, axis=0)       # (V_jpn,)\n",
    "        dh_dec = dlogits @ o_w\n",
    "        dh_dec = dh_dec.reshape(batch_len, target_len, hidden_size)\n",
    "\n",
    "        # Decoder GRU backward\n",
    "        dH_enc_from_dec, dx_dec = Decoder_GRU.backward(dh_dec, dec_store, lr)\n",
    "\n",
    "        # Update decoder embeddings and clip gradients\n",
    "        for b in range(batch_len):\n",
    "            for t in range(target_len):\n",
    "                idx_tok = decoder_input[b, t]\n",
    "                grad = np.clip(dx_dec[b, t], -1.0, 1.0)\n",
    "                jpn_embedding[idx_tok] -= lr * grad\n",
    "\n",
    "        # Encoder GRU backward\n",
    "        # Build gradient tensor with only last time-step having gradient\n",
    "        dH_enc_all = np.zeros_like(enc_h_all)           # (B, encoder_len, H)\n",
    "        dH_enc_all[:, -1, :] = dH_enc_from_dec          # gradient only on final encoder state\n",
    "        _, dx_enc = Encoder_GRU.backward(dH_enc_all, enc_store, lr)\n",
    "\n",
    "        # Update encoder embeddings and clip gradients\n",
    "        for b in range(batch_len):\n",
    "            for t in range(encoder_len):\n",
    "                tok = X_ids[b, t]\n",
    "                grad = np.clip(dx_enc[b, t], -1.0, 1.0)\n",
    "                eng_embedding[tok] -= lr * grad\n",
    "\n",
    "        # Clip output gradients\n",
    "        np.clip(do_w, -1.0, 1.0, out=do_w)\n",
    "        np.clip(do_b, -1.0, 1.0, out=do_b)\n",
    "\n",
    "        # Update output layer\n",
    "        o_w -= lr * do_w\n",
    "        o_b -= lr * do_b\n",
    "\n",
    "    tl = total_loss / X_train.shape[0]\n",
    "    print(f\"Epoch {epoch+1} | Loss: {tl:.4f}\")\n",
    "\n",
    "    past_loss.append(tl)\n",
    "    if len(past_loss) > 5:\n",
    "        if past_loss[-1] < tl and past_loss[-2] < tl:\n",
    "            break_loop = True\n",
    "\n",
    "    if break_loop:\n",
    "        break\n",
    "\n",
    "save_parameters(\"model_params.npz\", Encoder_GRU, Decoder_GRU, eng_embedding, jpn_embedding, o_w, o_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c40f25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_b -= o_b.mean()\n",
    "o_b = np.clip(o_b, -0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c66648f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T02:04:44.320803Z",
     "iopub.status.busy": "2025-08-06T02:04:44.320487Z",
     "iopub.status.idle": "2025-08-06T02:04:44.330475Z",
     "shell.execute_reply": "2025-08-06T02:04:44.329650Z"
    },
    "papermill": {
     "duration": 0.062183,
     "end_time": "2025-08-06T02:04:44.332083",
     "exception": false,
     "start_time": "2025-08-06T02:04:44.269900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(Encoder_GRU, Decoder_GRU, input_seq,\n",
    "            eng_to_ind, eng_embedding,\n",
    "            jpn_embedding, o_w, o_b,\n",
    "            pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
    "            max_len=20, encoder_len=3):\n",
    "    \"\"\"\n",
    "    Greedy decode.\n",
    "\n",
    "    Returns list of predicted token ids excluding <eos>.\n",
    "    \"\"\"\n",
    "    # Preprocess encoder input\n",
    "    import re, numpy as np\n",
    "    seq = re.sub(r\"[^a-z0-9\\s]\", \"\", input_seq.strip().lower())\n",
    "    src_tokens = [eng_to_ind.get(w, unk_id) for w in seq.split()]\n",
    "    if not src_tokens:\n",
    "        src_tokens = [pad_id]\n",
    "    while len(src_tokens) < encoder_len:\n",
    "        src_tokens.append(pad_id)\n",
    "    src_tokens = src_tokens[:encoder_len]\n",
    "\n",
    "    enc_embed = eng_embedding[src_tokens][None, ...]  # (1,T,D)\n",
    "    h_enc, _, _ = Encoder_GRU.forward(enc_embed)      # (1,H)\n",
    "\n",
    "    h = h_enc\n",
    "    prev_token = bos_id\n",
    "    preds = []\n",
    "\n",
    "    for step in range(max_len):\n",
    "        # Embed previous token\n",
    "        y_embed = jpn_embedding[prev_token][None, :]   # (1,D)\n",
    "\n",
    "        r = sigmoid(y_embed @ Decoder_GRU.r_w.T + h @ Decoder_GRU.r_u + Decoder_GRU.r_b)\n",
    "        z = sigmoid(y_embed @ Decoder_GRU.z_w.T + h @ Decoder_GRU.z_u + Decoder_GRU.z_b)\n",
    "        c = tanh(y_embed @ Decoder_GRU.c_w.T + (r * h) @ Decoder_GRU.c_u + Decoder_GRU.c_b)\n",
    "        h = z * h + (1 - z) * c # (1,H)\n",
    "\n",
    "        logits = h @ o_w.T + o_b # (1,V_jpn)\n",
    "\n",
    "\n",
    "        # Prevent <pad> and <bos> from being chosen\n",
    "        logits[0, pad_id] = -1e9\n",
    "        logits[0, bos_id] = -1e9\n",
    "\n",
    "        top_ids = np.argsort(logits[0])[::-1][:5]\n",
    "        print(\"Top logits:\", [(ind_to_jpn[i], logits[0, i]) for i in top_ids])\n",
    "\n",
    "        next_token = int(np.argmax(logits, axis=1)[0])\n",
    "\n",
    "        if next_token == eos_id:\n",
    "            break\n",
    "\n",
    "        preds.append(next_token)\n",
    "        prev_token = next_token\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ed811c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, embedding_dim=64, hidden_size=128):\n",
    "    params = np.load(path)\n",
    "\n",
    "    # Initialize GRUs\n",
    "    Encoder_GRU = GRU(embedding_dim, hidden_size)\n",
    "    Decoder_GRU = GRU(embedding_dim, hidden_size)\n",
    "\n",
    "    # Load Encoder weights\n",
    "    Encoder_GRU.r_w = params[\"enc_r_w\"]\n",
    "    Encoder_GRU.r_u = params[\"enc_r_u\"]\n",
    "    Encoder_GRU.r_b = params[\"enc_r_b\"]\n",
    "    Encoder_GRU.z_w = params[\"enc_z_w\"]\n",
    "    Encoder_GRU.z_u = params[\"enc_z_u\"]\n",
    "    Encoder_GRU.z_b = params[\"enc_z_b\"]\n",
    "    Encoder_GRU.c_w = params[\"enc_c_w\"]\n",
    "    Encoder_GRU.c_u = params[\"enc_c_u\"]\n",
    "    Encoder_GRU.c_b = params[\"enc_c_b\"]\n",
    "\n",
    "    # Load Decoder weights\n",
    "    Decoder_GRU.r_w = params[\"dec_r_w\"]\n",
    "    Decoder_GRU.r_u = params[\"dec_r_u\"]\n",
    "    Decoder_GRU.r_b = params[\"dec_r_b\"]\n",
    "    Decoder_GRU.z_w = params[\"dec_z_w\"]\n",
    "    Decoder_GRU.z_u = params[\"dec_z_u\"]\n",
    "    Decoder_GRU.z_b = params[\"dec_z_b\"]\n",
    "    Decoder_GRU.c_w = params[\"dec_c_w\"]\n",
    "    Decoder_GRU.c_u = params[\"dec_c_u\"]\n",
    "    Decoder_GRU.c_b = params[\"dec_c_b\"]\n",
    "\n",
    "    # Load embeddings and output layer\n",
    "    eng_embedding = params[\"eng_embedding\"]\n",
    "    jpn_embedding = params[\"jpn_embedding\"]\n",
    "    o_w = params[\"o_w\"]\n",
    "    o_b = params[\"o_b\"]\n",
    "\n",
    "    return Encoder_GRU, Decoder_GRU, eng_embedding, jpn_embedding, o_w, o_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bd9a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder_GRU, Decoder_GRU, eng_embedding, jpn_embedding, o_w, o_b = load_model(\"model_params.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59c8f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: [('私', 6.135399110335552), ('誰', 5.002809311125199), ('お', 4.732836240862098), ('あ', 4.620088477773685), ('み', 4.579725875190003)]\n",
      "Top logits: [('は', -1.8372215684955213), ('の', -2.531778242936504), ('、', -4.535960492287662), ('が', -4.607901783496226), ('に', -4.66106014020629)]\n",
      "Top logits: [('い', -16.367272635226357), ('こ', -16.701224937420236), ('あ', -16.754914662428753), ('そ', -16.86499442329881), ('<unk>', -16.89423592026097)]\n",
      "Top logits: [('い', -15.49598147215207), ('つ', -15.51885316259512), ('ま', -15.973045132853732), ('て', -16.573192114639138), ('、', -17.090345633519583)]\n",
      "Top logits: [('で', -16.250543424067963), ('こ', -17.41760090178544), ('の', -17.54004692236131), ('？', -17.743364342384595), ('か', -17.84273000479884)]\n",
      "Top logits: [('す', -18.923711375739764), ('い', -21.57670465976639), ('き', -21.625338628723956), ('も', -21.749334018724884), ('く', -22.053988908431243)]\n",
      "Top logits: [('か', -16.979630681889763), ('。', -18.637426517304085), ('る', -19.348373238774798), ('ね', -20.504414299132815), ('よ', -20.612127986380425)]\n",
      "Top logits: [('。', -15.187136047166002), ('？', -16.779023789250903), ('け', -20.768139532576253), ('！', -20.8201597581134), ('な', -20.998425137002293)]\n",
      "Top logits: [('<eos>', -4.355888455345306), ('？', -13.653428874857523), ('の', -14.003476683730293), ('。', -14.239069521969812), ('よ', -15.157808756934354)]\n",
      "Translation: 私はいいですか。\n"
     ]
    }
   ],
   "source": [
    "sentence = \"come over here\"\n",
    "translation_ids = predict(Encoder_GRU, Decoder_GRU, sentence,\n",
    "                          eng_to_ind, eng_embedding,\n",
    "                          jpn_embedding, o_w, o_b)\n",
    "\n",
    "translation = \"\".join([ind_to_jpn[i] for i in translation_ids])\n",
    "print(\"Translation:\", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e95bdf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters(Encoder_GRU, Decoder_GRU, eng_embedding, jpn_embedding, o_w, o_b):\n",
    "    def print_stats(arr, name):\n",
    "        print(f\"{name:20s} | shape: {arr.shape} | mean: {arr.mean(): .4f} | std: {arr.std(): .4f} | min: {arr.min(): .4f} | max: {arr.max(): .4f}\")\n",
    "\n",
    "    print(\"=== Embeddings ===\")\n",
    "    print_stats(eng_embedding, \"English Embedding\")\n",
    "    print_stats(jpn_embedding, \"Japanese Embedding\")\n",
    "\n",
    "    print(\"\\n=== Encoder GRU Parameters ===\")\n",
    "    for gate_name, w, u, b in [\n",
    "        (\"Reset\", Encoder_GRU.r_w, Encoder_GRU.r_u, Encoder_GRU.r_b),\n",
    "        (\"Update\", Encoder_GRU.z_w, Encoder_GRU.z_u, Encoder_GRU.z_b),\n",
    "        (\"Candidate\", Encoder_GRU.c_w, Encoder_GRU.c_u, Encoder_GRU.c_b),\n",
    "    ]:\n",
    "        print_stats(w, f\"Encoder {gate_name} W\")\n",
    "        print_stats(u, f\"Encoder {gate_name} U\")\n",
    "        print_stats(b, f\"Encoder {gate_name} b\")\n",
    "\n",
    "    print(\"\\n=== Decoder GRU Parameters ===\")\n",
    "    for gate_name, w, u, b in [\n",
    "        (\"Reset\", Decoder_GRU.r_w, Decoder_GRU.r_u, Decoder_GRU.r_b),\n",
    "        (\"Update\", Decoder_GRU.z_w, Decoder_GRU.z_u, Decoder_GRU.z_b),\n",
    "        (\"Candidate\", Decoder_GRU.c_w, Decoder_GRU.c_u, Decoder_GRU.c_b),\n",
    "    ]:\n",
    "        print_stats(w, f\"Decoder {gate_name} W\")\n",
    "        print_stats(u, f\"Decoder {gate_name} U\")\n",
    "        print_stats(b, f\"Decoder {gate_name} b\")\n",
    "\n",
    "    print(\"\\n=== Output Layer ===\")\n",
    "    print_stats(o_w, \"Output W\")\n",
    "    print_stats(o_b, \"Output b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3d0611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Embeddings ===\n",
      "English Embedding    | shape: (971, 64) | mean:  0.0002 | std:  0.1016 | min: -0.5743 | max:  0.6727\n",
      "Japanese Embedding   | shape: (847, 64) | mean: -0.0001 | std:  0.1289 | min: -1.3531 | max:  1.4365\n",
      "\n",
      "=== Encoder GRU Parameters ===\n",
      "Encoder Reset W      | shape: (128, 64) | mean:  0.0014 | std:  0.1023 | min: -0.3825 | max:  0.3872\n",
      "Encoder Reset U      | shape: (128, 128) | mean:  0.0003 | std:  0.1024 | min: -0.4455 | max:  0.4835\n",
      "Encoder Reset b      | shape: (128,) | mean:  0.0231 | std:  0.0253 | min: -0.0043 | max:  0.1761\n",
      "Encoder Update W     | shape: (128, 64) | mean: -0.0008 | std:  0.1022 | min: -0.3903 | max:  0.3410\n",
      "Encoder Update U     | shape: (128, 128) | mean: -0.0009 | std:  0.1021 | min: -0.3743 | max:  0.3469\n",
      "Encoder Update b     | shape: (128,) | mean: -0.0424 | std:  0.0445 | min: -0.3900 | max: -0.0016\n",
      "Encoder Candidate W  | shape: (128, 64) | mean:  0.0012 | std:  0.1129 | min: -0.5538 | max:  0.5459\n",
      "Encoder Candidate U  | shape: (128, 128) | mean:  0.0001 | std:  0.1056 | min: -0.4266 | max:  0.7140\n",
      "Encoder Candidate b  | shape: (128,) | mean:  0.0088 | std:  0.2061 | min: -0.4854 | max:  0.4776\n",
      "\n",
      "=== Decoder GRU Parameters ===\n",
      "Decoder Reset W      | shape: (128, 64) | mean: -0.0015 | std:  0.1152 | min: -0.4747 | max:  0.3993\n",
      "Decoder Reset U      | shape: (128, 128) | mean:  0.0018 | std:  0.1098 | min: -0.4220 | max:  0.4463\n",
      "Decoder Reset b      | shape: (128,) | mean:  0.0495 | std:  0.0242 | min: -0.0080 | max:  0.1160\n",
      "Decoder Update W     | shape: (128, 64) | mean:  0.0028 | std:  0.1075 | min: -0.4585 | max:  0.4124\n",
      "Decoder Update U     | shape: (128, 128) | mean: -0.0051 | std:  0.1181 | min: -0.4891 | max:  0.4780\n",
      "Decoder Update b     | shape: (128,) | mean: -0.0973 | std:  0.0526 | min: -0.2478 | max:  0.1114\n",
      "Decoder Candidate W  | shape: (128, 64) | mean: -0.0001 | std:  0.2245 | min: -1.1907 | max:  1.4994\n",
      "Decoder Candidate U  | shape: (128, 128) | mean:  0.0014 | std:  0.1498 | min: -0.9451 | max:  1.3631\n",
      "Decoder Candidate b  | shape: (128,) | mean:  0.0189 | std:  0.2010 | min: -0.4263 | max:  0.3900\n",
      "\n",
      "=== Output Layer ===\n",
      "Output W             | shape: (847, 128) | mean: -0.0179 | std:  0.3643 | min: -2.3116 | max:  2.1271\n",
      "Output b             | shape: (847,) | mean: -0.3671 | std:  0.0955 | min: -0.4226 | max:  1.8028\n"
     ]
    }
   ],
   "source": [
    "print_model_parameters(Encoder_GRU, Decoder_GRU, eng_embedding, jpn_embedding, o_w, o_b)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7937927,
     "sourceId": 12569836,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7828.683082,
   "end_time": "2025-08-06T02:04:44.914031",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-05T23:54:16.230949",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
