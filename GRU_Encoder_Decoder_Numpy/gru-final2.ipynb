{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef95664",
   "metadata": {
    "papermill": {
     "duration": 0.005736,
     "end_time": "2025-08-06T20:43:27.164997",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.159261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GRU-based Encoder–Decoder for Machine Translation (NumPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ee021",
   "metadata": {
    "papermill": {
     "duration": 0.00505,
     "end_time": "2025-08-06T20:43:27.174500",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.169450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this project, I created a machine translation model using an encoder–decoder architecture built entirely from scratch in NumPy, without using any deep learning frameworks. The model translates from English to Japanese using a GRU (Gated Recurrent Unit).\n",
    "- Dataset: English Japanese sentence pairs selected from the Tatoeba Project\n",
    "- Architecture: Encoder–Decoder with GRU\n",
    "- Objective: Generate Japanese translations using only NumPy operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfa4bd",
   "metadata": {
    "papermill": {
     "duration": 0.005548,
     "end_time": "2025-08-06T20:43:27.188186",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.182638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Architecture Overview\n",
    "- **Embedding**: English and Japanese character tokens are first mapped to integer indices. Each index is embedded into a dense vector using separate embedding matrices.\n",
    "- **Encoder**: A GRU processes the full embedded English input sequence, the final hidden state is returned and represents the context of the entire input sentence. \n",
    "- **Decoder**: The decoder is another GRU that predicts the Japanese output one character at a time. It takes the embedded previous character and the hidden state from the previous step. The initial hidden state is the encoder's final hidden state.\n",
    "- **Output**: Immediately after running the decoder GRU on a time step, apply an FC layer to the hidden state at that time step to get output logits. Store those logits and at the end apply softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08cacb",
   "metadata": {
    "papermill": {
     "duration": 0.004592,
     "end_time": "2025-08-06T20:43:27.198415",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.193823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![Encoder Decoder Architecture](figures/ED_arch.png)\n",
    "\n",
    "Image source: Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. - https://github.com/d2l-ai/d2l-en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9be0b",
   "metadata": {
    "papermill": {
     "duration": 0.003859,
     "end_time": "2025-08-06T20:43:27.206957",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.203098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Loading and Filtering Sentence Pairs\n",
    "\n",
    "Read English Japanese sentence pairs from the `jpn.txt` file. Because training a model from scratch using only NumPy means we can't use a gpu, we need to make the dataset smaller. We only use sentences with 3 or less words. lowercase the English text, and clean both languages using regular expressions. \n",
    "\n",
    "For English, we remove all non-alphanumeric characters. For Japanese, we preserve Hiragana, Katakana, Kanji, and a few key punctuation symbols (like 。 and 、). Duplicate English sentences are also removed to avoid redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b2a780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T20:43:27.219383Z",
     "iopub.status.busy": "2025-08-06T20:43:27.218648Z",
     "iopub.status.idle": "2025-08-06T20:43:27.883027Z",
     "shell.execute_reply": "2025-08-06T20:43:27.882116Z"
    },
    "papermill": {
     "duration": 0.672472,
     "end_time": "2025-08-06T20:43:27.885127",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.212655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentences: ['go', 'hi', 'run', 'who', 'wow', 'duck', 'fire', 'help', 'hide', 'jump']\n",
      "English Sentences Length: 1104\n",
      "Japanese Sentences: ['行け。', 'こんにちは。', '走れ。', '誰？', 'すごい！', '頭を下げろ！', '火事だ！', '助けて！', '隠れろ。', '飛び越えろ！']\n",
      "Japanese Sentences Length: 1104\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Lists for cleaned sentences\n",
    "eng_sentences = []\n",
    "jpn_sentences = []\n",
    "\n",
    "# To avoid duplicates\n",
    "seen = set()\n",
    "\n",
    "with open('/kaggle/input/japanese-english/jpn.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "\n",
    "        # Extract English and Japanese parts\n",
    "        eng = parts[0].strip().lower()\n",
    "        jpn = parts[1].strip().lower()\n",
    "\n",
    "        # Clean English: keep lowercase letters, numbers, and spaces\n",
    "        eng = re.sub(r\"[^a-z0-9\\s]\", \"\", eng)\n",
    "\n",
    "        # Clean Japanese: keep the punctuation\n",
    "        jpn = re.sub(r\"[^\\u3040-\\u30ff\\u4e00-\\u9fff。、！？\\s]\", \"\", jpn)\n",
    "        # Filtering out sentences, max 3 words\n",
    "        if len(eng.split()) <= 2 and len(jpn) <= 10 and eng not in seen:\n",
    "            eng_sentences.append(eng)\n",
    "            jpn_sentences.append(jpn)\n",
    "            seen.add(eng)\n",
    "\n",
    "print(f\"English Sentences: {eng_sentences[0:10]}\")\n",
    "print(f\"English Sentences Length: {len(eng_sentences)}\")\n",
    "print(f\"Japanese Sentences: {jpn_sentences[0:10]}\")\n",
    "print(f\"Japanese Sentences Length: {len(jpn_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1363f50b",
   "metadata": {
    "papermill": {
     "duration": 0.007196,
     "end_time": "2025-08-06T20:43:27.900061",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.892865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vocabulary Construction\n",
    "\n",
    "We now build vocabularies for both English and Japanese.\n",
    "\n",
    "- English is tokenized at word level. We assign indices to all words that appear more than twice.\n",
    "- Japanese is tokenized at the character level, since words are not separated by spaces. Characters appearing more than twice are included in the vocabulary.\n",
    "\n",
    "We also add special tokens:\n",
    "\n",
    "| Token | Meaning         |\n",
    "|-------|------------------|\n",
    "| `<pad>` | Padding token     |\n",
    "| `<unk>` | Unknown token     |\n",
    "| `<bos>` | Beginning of sequence |\n",
    "| `<eos>` | End of sequence     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4fc23b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T20:43:27.910599Z",
     "iopub.status.busy": "2025-08-06T20:43:27.910168Z",
     "iopub.status.idle": "2025-08-06T20:43:27.929613Z",
     "shell.execute_reply": "2025-08-06T20:43:27.928472Z"
    },
    "papermill": {
     "duration": 0.027021,
     "end_time": "2025-08-06T20:43:27.931627",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.904606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 298\n",
      "Japanese Vocabulary Size: 427\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "# Count word frequency\n",
    "for sent in eng_sentences:\n",
    "    counter.update(sent.split())\n",
    "\n",
    "# English to index mapping\n",
    "eng_to_ind = {'<pad>': 0, '<unk>':1}\n",
    "ind = 2\n",
    "\n",
    "# Include words that appear 2+ times.\n",
    "for word in counter:\n",
    "    if counter[word] > 1:\n",
    "        eng_to_ind[word] = ind\n",
    "        ind += 1\n",
    "        \n",
    "print(\"English Vocabulary Size:\", len(eng_to_ind))\n",
    "\n",
    "jcounter = Counter()\n",
    "\n",
    "# Count character frequency across Japanese sentences\n",
    "for sent in jpn_sentences:\n",
    "    jcounter.update(list(sent))\n",
    "\n",
    "# Japanese to index and index to Japanese mapping\n",
    "jpn_to_ind = {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3}\n",
    "ind_to_jpn = {0: '<pad>', 1: '<unk>', 2: '<bos>', 3: '<eos>'}\n",
    "ind = 4\n",
    "\n",
    "# Include characters that appear 2+ times\n",
    "for char, freq in jcounter.most_common():\n",
    "    if freq > 1:\n",
    "        jpn_to_ind[char] = ind\n",
    "        ind_to_jpn[ind] = char\n",
    "        ind += 1\n",
    "\n",
    "print(\"Japanese Vocabulary Size:\", len(jpn_to_ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a224e",
   "metadata": {
    "papermill": {
     "duration": 0.004182,
     "end_time": "2025-08-06T20:43:27.940487",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.936305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Encoding Sentences\n",
    "\n",
    "Now convert each sentence into a sequence of integer indices.\n",
    "\n",
    "- English Encoding: \n",
    "  - Tokenized at the word level.\n",
    "  - Each sentence is padded to 3 tokens using the `<pad>` (index 0).\n",
    "  - Unknown words are mapped to `<unk>` (index 1).\n",
    "\n",
    "- Japanese Encoding:\n",
    "  - Tokenized at the character level.\n",
    "  - Each sentence starts with `<bos>` (index 2) and ends with `<eos>` (index 3).\n",
    "  - Limit to 12 tokens total to include both `<bos>` and `<eos>`.\n",
    "  - Padding (`<pad>`) is added at the end if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "834aec95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T20:43:27.950596Z",
     "iopub.status.busy": "2025-08-06T20:43:27.950164Z",
     "iopub.status.idle": "2025-08-06T20:43:27.968692Z",
     "shell.execute_reply": "2025-08-06T20:43:27.967606Z"
    },
    "papermill": {
     "duration": 0.025657,
     "end_time": "2025-08-06T20:43:27.970429",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.944772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentences Encoded: [[2, 0], [3, 0], [4, 0], [5, 0], [1, 0], [1, 0], [6, 0], [7, 0], [8, 0], [9, 0]]\n",
      "English Sentences Encoded Length: 1104\n",
      "Japanese Sentences Encoded: [[2, 58, 40, 4, 3, 0, 0, 0, 0, 0], [2, 31, 25, 27, 38, 6, 4, 3, 0, 0], [2, 109, 20, 4, 3, 0, 0, 0, 0, 0], [2, 50, 42, 3, 0, 0, 0, 0, 0, 0], [2, 12, 66, 5, 18, 3, 0, 0, 0, 0]]\n",
      "Japanese Sentences Encoded Length: 1104\n"
     ]
    }
   ],
   "source": [
    "eng_encoded = []\n",
    "\n",
    "# Encode each English sentence at word level\n",
    "# Maximum of 3 words per sentence\n",
    "for sentence in eng_sentences:\n",
    "    s = []\n",
    "    for word in sentence.split():\n",
    "        if word in eng_to_ind:\n",
    "            s.append(eng_to_ind[word])\n",
    "        else:\n",
    "            s.append(1)\n",
    "    while(len(s) < 2):\n",
    "        s.append(0) # Padding\n",
    "    eng_encoded.append(s)\n",
    "\n",
    "jpn_encoded = []\n",
    "\n",
    "# Encode each Japanese sentence character level\n",
    "# Maximum of twelve characters per sentence including <bos> and <eos>\n",
    "for sentence in jpn_sentences:\n",
    "    s = [2] # 2 is <bos>\n",
    "    for ch in sentence:\n",
    "        if len(s) > 8:\n",
    "            break # We need room to fit <eos> token.\n",
    "        if ch in jpn_to_ind:\n",
    "            s.append(jpn_to_ind[ch])\n",
    "        else:\n",
    "            s.append(1)   # 1 is <unk>\n",
    "    s.append(3)           # 3 is <eos>\n",
    "    while(len(s) < 10):\n",
    "        s.append(0)       # 0 is <pad>\n",
    "    jpn_encoded.append(s)\n",
    "\n",
    "print(f\"English Sentences Encoded: {eng_encoded[0:10]}\")\n",
    "print(f\"English Sentences Encoded Length: {len(eng_encoded)}\")\n",
    "print(f\"Japanese Sentences Encoded: {jpn_encoded[0:5]}\")\n",
    "print(f\"Japanese Sentences Encoded Length: {len(jpn_encoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf0f9f",
   "metadata": {
    "papermill": {
     "duration": 0.004145,
     "end_time": "2025-08-06T20:43:27.980201",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.976056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating Embedding Matrices\n",
    "\n",
    "- Vocabulary Size: Approximately 900 for each language.\n",
    "- Embedding Dimension: I estimated using int(min(600, 1.6 * vocab_size ** 0.56)) = 72  \n",
    "  I wanted to use a power of 2, so I rounded down to 64.\n",
    "\n",
    "Each word or character will be embedded into a 64 dimensional dense vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff4a6b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T20:43:27.990567Z",
     "iopub.status.busy": "2025-08-06T20:43:27.990243Z",
     "iopub.status.idle": "2025-08-06T20:43:27.999059Z",
     "shell.execute_reply": "2025-08-06T20:43:27.997771Z"
    },
    "papermill": {
     "duration": 0.01627,
     "end_time": "2025-08-06T20:43:28.000943",
     "exception": false,
     "start_time": "2025-08-06T20:43:27.984673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English embedding shape: (298, 64)\n",
      "Japanese embedding shape: (427, 64)\n",
      "English input shape: (3, 64)\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 64\n",
    "V_eng = len(eng_to_ind)  # English vocab size\n",
    "V_jpn = len(jpn_to_ind)  # Japanese vocab size\n",
    "\n",
    "eng_embedding = np.random.randn(V_eng, embedding_size) * 0.1\n",
    "jpn_embedding = np.random.randn(V_jpn, embedding_size) * 0.1\n",
    "\n",
    "print(f\"English embedding shape: {eng_embedding.shape}\")\n",
    "print(f\"Japanese embedding shape: {jpn_embedding.shape}\")\n",
    "\n",
    "# Example\n",
    "eng_sentence = [5, 23, 8]\n",
    "embedded_sentence = eng_embedding[eng_sentence]  # Shape (3, 64)\n",
    "print(f\"English input shape: {embedded_sentence.shape}\")  # (seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f6c54",
   "metadata": {
    "papermill": {
     "duration": 0.00748,
     "end_time": "2025-08-06T20:43:28.016494",
     "exception": false,
     "start_time": "2025-08-06T20:43:28.009014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Activation and Loss Functions\n",
    "\n",
    "- Sigmoid: Used in GRU gates.\n",
    "- Tanh: Used for the candidate hidden state.\n",
    "- Softmax: Converts decoder logits into probabilities.\n",
    "- Cross Entropy Loss: Measures prediction error between predicted softmax outputs and true labels.\n",
    "\n",
    "Documentation generated by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1d3e80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T20:43:28.030059Z",
     "iopub.status.busy": "2025-08-06T20:43:28.029641Z",
     "iopub.status.idle": "2025-08-06T20:43:28.042562Z",
     "shell.execute_reply": "2025-08-06T20:43:28.041302Z"
    },
    "papermill": {
     "duration": 0.020335,
     "end_time": "2025-08-06T20:43:28.044325",
     "exception": false,
     "start_time": "2025-08-06T20:43:28.023990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Element-wise Sigmoid activation function.\n",
    "    Used in GRU for update/reset gates.\n",
    "    \n",
    "    Clips input to avoid overflow in exp.\n",
    "    \"\"\"\n",
    "    x = np.clip(x, -50, 50)  # Prevent overflow\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    \"\"\"\n",
    "    Derivative of the Sigmoid function.\n",
    "    Used during GRU backpropagation.\n",
    "    \"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Element-wise hyperbolic tangent activation.\n",
    "    Used in GRU for candidate hidden state.\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    \"\"\"\n",
    "    Derivative of tanh function.\n",
    "    Used in GRU backpropagation.\n",
    "    \"\"\"\n",
    "    return 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "def softmax(Z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply softmax over last dimension (class logits).\n",
    "    \n",
    "    Parameters:\n",
    "    - Z: np.ndarray of shape (batch_size, num_classes)\n",
    "      Raw logits for classification.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray of same shape: softmax probabilities.\n",
    "    \"\"\"\n",
    "    Z = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    exp_Z = np.exp(Z)\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "def CrossEntropy(yhat: np.ndarray, y: np.ndarray, eps: float = 1e-15) -> float:\n",
    "    \"\"\"\n",
    "    Compute mean cross-entropy loss.\n",
    "\n",
    "    Parameters:\n",
    "    - yhat: np.ndarray of shape (batch_size, num_classes)\n",
    "        Predicted probabilities (after softmax)\n",
    "    - y: np.ndarray of shape (batch_size,)\n",
    "        True class indices\n",
    "    - eps: float\n",
    "        Small constant to avoid log(0)\n",
    "\n",
    "    Returns:\n",
    "    - float: mean loss across the batch\n",
    "    \"\"\"\n",
    "    yhat = np.clip(yhat, eps, 1 - eps)  # Prevent log(0)\n",
    "    correct_probs = yhat[np.arange(len(y)), y]\n",
    "    return -np.mean(np.log(correct_probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a810ea8",
   "metadata": {
    "papermill": {
     "duration": 0.004282,
     "end_time": "2025-08-06T20:43:28.053186",
     "exception": false,
     "start_time": "2025-08-06T20:43:28.048904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GRU Architecture and Implementation\n",
    "\n",
    "The Gated Recurrent Unit (GRU) is an RNN cell designed to assist with the vanishing/exploding gradient problem and capture long range dependencies more efficiently than a vanilla RNN. Below is a overview of the implementation.\n",
    "\n",
    "- Reset Gate (R): Determines how much of the past hidden state to forget.\n",
    "- Update Gate (Z): Determines how much of the new candidate state to use versus retaining the past hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd6261",
   "metadata": {
    "papermill": {
     "duration": 0.004213,
     "end_time": "2025-08-06T20:43:28.061890",
     "exception": false,
     "start_time": "2025-08-06T20:43:28.057677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GRU Architecture and Implementation\n",
    "\n",
    "The Gated Recurrent Unit (GRU) is an RNN cell designed to assist with the vanishing/exploding gradient problem and capture long range dependencies more efficiently than a vanilla RNN. Below is a overview of the implementation.\n",
    "\n",
    "- Reset Gate (R): Determines how much of the past hidden state to forget.\n",
    "- Update Gate (Z): Determines how much of the new candidate state to use versus retaining the past hidden state.\n",
    "Rₜ = σ(xₜ @ W_r.T + hₜ₋₁ @ U_r + b_r)\n",
    "### Forward Pass (per time step \\(t\\))\n",
    "1. Compute gates\n",
    "    - Rₜ = σ(xₜ @ W_r.T + hₜ₋₁ @ U_r + b_r)\n",
    "    - Zₜ = σ(xₜ @ W_z.T + hₜ₋₁ @ U_z + b_z)\n",
    "2. Candidate state \n",
    "    - Cₜ = tanh(xₜ @ W_c.T + (Rₜ ⊙ hₜ₋₁) @ U_c + b_c)\n",
    "3. Hidden state update\n",
    "    - xₜ = zₜ ⊙ hₜ₋₁ + (1 - Zₜ) ⊙ Cₜ\n",
    "\n",
    "Store (xₜ, hₜ₋₁, Rₜ, Zₜ, Cₜ, hₜ) at each step to use during backpropagation.\n",
    "\n",
    "![GRU Architecture](figures/GRU_arch.png)\n",
    "\n",
    "### Gradient Calculations\n",
    "\n",
    "Calculating the gradients is a tedious process, below are my calculations to get this model to work. There were a lot of problems.\n",
    "\n",
    "![Backpropagation](figures/Backprop.png)\n",
    "\n",
    "### Integration into Encoder–Decoder\n",
    "- Encoder GRU: Processes the input sequence and returns the final hidden state \\(h_{enc}\\).  \n",
    "- Decoder GRU: Initialized with \\(h_{enc}\\), generates the target sequence by feeding its own previous output back as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "156dc4bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T20:43:28.073039Z",
     "iopub.status.busy": "2025-08-06T20:43:28.072594Z",
     "iopub.status.idle": "2025-08-06T20:43:28.098091Z",
     "shell.execute_reply": "2025-08-06T20:43:28.097138Z"
    },
    "papermill": {
     "duration": 0.033517,
     "end_time": "2025-08-06T20:43:28.099864",
     "exception": false,
     "start_time": "2025-08-06T20:43:28.066347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Xavier Normal std\n",
    "        std = np.sqrt(2 / (input_size + hidden_size))\n",
    "\n",
    "        # Reset gate (R)\n",
    "        self.r_w = np.random.normal(0, std, size=(hidden_size, input_size))\n",
    "        self.r_u = np.random.normal(0, std, size=(hidden_size, hidden_size))\n",
    "        self.r_b = np.zeros(hidden_size)\n",
    "\n",
    "        # Update gate (Z)\n",
    "        self.z_w = np.random.normal(0, std, size=(hidden_size, input_size))\n",
    "        self.z_u = np.random.normal(0, std, size=(hidden_size, hidden_size))\n",
    "        self.z_b = np.zeros(hidden_size)\n",
    "\n",
    "        # Candidate hidden state (C)\n",
    "        self.c_w = np.random.normal(0, std, size=(hidden_size, input_size))\n",
    "        self.c_u = np.random.normal(0, std, size=(hidden_size, hidden_size))\n",
    "        self.c_b = np.zeros(hidden_size)\n",
    "    \n",
    "    def forward(self, X, H=None):\n",
    "        batch_size, seq_len, embed_dim = X.shape # (N, T, D)\n",
    "        if H is None:\n",
    "            H = np.zeros(shape=(batch_size, self.hidden_size)) # (N, H)\n",
    "\n",
    "        hidden_states = []\n",
    "        store = [] # For backpropagation\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            x_t = X[:, i, :]  # (N, D)\n",
    "            R = sigmoid(x_t @ self.r_w.T + H @ self.r_u + self.r_b)  # (N, H)\n",
    "            Z = sigmoid(x_t @ self.z_w.T + H @ self.z_u + self.z_b)  # (N, H)\n",
    "            C = tanh(x_t @ self.c_w.T + (R * H) @ self.c_u + self.c_b)  # (N, H)\n",
    "\n",
    "            H_new = Z * H + (1 - Z) * C  # (N, H)\n",
    "\n",
    "            store.append((x_t, H, R, Z, C, H_new))\n",
    "            H = H_new\n",
    "            hidden_states.append(H[:, np.newaxis, :])\n",
    "            \n",
    "\n",
    "        return H, np.concatenate(hidden_states, axis=1), store\n",
    "\n",
    "    def backward(self, dH_all, store, lr):\n",
    "\n",
    "        batch, seq_len, _ = dH_all.shape\n",
    "\n",
    "        # Initialize accumulators for parameter gradients to zeros\n",
    "        dr_w = np.zeros_like(self.r_w)  # ∂L/∂R_w, shape (H, D)\n",
    "        dr_u = np.zeros_like(self.r_u)  # ∂L/∂R_u, shape (H, H)\n",
    "        dr_b = np.zeros_like(self.r_b)  # ∂L/∂R_b,  shape (H,)\n",
    "\n",
    "        dz_w = np.zeros_like(self.z_w)  # ∂L/∂Z_w\n",
    "        dz_u = np.zeros_like(self.z_u)  # ∂L/∂Z_u\n",
    "        dz_b = np.zeros_like(self.z_b)  # ∂L/∂Z_b\n",
    "\n",
    "        dc_w = np.zeros_like(self.c_w)  # ∂L/∂C_w\n",
    "        dc_u = np.zeros_like(self.c_u)  # ∂L/∂C_u\n",
    "        dc_b = np.zeros_like(self.c_b)  # ∂L/∂C_b\n",
    "\n",
    "        # dh accumulates the gradient flowing from future time steps\n",
    "        dH = np.zeros((batch, self.hidden_size)) # (N, H)\n",
    "        \n",
    "        # Will hold gradient w.r.t. the input embeddings at each time step\n",
    "        dx = np.zeros((batch, seq_len, self.input_size))  # (N, T, D)\n",
    "\n",
    "        # Loop backwards through time: t = seq_len-1 ... 0\n",
    "        for t in reversed(range(seq_len)):\n",
    "            # Unpack stored values from forward pass\n",
    "            # x_t:   (N, D) input at time t\n",
    "            # h_prev:(N, H) previous hidden state\n",
    "            # r_t:   (N, H) reset gate\n",
    "            # z_t:   (N, H) update gate\n",
    "            # c_t:   (N, H) candidate hidden state\n",
    "            # h_t:   (N, H) final hidden state at time t\n",
    "            x_t, h_prev, r_t, z_t, c_t, h_t = store[t]\n",
    "\n",
    "            # dh: (N, H) total gradient wrt current hidden state h_t\n",
    "            # Includes gradient from output loss and from next time step dH\n",
    "            dh = dH_all[:, t, :] + dH\n",
    "            \n",
    "            # Gradients through update gate z_t and candidate c_t\n",
    "            # ∂L/∂z_t = dh ⊙ (h_prev - c_t)\n",
    "            dz = dh * (h_prev - c_t) # (N, H)\n",
    "            # ∂L/∂c_t = dh ⊙ (1 - z_t)\n",
    "            dc = dh * (1 - z_t)      # (N, H)\n",
    "            # ∂L/∂h_prev = dh ⊙ z_t, one part of ∂L/∂h_{t-1}\n",
    "            dh_prev = dh * z_t       # (N, H)\n",
    "\n",
    "            # Backprop through tanh: ∂L/∂c_t_preact = dc ⊙ (1 - c_t²)\n",
    "            dc_preact = dc * (1 - c_t**2)   # (N, H)\n",
    "            \n",
    "            # Accumulate parameter gradients for candidate weights\n",
    "            # ∂L/∂C_w = dc_preact.T @ x_t\n",
    "            dc_w += dc_preact.T @ x_t              # (H, D)\n",
    "            # ∂L/∂C_u = (r_t ⊙ h_prev).T @ dc_preact\n",
    "            dc_u += (r_t * h_prev).T @ dc_preact   # (H, H)\n",
    "            # ∂L/∂C_b = sum over batch of dc_preact\n",
    "            dc_b += np.sum(dc_preact, axis=0)      # (H,)\n",
    "\n",
    "            # Reset gate gradient from C preact's hidden side\n",
    "            # ∂L/∂r_t = ∂L/(∂C preact) ⊙ ∂(C preact)/∂r_t\n",
    "            # ∂L/∂r_t = (dc_preact @ C_u.T) ⊙ h_prev\n",
    "            dr = (dc_preact @ self.c_u.T) * h_prev   # (N, H)\n",
    "            # Backprop through sigmoid: ∂L/∂r_preact = dr ⊙ r_t ⊙ (1 - r_t)\n",
    "            dr_preact = dr * (r_t * (1 - r_t))       # (N, H)\n",
    "            # Accumulate Reset gate gradients\n",
    "            dr_w += dr_preact.T @ x_t                # (H, D)\n",
    "            dr_u += h_prev.T @ dr_preact             # (H, H)\n",
    "            dr_b += np.sum(dr_preact, axis=0)        # (H,)\n",
    "\n",
    "            # Backprop through update gate sigmoid\n",
    "            dz_preact = dz * z_t * (1 - z_t)         # (N, H)\n",
    "\n",
    "            # Accumulate update gate gradients\n",
    "            dz_w += dz_preact.T @ x_t                # (H, D)\n",
    "            dz_u += h_prev.T @ dz_preact             # (H, H)\n",
    "            dz_b += np.sum(dz_preact, axis=0)        # (H,)\n",
    "\n",
    "            # Add to total gradient wrt h_prev\n",
    "            # dh_prev = sum of all partials wrt h_prev from C, R, Z\n",
    "            dh_prev += (dc_preact @ self.c_u.T) * r_t        # (N, H)\n",
    "            dh_prev += dr_preact @ self.r_u.T                # (N, H)\n",
    "            dh_prev += dz_preact @ self.z_u.T                # (N, H)\n",
    "            \n",
    "\n",
    "            # Set dH for next iteration (t-1)\n",
    "            dH = dh_prev  # (N, H)\n",
    "\n",
    "            # Gradient wrt. input x_t combines contributions from each gate\n",
    "            # Paths: dr_preact, dz_preact, dc_preact\n",
    "            dx[:, t, :] = (\n",
    "                dr_preact @ self.r_w +  # via reset gate input weights\n",
    "                dz_preact @ self.z_w +  # via update gate input weights\n",
    "                dc_preact @ self.c_w    # via candidate input weights\n",
    "            )  # shape (N, D)\n",
    "\n",
    "\n",
    "        grads = [dc_w, dc_u, dc_b, dr_w, dr_u, dr_b, dz_w, dz_u, dz_b]\n",
    "\n",
    "        for i in range(len(grads)):\n",
    "            np.clip(grads[i], -1.0, 1.0, out=grads[i])  # Gradient Clipping\n",
    "\n",
    "        self.c_w -= lr * dc_w\n",
    "        self.c_u -= lr * dc_u\n",
    "        self.c_b -= lr * dc_b\n",
    "\n",
    "        self.r_w -= lr * dr_w\n",
    "        self.r_u -= lr * dr_u\n",
    "        self.r_b -= lr * dr_b\n",
    "\n",
    "        self.z_w -= lr * dz_w\n",
    "        self.z_u -= lr * dz_u\n",
    "        self.z_b -= lr * dz_b\n",
    "\n",
    "        return dH, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953298bf",
   "metadata": {
    "papermill": {
     "duration": 0.007129,
     "end_time": "2025-08-06T20:43:28.114927",
     "exception": false,
     "start_time": "2025-08-06T20:43:28.107798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Saving parameters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce8e7107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T20:43:28.125748Z",
     "iopub.status.busy": "2025-08-06T20:43:28.125346Z",
     "iopub.status.idle": "2025-08-06T20:43:28.134296Z",
     "shell.execute_reply": "2025-08-06T20:43:28.133385Z"
    },
    "papermill": {
     "duration": 0.016238,
     "end_time": "2025-08-06T20:43:28.135725",
     "exception": false,
     "start_time": "2025-08-06T20:43:28.119487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_parameters(path, Encoder_GRU, Decoder_GRU, eng_embedding, jpn_embedding, o_w, o_b):\n",
    "\n",
    "    np.savez(\n",
    "        path,\n",
    "        eng_embedding=eng_embedding,\n",
    "        jpn_embedding=jpn_embedding,\n",
    "        o_w=o_w, o_b=o_b,\n",
    "        enc_r_w=Encoder_GRU.r_w, enc_r_u=Encoder_GRU.r_u, enc_r_b=Encoder_GRU.r_b,\n",
    "        enc_z_w=Encoder_GRU.z_w, enc_z_u=Encoder_GRU.z_u, enc_z_b=Encoder_GRU.z_b,\n",
    "        enc_c_w=Encoder_GRU.c_w, enc_c_u=Encoder_GRU.c_u, enc_c_b=Encoder_GRU.c_b,\n",
    "        dec_r_w=Decoder_GRU.r_w, dec_r_u=Decoder_GRU.r_u, dec_r_b=Decoder_GRU.r_b,\n",
    "        dec_z_w=Decoder_GRU.z_w, dec_z_u=Decoder_GRU.z_u, dec_z_b=Decoder_GRU.z_b,\n",
    "        dec_c_w=Decoder_GRU.c_w, dec_c_u=Decoder_GRU.c_u, dec_c_b=Decoder_GRU.c_b\n",
    "    )\n",
    "    print(\"Saved Parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c496af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T20:43:28.146609Z",
     "iopub.status.busy": "2025-08-06T20:43:28.146225Z",
     "iopub.status.idle": "2025-08-06T20:47:45.163935Z",
     "shell.execute_reply": "2025-08-06T20:47:45.162778Z"
    },
    "papermill": {
     "duration": 257.025173,
     "end_time": "2025-08-06T20:47:45.165560",
     "exception": false,
     "start_time": "2025-08-06T20:43:28.140387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 6.0446\n",
      "Epoch 2 | Loss: 6.0208\n",
      "Epoch 3 | Loss: 5.9969\n",
      "Epoch 4 | Loss: 5.9731\n",
      "Epoch 5 | Loss: 5.9488\n",
      "Epoch 6 | Loss: 5.9243\n",
      "Epoch 7 | Loss: 5.8993\n",
      "Epoch 8 | Loss: 5.8739\n",
      "Epoch 9 | Loss: 5.8474\n",
      "Epoch 10 | Loss: 5.8200\n",
      "Epoch 11 | Loss: 5.7909\n",
      "Epoch 12 | Loss: 5.7607\n",
      "Epoch 13 | Loss: 5.7283\n",
      "Epoch 14 | Loss: 5.6925\n",
      "Epoch 15 | Loss: 5.6537\n",
      "Epoch 16 | Loss: 5.6104\n",
      "Epoch 17 | Loss: 5.5608\n",
      "Epoch 18 | Loss: 5.5043\n",
      "Epoch 19 | Loss: 5.4388\n",
      "Epoch 20 | Loss: 5.3610\n",
      "Epoch 21 | Loss: 5.2701\n",
      "Epoch 22 | Loss: 5.1653\n",
      "Epoch 23 | Loss: 5.0509\n",
      "Epoch 24 | Loss: 4.9362\n",
      "Epoch 25 | Loss: 4.8313\n",
      "Epoch 26 | Loss: 4.7440\n",
      "Epoch 27 | Loss: 4.6680\n",
      "Epoch 28 | Loss: 4.6035\n",
      "Epoch 29 | Loss: 4.5426\n",
      "Epoch 30 | Loss: 4.4875\n",
      "Epoch 31 | Loss: 4.4397\n",
      "Epoch 32 | Loss: 4.3937\n",
      "Epoch 33 | Loss: 4.3498\n",
      "Epoch 34 | Loss: 4.3088\n",
      "Epoch 35 | Loss: 4.2714\n",
      "Epoch 36 | Loss: 4.2375\n",
      "Epoch 37 | Loss: 4.2090\n",
      "Epoch 38 | Loss: 4.1794\n",
      "Epoch 39 | Loss: 4.1508\n",
      "Epoch 40 | Loss: 4.1254\n",
      "Epoch 41 | Loss: 4.1035\n",
      "Epoch 42 | Loss: 4.0817\n",
      "Epoch 43 | Loss: 4.0602\n",
      "Epoch 44 | Loss: 4.0382\n",
      "Epoch 45 | Loss: 4.0183\n",
      "Epoch 46 | Loss: 3.9997\n",
      "Epoch 47 | Loss: 3.9777\n",
      "Epoch 48 | Loss: 3.9592\n",
      "Epoch 49 | Loss: 3.9414\n",
      "Epoch 50 | Loss: 3.9250\n",
      "Epoch 51 | Loss: 3.9081\n",
      "Epoch 52 | Loss: 3.8919\n",
      "Epoch 53 | Loss: 3.8743\n",
      "Epoch 54 | Loss: 3.8574\n",
      "Epoch 55 | Loss: 3.8405\n",
      "Epoch 56 | Loss: 3.8253\n",
      "Epoch 57 | Loss: 3.8116\n",
      "Epoch 58 | Loss: 3.7965\n",
      "Epoch 59 | Loss: 3.7816\n",
      "Epoch 60 | Loss: 3.7676\n",
      "Epoch 61 | Loss: 3.7553\n",
      "Epoch 62 | Loss: 3.7421\n",
      "Epoch 63 | Loss: 3.7299\n",
      "Epoch 64 | Loss: 3.7171\n",
      "Epoch 65 | Loss: 3.7062\n",
      "Epoch 66 | Loss: 3.6950\n",
      "Epoch 67 | Loss: 3.6859\n",
      "Epoch 68 | Loss: 3.6739\n",
      "Epoch 69 | Loss: 3.6643\n",
      "Epoch 70 | Loss: 3.6567\n",
      "Epoch 71 | Loss: 3.6487\n",
      "Epoch 72 | Loss: 3.6366\n",
      "Epoch 73 | Loss: 3.6310\n",
      "Epoch 74 | Loss: 3.6201\n",
      "Epoch 75 | Loss: 3.6127\n",
      "Epoch 76 | Loss: 3.6068\n",
      "Epoch 77 | Loss: 3.5973\n",
      "Epoch 78 | Loss: 3.5910\n",
      "Epoch 79 | Loss: 3.5835\n",
      "Epoch 80 | Loss: 3.5784\n",
      "Epoch 81 | Loss: 3.5709\n",
      "Epoch 82 | Loss: 3.5648\n",
      "Epoch 83 | Loss: 3.5597\n",
      "Epoch 84 | Loss: 3.5545\n",
      "Epoch 85 | Loss: 3.5466\n",
      "Epoch 86 | Loss: 3.5414\n",
      "Epoch 87 | Loss: 3.5359\n",
      "Epoch 88 | Loss: 3.5290\n",
      "Epoch 89 | Loss: 3.5254\n",
      "Epoch 90 | Loss: 3.5202\n",
      "Epoch 91 | Loss: 3.5167\n",
      "Epoch 92 | Loss: 3.5058\n",
      "Epoch 93 | Loss: 3.5034\n",
      "Epoch 94 | Loss: 3.4982\n",
      "Epoch 95 | Loss: 3.4951\n",
      "Epoch 96 | Loss: 3.4911\n",
      "Epoch 97 | Loss: 3.4869\n",
      "Epoch 98 | Loss: 3.4832\n",
      "Epoch 99 | Loss: 3.4766\n",
      "Epoch 100 | Loss: 3.4743\n",
      "Epoch 101 | Loss: 3.4696\n",
      "Epoch 102 | Loss: 3.4664\n",
      "Epoch 103 | Loss: 3.4607\n",
      "Epoch 104 | Loss: 3.4592\n",
      "Epoch 105 | Loss: 3.4540\n",
      "Epoch 106 | Loss: 3.4503\n",
      "Epoch 107 | Loss: 3.4463\n",
      "Epoch 108 | Loss: 3.4426\n",
      "Epoch 109 | Loss: 3.4426\n",
      "Epoch 110 | Loss: 3.4389\n",
      "Epoch 111 | Loss: 3.4361\n",
      "Epoch 112 | Loss: 3.4314\n",
      "Epoch 113 | Loss: 3.4279\n",
      "Epoch 114 | Loss: 3.4268\n",
      "Epoch 115 | Loss: 3.4242\n",
      "Epoch 116 | Loss: 3.4217\n",
      "Epoch 117 | Loss: 3.4173\n",
      "Epoch 118 | Loss: 3.4157\n",
      "Epoch 119 | Loss: 3.4107\n",
      "Epoch 120 | Loss: 3.4100\n",
      "Epoch 121 | Loss: 3.4070\n",
      "Epoch 122 | Loss: 3.4056\n",
      "Epoch 123 | Loss: 3.4012\n",
      "Epoch 124 | Loss: 3.3995\n",
      "Epoch 125 | Loss: 3.3944\n",
      "Epoch 126 | Loss: 3.3951\n",
      "Epoch 127 | Loss: 3.3915\n",
      "Epoch 128 | Loss: 3.3880\n",
      "Epoch 129 | Loss: 3.3844\n",
      "Epoch 130 | Loss: 3.3841\n",
      "Epoch 131 | Loss: 3.3834\n",
      "Epoch 132 | Loss: 3.3795\n",
      "Epoch 133 | Loss: 3.3796\n",
      "Epoch 134 | Loss: 3.3767\n",
      "Epoch 135 | Loss: 3.3723\n",
      "Epoch 136 | Loss: 3.3707\n",
      "Epoch 137 | Loss: 3.3681\n",
      "Epoch 138 | Loss: 3.3679\n",
      "Epoch 139 | Loss: 3.3665\n",
      "Epoch 140 | Loss: 3.3617\n",
      "Epoch 141 | Loss: 3.3623\n",
      "Epoch 142 | Loss: 3.3590\n",
      "Epoch 143 | Loss: 3.3581\n",
      "Epoch 144 | Loss: 3.3567\n",
      "Epoch 145 | Loss: 3.3522\n",
      "Epoch 146 | Loss: 3.3507\n",
      "Epoch 147 | Loss: 3.3505\n",
      "Epoch 148 | Loss: 3.3470\n",
      "Epoch 149 | Loss: 3.3463\n",
      "Epoch 150 | Loss: 3.3439\n",
      "Epoch 151 | Loss: 3.3408\n",
      "Epoch 152 | Loss: 3.3397\n",
      "Epoch 153 | Loss: 3.3369\n",
      "Epoch 154 | Loss: 3.3385\n",
      "Epoch 155 | Loss: 3.3378\n",
      "Epoch 156 | Loss: 3.3329\n",
      "Epoch 157 | Loss: 3.3296\n",
      "Epoch 158 | Loss: 3.3305\n",
      "Epoch 159 | Loss: 3.3280\n",
      "Epoch 160 | Loss: 3.3260\n",
      "Epoch 161 | Loss: 3.3235\n",
      "Epoch 162 | Loss: 3.3223\n",
      "Epoch 163 | Loss: 3.3220\n",
      "Epoch 164 | Loss: 3.3196\n",
      "Epoch 165 | Loss: 3.3189\n",
      "Epoch 166 | Loss: 3.3170\n",
      "Epoch 167 | Loss: 3.3168\n",
      "Epoch 168 | Loss: 3.3134\n",
      "Epoch 169 | Loss: 3.3115\n",
      "Epoch 170 | Loss: 3.3097\n",
      "Epoch 171 | Loss: 3.3093\n",
      "Epoch 172 | Loss: 3.3084\n",
      "Epoch 173 | Loss: 3.3060\n",
      "Epoch 174 | Loss: 3.3039\n",
      "Epoch 175 | Loss: 3.3020\n",
      "Epoch 176 | Loss: 3.2993\n",
      "Epoch 177 | Loss: 3.2993\n",
      "Epoch 178 | Loss: 3.2963\n",
      "Epoch 179 | Loss: 3.2981\n",
      "Epoch 180 | Loss: 3.2950\n",
      "Epoch 181 | Loss: 3.2929\n",
      "Epoch 182 | Loss: 3.2916\n",
      "Epoch 183 | Loss: 3.2901\n",
      "Epoch 184 | Loss: 3.2883\n",
      "Epoch 185 | Loss: 3.2879\n",
      "Epoch 186 | Loss: 3.2858\n",
      "Epoch 187 | Loss: 3.2845\n",
      "Epoch 188 | Loss: 3.2820\n",
      "Epoch 189 | Loss: 3.2772\n",
      "Epoch 190 | Loss: 3.2781\n",
      "Epoch 191 | Loss: 3.2788\n",
      "Saved Parameters\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "epochs = 3000\n",
    "lr = 0.002\n",
    "encoder_len = 2        # English length\n",
    "target_len = 10        # Japanese length\n",
    "hidden_size = 64\n",
    "\n",
    "bos_id, pad_id, eos_id = 2, 0, 3\n",
    "\n",
    "Encoder_GRU = GRU(64, hidden_size)\n",
    "Decoder_GRU = GRU(64, hidden_size)\n",
    "\n",
    "X = np.array(eng_encoded, dtype=int)\n",
    "y = np.array(jpn_encoded, dtype=int)\n",
    "\n",
    "V_jpn = jpn_embedding.shape[0]\n",
    "o_w = np.random.randn(V_jpn, hidden_size) * 0.01\n",
    "o_b = np.zeros(V_jpn)\n",
    "\n",
    "past_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    X_train = X[perm]\n",
    "    y_train = y[perm]\n",
    "\n",
    "    for batch in range(0, X.shape[0], batch_size):\n",
    "        end = min(batch + batch_size, X_train.shape[0])\n",
    "        X_ids = X_train[batch:end]\n",
    "        y_train_batch = y_train[batch:end]\n",
    "        batch_len = X_ids.shape[0]\n",
    "        \n",
    "        # Encoder forward\n",
    "        X_emb = eng_embedding[X_ids]\n",
    "        h_enc, enc_h_all, enc_store = Encoder_GRU.forward(X_emb)\n",
    "\n",
    "        decoder_input = np.concatenate(\n",
    "            [np.full((batch_len, 1), bos_id, dtype=int), y_train_batch[:, :-1]],\n",
    "            axis=1\n",
    "        )\n",
    "        decoder_target = y_train_batch\n",
    "\n",
    "        dec_in_emb = jpn_embedding[decoder_input]        # (N, T, D)\n",
    "        _, h_dec_all, dec_store = Decoder_GRU.forward(dec_in_emb, h_enc)\n",
    "\n",
    "        logits_flat = (h_dec_all @ o_w.T + o_b).reshape(-1, V_jpn) # (bs*seq_len, V_jpn)\n",
    "        targets_flat = decoder_target.reshape(-1)\n",
    "\n",
    "        mask = (targets_flat != pad_id)\n",
    "        probs = softmax(logits_flat)\n",
    "        probs = np.clip(probs, 1e-12, 1 - 1e-12)\n",
    "        \n",
    "        loss = -np.sum(np.log(probs[np.where(mask)[0], targets_flat[mask]])) / mask.sum()\n",
    "        total_loss += loss * batch_len\n",
    "\n",
    "        # Backprop through softmax + cross entropy\n",
    "        dlogits = probs\n",
    "        dlogits[np.where(mask)[0], targets_flat[mask]] -= 1\n",
    "        dlogits /= mask.sum()  # average over non pad tokens\n",
    "\n",
    "        # Final linear layer grad\n",
    "        h_dec_all_flat = h_dec_all.reshape(-1, hidden_size)\n",
    "        do_w = dlogits.T @ h_dec_all_flat    # (V_jpn, H)\n",
    "        do_b = np.sum(dlogits, axis=0)       # (V_jpn,)\n",
    "        dh_dec = dlogits @ o_w\n",
    "        dh_dec = dh_dec.reshape(batch_len, target_len, hidden_size)\n",
    "\n",
    "        # Decoder GRU backward\n",
    "        dH_enc_from_dec, dx_dec = Decoder_GRU.backward(dh_dec, dec_store, lr)\n",
    "\n",
    "        # Update decoder embeddings and clip gradients\n",
    "        for b in range(batch_len):\n",
    "            for t in range(target_len):\n",
    "                idx_tok = decoder_input[b, t]\n",
    "                grad = np.clip(dx_dec[b, t], -1.0, 1.0)\n",
    "                jpn_embedding[idx_tok] -= lr * grad\n",
    "\n",
    "        # Encoder GRU backward\n",
    "        # Build gradient tensor with only last time-step having gradient\n",
    "        dH_enc_all = np.zeros_like(enc_h_all)           # (B, encoder_len, H)\n",
    "        dH_enc_all[:, -1, :] = dH_enc_from_dec          # gradient only on final encoder state\n",
    "        _, dx_enc = Encoder_GRU.backward(dH_enc_all, enc_store, lr)\n",
    "\n",
    "        # Update encoder embeddings and clip gradients\n",
    "        for b in range(batch_len):\n",
    "            for t in range(encoder_len):\n",
    "                tok = X_ids[b, t]\n",
    "                grad = np.clip(dx_enc[b, t], -1.0, 1.0)\n",
    "                eng_embedding[tok] -= lr * grad\n",
    "\n",
    "        # Clip output gradients\n",
    "        np.clip(do_w, -1.0, 1.0, out=do_w)\n",
    "        np.clip(do_b, -1.0, 1.0, out=do_b)\n",
    "\n",
    "        # Update output layer\n",
    "        o_w -= lr * do_w\n",
    "        o_b -= lr * do_b\n",
    "\n",
    "    tl = total_loss / X_train.shape[0]\n",
    "    print(f\"Epoch {epoch+1} | Loss: {tl:.4f}\")\n",
    "\n",
    "    past_loss.append(tl)\n",
    "\n",
    "    if len(past_loss) > 2:\n",
    "        if past_loss[-1] > past_loss[-2] and past_loss[-2] > past_loss[-3]:\n",
    "            break\n",
    "\n",
    "save_parameters(\"model_params4.npz\", Encoder_GRU, Decoder_GRU, eng_embedding, jpn_embedding, o_w, o_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b04914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T20:47:45.195543Z",
     "iopub.status.busy": "2025-08-06T20:47:45.194782Z",
     "iopub.status.idle": "2025-08-06T20:47:45.204349Z",
     "shell.execute_reply": "2025-08-06T20:47:45.203595Z"
    },
    "papermill": {
     "duration": 0.025472,
     "end_time": "2025-08-06T20:47:45.205758",
     "exception": false,
     "start_time": "2025-08-06T20:47:45.180286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(Encoder_GRU, Decoder_GRU, input_seq,\n",
    "            eng_to_ind, eng_embedding,\n",
    "            jpn_embedding, o_w, o_b,\n",
    "            pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
    "            max_len=20, encoder_len=3):\n",
    "    \"\"\"\n",
    "    Greedy decode.\n",
    "\n",
    "    Returns list of predicted token ids excluding <eos>.\n",
    "    \"\"\"\n",
    "    # Preprocess encoder input\n",
    "    import re, numpy as np\n",
    "    seq = re.sub(r\"[^a-z0-9\\s]\", \"\", input_seq.strip().lower())\n",
    "    src_tokens = [eng_to_ind.get(w, unk_id) for w in seq.split()]\n",
    "    if not src_tokens:\n",
    "        src_tokens = [pad_id]\n",
    "    while len(src_tokens) < encoder_len:\n",
    "        src_tokens.append(pad_id)\n",
    "    src_tokens = src_tokens[:encoder_len]\n",
    "\n",
    "    enc_embed = eng_embedding[src_tokens][None, ...]  # (1,T,D)\n",
    "    h_enc, _, _ = Encoder_GRU.forward(enc_embed)      # (1,H)\n",
    "\n",
    "    h = h_enc\n",
    "    prev_token = bos_id\n",
    "    preds = []\n",
    "\n",
    "    for step in range(max_len):\n",
    "        # Embed previous token\n",
    "        y_embed = jpn_embedding[prev_token][None, :]   # (1,D)\n",
    "\n",
    "        r = sigmoid(y_embed @ Decoder_GRU.r_w.T + h @ Decoder_GRU.r_u + Decoder_GRU.r_b)\n",
    "        z = sigmoid(y_embed @ Decoder_GRU.z_w.T + h @ Decoder_GRU.z_u + Decoder_GRU.z_b)\n",
    "        c = tanh(y_embed @ Decoder_GRU.c_w.T + (r * h) @ Decoder_GRU.c_u + Decoder_GRU.c_b)\n",
    "        h = z * h + (1 - z) * c # (1,H)\n",
    "\n",
    "        logits = h @ o_w.T + o_b # (1,V_jpn)\n",
    "\n",
    "        # Prevent <pad> and <bos> from being chosen\n",
    "        logits[0, pad_id] = -1e9\n",
    "        logits[0, bos_id] = -1e9\n",
    "\n",
    "        next_token = int(np.argmax(logits, axis=1)[0])\n",
    "\n",
    "        if next_token == eos_id:\n",
    "            break\n",
    "\n",
    "        preds.append(next_token)\n",
    "        prev_token = next_token\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da30c46b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T20:47:45.233061Z",
     "iopub.status.busy": "2025-08-06T20:47:45.232740Z",
     "iopub.status.idle": "2025-08-06T20:47:45.240380Z",
     "shell.execute_reply": "2025-08-06T20:47:45.239407Z"
    },
    "papermill": {
     "duration": 0.023029,
     "end_time": "2025-08-06T20:47:45.242017",
     "exception": false,
     "start_time": "2025-08-06T20:47:45.218988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(path, embedding_dim=64, hidden_size=128):\n",
    "    params = np.load(path)\n",
    "\n",
    "    # Initialize GRUs\n",
    "    Encoder_GRU = GRU(embedding_dim, hidden_size)\n",
    "    Decoder_GRU = GRU(embedding_dim, hidden_size)\n",
    "\n",
    "    # Load Encoder weights\n",
    "    Encoder_GRU.r_w = params[\"enc_r_w\"]\n",
    "    Encoder_GRU.r_u = params[\"enc_r_u\"]\n",
    "    Encoder_GRU.r_b = params[\"enc_r_b\"]\n",
    "    Encoder_GRU.z_w = params[\"enc_z_w\"]\n",
    "    Encoder_GRU.z_u = params[\"enc_z_u\"]\n",
    "    Encoder_GRU.z_b = params[\"enc_z_b\"]\n",
    "    Encoder_GRU.c_w = params[\"enc_c_w\"]\n",
    "    Encoder_GRU.c_u = params[\"enc_c_u\"]\n",
    "    Encoder_GRU.c_b = params[\"enc_c_b\"]\n",
    "\n",
    "    # Load Decoder weights\n",
    "    Decoder_GRU.r_w = params[\"dec_r_w\"]\n",
    "    Decoder_GRU.r_u = params[\"dec_r_u\"]\n",
    "    Decoder_GRU.r_b = params[\"dec_r_b\"]\n",
    "    Decoder_GRU.z_w = params[\"dec_z_w\"]\n",
    "    Decoder_GRU.z_u = params[\"dec_z_u\"]\n",
    "    Decoder_GRU.z_b = params[\"dec_z_b\"]\n",
    "    Decoder_GRU.c_w = params[\"dec_c_w\"]\n",
    "    Decoder_GRU.c_u = params[\"dec_c_u\"]\n",
    "    Decoder_GRU.c_b = params[\"dec_c_b\"]\n",
    "\n",
    "    # Load embeddings and output layer\n",
    "    eng_embedding = params[\"eng_embedding\"]\n",
    "    jpn_embedding = params[\"jpn_embedding\"]\n",
    "    o_w = params[\"o_w\"]\n",
    "    o_b = params[\"o_b\"]\n",
    "\n",
    "    return Encoder_GRU, Decoder_GRU, eng_embedding, jpn_embedding, o_w, o_b"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7937927,
     "sourceId": 12569836,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 264.765258,
   "end_time": "2025-08-06T20:47:45.678002",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-06T20:43:20.912744",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
