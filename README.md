# ML-Models-on-Datasets
Collection of machine learning projects created using scikit-learn, PyTorch, or implemented from scratch in NumPy. All projects are original, built from the ground up based on my own understanding. No tutorials were used, these are genuine implementations to deepen my grasp of ML concepts. All projects were created as I learned, so a few include less practical decisions, like evaluating an imbalanced classification problem using only accuracy.

# üèÜ Top Projects
These are projects I am especially proud of. Each projects ipynb file contains fleshed out explanations of my thought process, and the code. This list will evolve as I continue to learn and build more.

| Project                                  | Description                                                                                                                                                                                  | Location                                      |
|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|
| **LeNet-5 on MNIST (NumPy & PyTorch)**   | Implemented LeNet-5 entirely from scratch in NumPy, with manual forward & backward for convolutions, pooling, and dense layers. Later replicated in PyTorch to achieve ~99% on MNIST.        | [`CNN from Scratch with NumPy/`](./CNN_from_Scratch_with_NumPy)           |
| **English‚ÄìJapanese GRU Translator (NumPy)** | Encoder‚Äìdecoder model built entirely in NumPy to translate short English sentences into Japanese. Manual GRU forward/backward, teacher forcing, gradient clipping, and backpropagation calculations written on paper.  | [`GRU Translator/`](./Gru_Encoder_Decoder_Numpy) |
| **GoogLeNet from Scratch (Tiny ImageNet)** | Built Inception v1 from scratch in PyTorch, adapted for 64√ó64 Tiny ImageNet by modifying pooling & convolutions. Included hand-drawn architecture diagrams and tensor checks.    | [`GoogLeNet from Scratch/`](./GoogLeNet_from_Scratch_with_Pytorch) |
| **Transformer English‚ÄìJapanese Translator (PyTorch)** | Implemented full Transformer encoder‚Äìdecoder model from scratch in PyTorch, including attention, positional encodings, masking, and a custom LR scheduler. Outputs generated via beam search. | [`Transformer Translator/`](./Transformer Encoder-Decoder) |
| **Stellar Classification (SDSS)**        | Classified stars, galaxies, and quasars using Sloan Digital Sky Survey data. Engineered rich photometric features, handled imbalance with SMOTE, tuned CatBoost/LightGBM/XGBoost to ~98.5%.  | [`Stellar Classification/`](./Stellar_Classification) |
| **Tiny ImageNet ResNet-34 (PyTorch)**    | Modified ResNet-34, and built from scratch in Pytorch for small 64√ó64 Tiny ImageNet images by removing early pooling and adjusting initial convolutions. Trained on Kaggle GPUs to ~61% top-1 accuracy.                         | [`ResNet-34 from Scratch/`](./ResNet-34_from_Scratch_with_Pytorch)   |
| **Invariant Mass from Dielectron Events**| Predicted invariant mass of electron pairs from CMS proton-proton collisions (CERN). Used physics-driven features & XGBoost tuned to ~0.997 R¬≤ on test data.                                 | [`Invariant Mass CERN/`](./Invariant_Mass_CERN) |

# üóÇÔ∏è Other Projects
Additional ML projects. These projects are still narrated in the ipynb files.

| Project                                    | Description                                                                                                                                                                                                                                                                                                                                      | Locatiom                                           |
|--------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------|
| **Char-level RNN from Scratch (NumPy)**    | Built a vanilla RNN from scratch using NumPy to generate Shakespeare-like text. Implemented manual forward and BPTT, with gradient clipping and softmax temperature sampling for creativity.                                                                                                                                                     | [`RNN from Scratch with NumPy/`](./RNN_from_scratch_with_Numpy) |
| **Exoplanet Detection (Kepler)**           | Classified NASA Kepler objects as true exoplanets or false positives using LightGBM, XGBoost, Random Forest, and SGD. Conducted careful leakage removal and preprocessing. Tuned models to achieve \~95% test accuracy. Used 5-fold CV and plotted correlation heatmaps.                                                                         | [`Exoplanet Detection/`](./Exoplanet_Detection)     |
| **Mental Health Depression Detection**     | Predicted depression from a large-scale survey dataset using manual feature engineering and a ColumnTransformer pipeline. Applied XGBoost with Optuna tuning, achieving \~94% test accuracy. Included features from external data (crime rates) and handled diverse categorical variables.                                                       | [`Mental Health/`](./Mental_Health)     |
| **MLP on MNIST (NumPy)**                   | Built a fully connected neural network from scratch in NumPy with ReLU, softmax, cross-entropy, and explicit backprop. Used mini-batch gradient descent and He initialization, achieving \~98.3% accuracy on MNIST. A pure linear algebra implementation demonstrating how MLPs learn.                                                           | [`MLP From Scratch in Numpy/`](./MLP_from_scratch_with_NumPy)     |
| **Abalone Age Prediction**                 | Predicted the age of abalone based on physical measurements (length, diameter, height, whole weight, etc.), replacing the tedious ring-counting lab process. Used XGBoost, Random Forest, and SGDRegressor, achieving a test RMSE as low as \~2.01. Applied feature engineering, outlier removal, scaling, and Optuna for hyperparameter tuning. | [`Abalone Age/`](./Abalone)    |
| **Softmax Classifier on Iris (NumPy)**     | Implemented a multi-class logistic regression (softmax) classifier from scratch using only NumPy, trained on the Iris dataset. Used SGD and cross-entropy loss, achieving \~97.4% test accuracy, illustrating fundamental multiclass classification without ML libraries.                                                                        | [`Classifier From Scratch in NumPy/`](./Classification_with_NumPy)    |
| **Pulsar Detection**                       | Detected pulsars versus radio noise using the HTRU 2 survey data. Compared SVM (SGD), XGBoost, and a small neural network, achieving ~97.8% test accuracy. Highlights working with highly imbalanced astronomical datasets and evaluating across multiple models.                                                                                | [`Pulsar Detection/`](./Classification_models/Pulsar_Detection)     |


Built by Kaizen Rowe